{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import setGPU\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from sklearn.utils import shuffle\n",
    "import h5py\n",
    "\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Anomalous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, RobustScaler, normalize, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature names\n",
    "var_names_reduced5 = ['qPFJetPt', 'qPFJetEta', 'qPFJetPhi', 'qPFJet0Pt', 'qPFJet1Pt', 'qPFJet2Pt', 'qPFJet3Pt', 'qPFJet4Pt', 'qPFJet5Pt', 'qPFJet0Eta', 'qPFJet1Eta', 'qPFJet2Eta', 'qPFJet3Eta', 'qPFJet4Eta', 'qPFJet5Eta', 'qPFJet0Phi', 'qPFJet1Phi', 'qPFJet2Phi', 'qPFJet3Phi', 'qPFJet4Phi', 'qPFJet5Phi', 'qPFJet4CHS0Pt', 'qPFJet4CHS1Pt', 'qPFJet4CHS2Pt', 'qPFJet4CHS3Pt', 'qPFJet4CHS4Pt', 'qPFJet4CHS5Pt', 'qPFJet4CHS0Eta', 'qPFJet4CHS1Eta', 'qPFJet4CHS2Eta', 'qPFJet4CHS3Eta', 'qPFJet4CHS4Eta', 'qPFJet4CHS5Eta', 'qPFJet4CHS0Phi', 'qPFJet4CHS1Phi', 'qPFJet4CHS2Phi', 'qPFJet4CHS3Phi', 'qPFJet4CHS4Phi', 'qPFJet4CHS5Phi', 'qPFJet8CHS0Pt', 'qPFJet8CHS1Pt', 'qPFJet8CHS2Pt', 'qPFJet8CHS3Pt', 'qPFJet8CHS4Pt', 'qPFJet8CHS5Pt', 'qPFJet8CHS0Eta', 'qPFJet8CHS1Eta', 'qPFJet8CHS2Eta', 'qPFJet8CHS3Eta', 'qPFJet8CHS4Eta', 'qPFJet8CHS5Eta', 'qPFJet8CHS0Phi', 'qPFJet8CHS1Phi', 'qPFJet8CHS2Phi', 'qPFJet8CHS3Phi', 'qPFJet8CHS4Phi', 'qPFJet8CHS5Phi', 'qPFJetEI0Pt', 'qPFJetEI1Pt', 'qPFJetEI2Pt', 'qPFJetEI3Pt', 'qPFJetEI4Pt', 'qPFJetEI5Pt', 'qPFJetEI0Eta', 'qPFJetEI1Eta', 'qPFJetEI2Eta', 'qPFJetEI3Eta', 'qPFJetEI4Eta', 'qPFJetEI5Eta', 'qPFJetEI0Phi', 'qPFJetEI1Phi', 'qPFJetEI2Phi', 'qPFJetEI3Phi', 'qPFJetEI4Phi', 'qPFJetEI5Phi', 'qPFJet8CHSSD0Pt', 'qPFJet8CHSSD1Pt', 'qPFJet8CHSSD2Pt', 'qPFJet8CHSSD3Pt', 'qPFJet8CHSSD4Pt', 'qPFJet8CHSSD5Pt', 'qPFJet8CHSSD0Eta', 'qPFJet8CHSSD1Eta', 'qPFJet8CHSSD2Eta', 'qPFJet8CHSSD3Eta', 'qPFJet8CHSSD4Eta', 'qPFJet8CHSSD5Eta', 'qPFJet8CHSSD0Phi', 'qPFJet8CHSSD1Phi', 'qPFJet8CHSSD2Phi', 'qPFJet8CHSSD3Phi', 'qPFJet8CHSSD4Phi', 'qPFJet8CHSSD5Phi', 'qPFJetTopCHS0Pt', 'qPFJetTopCHS1Pt', 'qPFJetTopCHS2Pt', 'qPFJetTopCHS3Pt', 'qPFJetTopCHS4Pt', 'qPFJetTopCHS5Pt', 'qPFJetTopCHS0Eta', 'qPFJetTopCHS1Eta', 'qPFJetTopCHS2Eta', 'qPFJetTopCHS3Eta', 'qPFJetTopCHS4Eta', 'qPFJetTopCHS5Eta', 'qPFJetTopCHS0Phi', 'qPFJetTopCHS1Phi', 'qPFJetTopCHS2Phi', 'qPFJetTopCHS3Phi', 'qPFJetTopCHS4Phi', 'qPFJetTopCHS5Phi', 'qCalJet0Pt', 'qCalJet1Pt', 'qCalJet2Pt', 'qCalJet3Pt', 'qCalJet4Pt', 'qCalJet5Pt', 'qCalJet0Eta', 'qCalJet1Eta', 'qCalJet2Eta', 'qCalJet3Eta', 'qCalJet4Eta', 'qCalJet5Eta', 'qCalJet0Phi', 'qCalJet1Phi', 'qCalJet2Phi', 'qCalJet3Phi', 'qCalJet4Phi', 'qCalJet5Phi', 'qCalJet0En', 'qCalJet1En', 'qCalJet2En', 'qCalJet3En', 'qCalJet4En', 'qCalJet5En', 'qPho0Pt', 'qPho1Pt', 'qPho2Pt', 'qPho3Pt', 'qPho4Pt', 'qPho5Pt', 'qPho0Eta', 'qPho1Eta', 'qPho2Eta', 'qPho3Eta', 'qPho4Eta', 'qPho5Eta', 'qPho0Phi', 'qPho1Phi', 'qPho2Phi', 'qPho3Phi', 'qPho4Phi', 'qPho5Phi', 'qPho0En', 'qPho1En', 'qPho2En', 'qPho3En', 'qPho4En', 'qPho5En', 'qgedPho0Pt', 'qgedPho1Pt', 'qgedPho2Pt', 'qgedPho3Pt', 'qgedPho4Pt', 'qgedPho5Pt', 'qgedPho0Eta', 'qgedPho1Eta', 'qgedPho2Eta', 'qgedPho3Eta', 'qgedPho4Eta', 'qgedPho5Eta', 'qgedPho0Phi', 'qgedPho1Phi', 'qgedPho2Phi', 'qgedPho3Phi', 'qgedPho4Phi', 'qgedPho5Phi', 'qgedPho0En', 'qgedPho1En', 'qgedPho2En', 'qgedPho3En', 'qgedPho4En', 'qgedPho5En', 'qMu0Pt', 'qMu1Pt', 'qMu2Pt', 'qMu3Pt', 'qMu4Pt', 'qMu5Pt', 'qMu0Eta', 'qMu1Eta', 'qMu2Eta', 'qMu3Eta', 'qMu4Eta', 'qMu5Eta', 'qMu0Phi', 'qMu1Phi', 'qMu2Phi', 'qMu3Phi', 'qMu4Phi', 'qMu5Phi', 'qMu0En', 'qMu1En', 'qMu2En', 'qMu3En', 'qMu4En', 'qMu5En', 'qMuCosm0Pt', 'qMuCosm1Pt', 'qMuCosm2Pt', 'qMuCosm3Pt', 'qMuCosm4Pt', 'qMuCosm5Pt', 'qMuCosm0Eta', 'qMuCosm1Eta', 'qMuCosm2Eta', 'qMuCosm3Eta', 'qMuCosm4Eta', 'qMuCosm5Eta', 'qMuCosm0Phi', 'qMuCosm1Phi', 'qMuCosm2Phi', 'qMuCosm3Phi', 'qMuCosm4Phi', 'qMuCosm5Phi', 'qMuCosm0En', 'qMuCosm1En', 'qMuCosm2En', 'qMuCosm3En', 'qMuCosm4En', 'qMuCosm5En', 'qMuCosmLeg0Pt', 'qMuCosmLeg1Pt', 'qMuCosmLeg2Pt', 'qMuCosmLeg3Pt', 'qMuCosmLeg4Pt', 'qMuCosmLeg5Pt', 'qMuCosmLeg0Eta', 'qMuCosmLeg1Eta', 'qMuCosmLeg2Eta', 'qMuCosmLeg3Eta', 'qMuCosmLeg4Eta', 'qMuCosmLeg5Eta', 'qMuCosmLeg0Phi', 'qMuCosmLeg1Phi', 'qMuCosmLeg2Phi', 'qMuCosmLeg3Phi', 'qMuCosmLeg4Phi', 'qMuCosmLeg5Phi', 'qMuCosmLeg0En', 'qMuCosmLeg1En', 'qMuCosmLeg2En', 'qMuCosmLeg3En', 'qMuCosmLeg4En', 'qMuCosmLeg5En', 'qPFJet4CHSPt', 'qPFJet4CHSEta', 'qPFJet4CHSPhi', 'qPFJet8CHSPt', 'qPFJet8CHSEta', 'qPFJet8CHSPhi', 'qPFJetEIPt', 'qPFJetEIEta', 'qPFJetEIPhi', 'qPFJet8CHSSDPt', 'qPFJet8CHSSDEta', 'qPFJet8CHSSDPhi', 'qPFJetTopCHSPt', 'qPFJetTopCHSEta', 'qPFJetTopCHSPhi', 'qPFChMetPt', 'qPFChMetPhi', 'qPFMetPt', 'qPFMetPhi', 'qNVtx', 'qCalJetPt', 'qCalJetEta', 'qCalJetPhi', 'qCalJetEn', 'qCalMETPt', 'qCalMETPhi', 'qCalMETEn', 'qCalMETBEPt', 'qCalMETBEPhi', 'qCalMETBEEn', 'qCalMETBEFOPt', 'qCalMETBEFOPhi', 'qCalMETBEFOEn', 'qCalMETMPt', 'qCalMETMPhi', 'qCalMETMEn', 'qSCEn', 'qSCEta', 'qSCPhi', 'qSCEtaWidth', 'qSCPhiWidth', 'qSCEnhfEM', 'qSCEtahfEM', 'qSCPhihfEM', 'qSCEn5x5', 'qSCEta5x5', 'qSCPhi5x5', 'qSCEtaWidth5x5', 'qSCPhiWidth5x5', 'qCCEn', 'qCCEta', 'qCCPhi', 'qCCEn5x5', 'qCCEta5x5', 'qCCPhi5x5', 'qPhoPt', 'qPhoEta', 'qPhoPhi', 'qPhoEn_', 'qPhoe1x5_', 'qPhoe2x5_', 'qPhoe3x3_', 'qPhoe5x5_', 'qPhomaxenxtal_', 'qPhosigmaeta_', 'qPhosigmaIeta_', 'qPhor1x5_', 'qPhor2x5_', 'qPhor9_', 'qgedPhoPt', 'qgedPhoEta', 'qgedPhoPhi', 'qgedPhoEn_', 'qgedPhoe1x5_', 'qgedPhoe2x5_', 'qgedPhoe3x3_', 'qgedPhoe5x5_', 'qgedPhomaxenxtal_', 'qgedPhosigmaeta_', 'qgedPhosigmaIeta_', 'qgedPhor1x5_', 'qgedPhor2x5_', 'qgedPhor9_', 'qMuPt', 'qMuEta', 'qMuPhi', 'qMuEn_', 'qMuCh_', 'qMuChi2_', 'qMuCosmPt', 'qMuCosmEta', 'qMuCosmPhi', 'qMuCosmEn_', 'qMuCosmCh_', 'qMuCosmChi2_', 'qMuCosmLegPt', 'qMuCosmLegEta', 'qMuCosmLegPhi', 'qMuCosmLegEn_', 'qMuCosmLegCh_', 'qMuCosmLegChi2_', 'qSigmaIEta', 'qSigmaIPhi', 'qr9', 'qHadOEm', 'qdrSumPt', 'qdrSumEt', 'qeSCOP', 'qecEn', 'qUNSigmaIEta', 'qUNSigmaIPhi', 'qUNr9', 'qUNHadOEm', 'qUNdrSumPt', 'qUNdrSumEt', 'qUNeSCOP', 'qUNecEn', 'qEBenergy', 'qEBtime', 'qEBchi2', 'qEBiEta', 'qEBiPhi', 'qEEenergy', 'qEEtime', 'qEEchi2', 'qEEix', 'qEEiy', 'qESenergy', 'qEStime', 'qESix', 'qESiy', 'qHBHEenergy', 'qHBHEtime', 'qHBHEauxe', 'qHBHEieta', 'qHBHEiphi', 'qHFenergy', 'qHFtime', 'qHFieta', 'qHFiphi', 'qPreShEn', 'qPreShEta', 'qPreShPhi', 'qPreShYEn', 'qPreShYEta', 'qPreShYPhi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Authenticate in order to get permission for eos\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load h5 files\n",
    "\n",
    "\n",
    "#Choose where to load the files from\n",
    "# b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "# b_h5       = '/eos/cms/store/user/fsiroky/lumih5/'\n",
    "b_h5 = '/eos/cms/store/user/fsiroky/consistentlumih5/'\n",
    "# b_h5   = '/afs/cern.ch/user/f/fsiroky/public/'\n",
    "# b_h5 = '/mnt/hdf5test/'\n",
    "# b_h5   = '/home/test_local/'\n",
    "\n",
    "pds  = {1: 'BTagCSV', 2: 'BTagMu', 3: 'Charmonium', 4:'DisplacedJet', 5: 'DoubleEG',\n",
    "        6: 'DoubleMuon', 7: 'DoubleMuonLowMass',\n",
    "       # 8: 'FSQJets', 9: 'HighMultiplicityEOF', #NOT ENOUGH DATA, NOTEBOOK FAILES\n",
    "        10: 'HTMHT', 11: 'JetHT', 12: 'MET',\n",
    "       # 13: 'MinimumBias', #NOT ENOUGH DATA\n",
    "        14: 'MuonEG', 15: 'MuOnia',\n",
    "       # 16: 'NoBPTX',\n",
    "        17: 'SingleElectron', 18: 'SingleMuon', 19: 'SinglePhoton', 20: 'Tau', 21: 'ZeroBias'\n",
    "}\n",
    "\n",
    "      \n",
    "def get_jets(bg_files, bg_jets, sig_files, sig_jets):\n",
    "    #Use np.empty([0,2802]) for both good and bad jets, if you use b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "    good_jets = np.empty([0,2813])\n",
    "    bad_jets  = np.empty([0,2813])\n",
    "                   # Control which time intervals files per PD to load with range in the for loop\n",
    "    for i in range(0,len(bg_files)):   #0\n",
    "        try:\n",
    "            bg_jetfile  = h5py.File(bg_files[i],'r')\n",
    "            bg_jet      = bg_jetfile[bg_jets[i]][:]\n",
    "            sig_jetfile = h5py.File(sig_files[i],'r')\n",
    "            sig_jet     = sig_jetfile[sig_jets[i]][:]\n",
    "            # print(bad_jets.shape, bg_jet.shape)\n",
    "            bad_jets    = np.concatenate((bad_jets, bg_jet), axis=0)\n",
    "            good_jets = np.concatenate((good_jets, sig_jet), axis=0)\n",
    "            print( \"Number of good lumis: \", len(sig_jet), \" Number of bad lumis: \", len(bg_jet)) \n",
    "\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have \", bg_jets[i], error )\n",
    "            continue\n",
    "    return good_jets, bad_jets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis:  17238  Number of bad lumis:  492\n",
      "Number of good lumis:  26782  Number of bad lumis:  121\n",
      "Number of good lumis:  15558  Number of bad lumis:  638\n",
      "Number of good lumis:  17901  Number of bad lumis:  70\n",
      "Number of good lumis:  40180  Number of bad lumis:  201\n",
      "Number of good lumis:  41347  Number of bad lumis:  3156\n"
     ]
    }
   ],
   "source": [
    "#Choose which PD to load\n",
    "nbr = 11 #Jvariable\n",
    "\n",
    "bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "#Load good and bad jets\n",
    "good_jets, bad_jets = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 3 #Charmonium\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets2, bad_jets2 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 15 #\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets3, bad_jets3 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 14\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets4, bad_jets4 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "#Assign good jets class label 0\n",
    "df1 = pd.DataFrame(good_jets)\n",
    "# cutted_df = df1.iloc[0:25000, :]   #Temporarily to make training faster\n",
    "# df1 = cutted_df                   #Temporarily to make training faster\n",
    "df1['class'] = 0\n",
    "\n",
    "#Assign bad_jets class label  1\n",
    "df2 = pd.DataFrame(bad_jets)\n",
    "# cutted_df = df2.iloc[0:, :]    #Temporarily to make training faster\n",
    "# df2 = cutted_df                   #Temporarily to make training faster\n",
    "df2['class'] = 1\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df3 = pd.DataFrame(good_jets2)\n",
    "\n",
    "# df3['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df4 = pd.DataFrame(bad_jets2)\n",
    "\n",
    "# df4['class'] = 1\n",
    "\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df5 = pd.DataFrame(good_jets3)\n",
    "\n",
    "# df5['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df6 = pd.DataFrame(bad_jets3)\n",
    "\n",
    "# df6['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df7 = pd.DataFrame(good_jets4)\n",
    "# df7['class'] = 0\n",
    "\n",
    "# df8 = pd.DataFrame(bad_jets4)\n",
    "# df8['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# del(good_jets)\n",
    "# del(bad_jets)\n",
    "#Concatenate them\n",
    "frames  = [df1,df2] \n",
    "#frames = [df1,df2,df3,df4,df5,df6]\n",
    "# frames = [df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "data   = pd.concat(frames)\n",
    "del(frames)\n",
    "# del(df1)\n",
    "# del(df2)\n",
    "\n",
    "data.drop(2805+7, axis=1, inplace=True) #Drop per_pd flags\n",
    "\n",
    "data = data.sort_values([2800+7,2801+7], ascending=[True,True]) #Sort by runID and then by lumiID\n",
    "data = data.reset_index(drop=True)  #Reset index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = data.reindex(index=range(0,len(data)))\n",
    "#Shuffle them randomly\n",
    "# data = shuffle(data)\n",
    "# data = data.reset_index(drop=True)\n",
    "\n",
    "#Save labels and delete them from df not to cheat during training\n",
    "# labels = data['class'].astype(int)\n",
    "# del data['class']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Relabelling incorrect Fede json with updated one by current choice\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    outcome = 5\n",
    "    for k,v in json_file.items():\n",
    "        if (int(k) == orig_runid):\n",
    "            for d in v: #Checks each inner loop of the json per runID\n",
    "                for i in range (d[0], d[1]+1):\n",
    "#                     print(\"key of json is \", k, \" value of json is \", v)\n",
    "# #                     print(v[0][0], \"and\", v[0][1])\n",
    "#                     print(\"current inner list is\", d, \"and range is\", d[0], \" to \", d[1])\n",
    "#                     print(\"i is \", i)\n",
    "                    if i == orig_lumid:\n",
    "#                         print(\"Flagging as bad\")\n",
    "                        outcome =0  #0 means good lumi! (to be compatible with code anomaly_detection.ipynb[mse ae])\n",
    "                        return(outcome)\n",
    "                \n",
    "            \n",
    "        \n",
    "    outcome = 1 #1 means bad lumisection! (to be compatible with code anomaly_detection.ipynb [mse autoencoder])\n",
    "    return(outcome)\n",
    "\n",
    "json_file_path = '/afs/cern.ch/user/f/fsiroky/public/Cert_271036-284044_13TeV_PromptReco_Collisions16_JSON.txt'\n",
    "\n",
    "def add_flags_from_json(output_json, data):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([data.shape[0],1])\n",
    "    for i in range(0, data.shape[0]):\n",
    "        orig_runid = data[2800+7][i]\n",
    "        orig_runid = int(orig_runid)\n",
    "        orig_lumid = data[2801+7][i]\n",
    "        orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    data['preco_json'] = new_json_class #PromptReco GOLDEN json\n",
    "    return data\n",
    "\n",
    "new_data = add_flags_from_json(json_file_path, data)\n",
    " \n",
    "del(new_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO!\n",
    "#Check how many good lumis and anomalous ones we have\n",
    "\n",
    "# print(\"Laaalelaaa\", data)\n",
    "\n",
    "# anomalies = data[data['class'] == 1]\n",
    "# normal    = data[data['class'] == 0]\n",
    "\n",
    "# print(\"Number of anomalies: \", anomalies.shape)\n",
    "# del(anomalies)\n",
    "\n",
    "# print(\"Number of normals: \", normal.shape)\n",
    "# del(normal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save of RunIDs and LumiIDs done\n",
      "RunID and LumiID dropped\n"
     ]
    }
   ],
   "source": [
    "#Save runIDs and lumiIDs and instantaneous luminosities for later, because now we drop them before training\n",
    "\n",
    "runIDs  = data[2800+7].astype(int)\n",
    "lumiIDs = data[2801+7].astype(int)\n",
    "lumisections = data[2802+7].astype(float)\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datarunIDs.npy',  runIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datalumiIDs.npy', lumiIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/lumisections.npy', lumisections)\n",
    "\n",
    "\n",
    "print(\"Save of RunIDs and LumiIDs done\")\n",
    "\n",
    "# print(data)\n",
    "data.drop(2800+7, axis=1, inplace=True) #drop RunID before normalizing and training\n",
    "data.drop(2801+7, axis=1, inplace=True) #drop LumiID before normalizing and training\n",
    "print(\"RunID and LumiID dropped\")\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the data to make training better\n",
    "\n",
    "cutted_data = data.iloc[:, 0:2803+7]\n",
    "#classes     = data.iloc[:, 2805:2806] \n",
    "classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "\n",
    "# print(classes.shape)\n",
    "np_scaled = StandardScaler().fit_transform(cutted_data.values)\n",
    "# np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "datas = pd.DataFrame(np_scaled)\n",
    "# datas = pd.DataFrame(np_scaled, index=cutted_data.index, columns=cutted_data.columns)\n",
    "\n",
    "# print(\"2222\",datas)\n",
    "\n",
    "# del(np_scaled)\n",
    "del(cutted_data)\n",
    "# print(\"Datas first: \", datas)\n",
    "datas[2803+7] = runIDs   #Append runID back after scaling\n",
    "datas[2804+7] = lumiIDs  #Append lumiID back after scaling\n",
    "datas['qlabel'] = classes  #qlabel is goldenJSON now\n",
    "\n",
    "# print(\"After scale\", datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163684, 2813)\n"
     ]
    }
   ],
   "source": [
    "#TEST/TRAIN SPLIT\n",
    "\n",
    "# X_train, X_test = train_test_split(datas, test_size=0.15, random_state=RANDOM_SEED) # This works when we split rndmly\n",
    "split_nbr = round(datas.shape[0]*0.20)  #0.10 means 10% to the validation set\n",
    "\n",
    "print(datas.shape)\n",
    "X_train = datas.iloc[0:(datas.shape[0] - split_nbr) ,:]\n",
    "X_test  = datas.iloc[(datas.shape[0] - split_nbr): (datas.shape[0]) ,:]\n",
    "last_train_idx = X_train.shape[0]\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/last_train_idx.npy', last_train_idx)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "del(datas)\n",
    "X_train = X_train[X_train['qlabel']== 0]\n",
    "# print(X_train)\n",
    "X_train = X_train.drop(['qlabel'], axis=1)\n",
    "\n",
    "ae_lumis = X_train[2800+7].astype(float)\n",
    "# print(\"ae lumis\", ae_lumis, \"ae_lumis shape\", ae_lumis.shape)\n",
    "# print(\"XTEEEEST before PerPD json beginn\")\n",
    "# print(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "json_file_path_PD = '/afs/cern.ch/user/f/fsiroky/Documents/gen_config/jsons/JetHT.json'\n",
    "\n",
    "def add_flags_from_json_PD(output_json, X_test):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([X_test.shape[0],1])\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        orig_runid = X_test[2803+7][i+last_train_idx]\n",
    "        # orig_runid = int(orig_runid)\n",
    "        orig_lumid = X_test[2804+7][i+last_train_idx]\n",
    "        # orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    X_test['PD_json'] = new_json_class\n",
    "    return X_test\n",
    "\n",
    "new_data = add_flags_from_json_PD(json_file_path_PD, X_test)\n",
    "\n",
    "del(new_data)\n",
    "# print(\"Now new X_test label\")\n",
    "# print(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#y_test = X_test['qlabel']\n",
    "\n",
    "y_test = X_test['PD_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis in X_test:  32076\n",
      "Number of bad lumis in X_test:  661\n"
     ]
    }
   ],
   "source": [
    "#Dropping labels before training and saving Test set lumisections\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of good lumis in X_test: \", len(X_test[y_test==0]))\n",
    "print(\"Number of bad lumis in X_test: \",  len(X_test[y_test==1]))\n",
    "\n",
    "X_test.drop(['qlabel'], axis=1, inplace=True)\n",
    "X_test.drop(['PD_json'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "X_train.drop(2803+7, axis=1, inplace=True) #drop RunID before training\n",
    "X_train.drop(2804+7, axis=1, inplace=True) #drop LumiID before training\n",
    "X_test.drop(2803+7, axis=1, inplace=True) #drop RunID before training\n",
    "X_test.drop(2804+7, axis=1, inplace=True) #drop LumiID before training\n",
    "\n",
    "\n",
    "# print(\"X_test before saving: \", X_test)\n",
    "\n",
    "luminosity_vals = lumisections.iloc[:int(last_train_idx)].values\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/X_testfor3pds_model.npy', X_test)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/y_testfor3pds_model.npy', y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "# def custom_activation(x):\n",
    "#     return ((((x**2+1)**(.5) - 1) / 2 ) + x)\n",
    "\n",
    "# get_custom_objects().update({'custom_activation': custom_activation})\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 1000\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "# prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# encoder = Dense(2600, #activation=\"custom_activation\",\n",
    "# # kernel_regularizer=regularizers.l2(0.005),\n",
    "#                 activity_regularizer=regularizers.l1(10e-5) \n",
    "#                               )(input_layer)\n",
    "# encoder = prellll(encoder)\n",
    "\n",
    "# encoder = prellll(encoder)\n",
    "# luminosity_neuron = Input(shape=(1,))\n",
    "\n",
    "# luminosity_neuron_dense = Dense(1,)(luminosity_neuron)\n",
    "\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# encoded = Dense(2200, #activation=\"relu\", \n",
    "# # kernel_regularizer=regularizers.l2(0.005),\n",
    "#                 # activity_regularizer=regularizers.l1(10e-5)    \n",
    "#                 )(encoder)\n",
    "# encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "# encoded = Dense(2600, activation='relu')(encoder)\n",
    "\n",
    "# x = concatenate([encoded, luminosity_neuron_dense])\n",
    "# prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "prellll = LeakyReLU(alpha=0.3)\n",
    "encoded = Dense(encoding_dim, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l2(10e-5),\n",
    "                # activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(input_layer)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "# luminosity_neuron = Input(shape=(1,), name='l_neu')\n",
    "# decoded = Dense(2600, activation='relu')(encoded)\n",
    "\n",
    "# x = concatenate([decoded, luminosity_neuron])\n",
    "\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# decoded = Dense(2200, # activation='relu',\n",
    "#     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# )(encoded)\n",
    "# decoded = prellll(decoded)\n",
    "\n",
    "\n",
    "# prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# decoded = Dense(2600, # activation='relu',\n",
    "#     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# )(encoded)\n",
    "# decoded = prellll(decoded)\n",
    "\n",
    "# encoder = Dense(int(encoding_dim / 1.2), activation=\"relu\")(encoder)\n",
    "\n",
    "# encoder = Dense(int(encoding_dim / 1.5), activation=\"relu\")(encoder)\n",
    "\n",
    "# decoder = Dense(2000, activation='relu')(encoded)\n",
    "# prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "prellll = LeakyReLU(alpha=0.3)\n",
    "\n",
    "decoder = Dense(input_dim)(encoded)\n",
    "decoder = prellll(decoder)\n",
    "# decoder = Dense(input_dim)(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, encoding_dim, activation, activation2, regularizer):\n",
    "    input_layer = Input(shape=(input_dim, ), name=\"Input\")\n",
    "    encoded = Dense(encoding_dim, kernel_regularizer=regularizer, name=\"First_Hidden\")(input_layer)\n",
    "    encoded = activation(encoded)\n",
    "    decoder = Dense(input_dim, name=\"Output\")(encoded)\n",
    "    decoder = activation2(decoder)\n",
    "    return Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_foo(input_dim, encoding_dim, activation, activation2, reg_val):\n",
    "    models = []\n",
    "    for x in [None, regularizers.l2(reg_val), regularizers.l1(reg_val)]:\n",
    "        models.append(get_model(X_train.shape[1], encoding_dim, activation, activation2, x))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# MODEL A\n",
    "activation = LeakyReLU(alpha=0.3, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.3, name=\"Second_Activation\")\n",
    "autoencoderA = get_model_foo(X_train.shape[1], 100, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL B\n",
    "activation = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "activation2 = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "autoencoderB = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL C\n",
    "activation = LeakyReLU(alpha=0.1, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.1, name=\"Second_Activation\")\n",
    "autoencoderC = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL D\n",
    "activation = LeakyReLU(alpha=0.6, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.6, name=\"Second_Activation\")\n",
    "autoencoderD = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL E\n",
    "from keras.layers import Activation\n",
    "activation = Activation(\"linear\", name=\"First_Activation\")\n",
    "activation2 = Activation(\"linear\", name=\"Second_Activation\")\n",
    "autoencoderE = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL F\n",
    "activation = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "activation2 = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "autoencoderF = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 2810)              0         \n",
      "_________________________________________________________________\n",
      "First_Hidden (Dense)         (None, 100)               281100    \n",
      "_________________________________________________________________\n",
      "First_Activation (LeakyReLU) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2810)              283810    \n",
      "_________________________________________________________________\n",
      "Second_Activation (LeakyReLU (None, 2810)              0         \n",
      "=================================================================\n",
      "Total params: 564,910\n",
      "Trainable params: 564,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 2810)              0         \n",
      "_________________________________________________________________\n",
      "First_Hidden (Dense)         (None, 100)               281100    \n",
      "_________________________________________________________________\n",
      "First_Activation (LeakyReLU) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2810)              283810    \n",
      "_________________________________________________________________\n",
      "Second_Activation (LeakyReLU (None, 2810)              0         \n",
      "=================================================================\n",
      "Total params: 564,910\n",
      "Trainable params: 564,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 2810)              0         \n",
      "_________________________________________________________________\n",
      "First_Hidden (Dense)         (None, 100)               281100    \n",
      "_________________________________________________________________\n",
      "First_Activation (LeakyReLU) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2810)              283810    \n",
      "_________________________________________________________________\n",
      "Second_Activation (LeakyReLU (None, 2810)              0         \n",
      "=================================================================\n",
      "Total params: 564,910\n",
      "Trainable params: 564,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for x in autoencoderA:\n",
    "    x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=256):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "            \n",
    "            valuename = validation_set_name + '_loss'\n",
    "            print(\"test_loss: \",results)\n",
    "            self.history.setdefault(valuename, []).append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 115710 samples, validate on 12857 samples\n",
      "Epoch 1/8192\n",
      "test_loss:  0.498545428226\n",
      "30s - loss: 0.2802 - val_loss: 0.1900\n",
      "Epoch 2/8192\n",
      "test_loss:  1.25017296016\n",
      "25s - loss: 0.2467 - val_loss: 0.2033\n",
      "Epoch 3/8192\n",
      "test_loss:  2.47545369415\n",
      "25s - loss: 0.2804 - val_loss: 0.4680\n",
      "Epoch 4/8192\n",
      "test_loss:  12.7425953819\n",
      "25s - loss: 0.5287 - val_loss: 0.7255\n",
      "Epoch 5/8192\n",
      "test_loss:  26.6313917168\n",
      "24s - loss: 0.8974 - val_loss: 0.5077\n",
      "Epoch 6/8192\n",
      "test_loss:  104.102971094\n",
      "25s - loss: 0.7224 - val_loss: 1.3529\n",
      "Epoch 7/8192\n",
      "test_loss:  199.574129669\n",
      "25s - loss: 0.4317 - val_loss: 2.2502\n",
      "Epoch 8/8192\n",
      "test_loss:  181.444947585\n",
      "25s - loss: 0.3095 - val_loss: 2.4107\n",
      "Epoch 9/8192\n",
      "test_loss:  219.648642005\n",
      "25s - loss: 0.2177 - val_loss: 2.5140\n",
      "Epoch 10/8192\n",
      "test_loss:  199.560086952\n",
      "24s - loss: 0.2229 - val_loss: 1.2657\n",
      "Epoch 11/8192\n",
      "test_loss:  183.994814043\n",
      "25s - loss: 0.1352 - val_loss: 1.3763\n",
      "Epoch 12/8192\n",
      "test_loss:  175.000916218\n",
      "25s - loss: 0.1133 - val_loss: 1.1116\n",
      "Epoch 13/8192\n",
      "test_loss:  177.755747381\n",
      "23s - loss: 0.1042 - val_loss: 1.3441\n",
      "Epoch 14/8192\n",
      "test_loss:  162.533072336\n",
      "24s - loss: 0.0960 - val_loss: 1.0621\n",
      "Epoch 15/8192\n",
      "test_loss:  183.63486899\n",
      "24s - loss: 0.0930 - val_loss: 1.3184\n",
      "Epoch 16/8192\n",
      "test_loss:  199.274202353\n",
      "25s - loss: 0.0983 - val_loss: 1.1806\n",
      "Epoch 17/8192\n",
      "test_loss:  206.552949564\n",
      "25s - loss: 0.1052 - val_loss: 1.3459\n",
      "Epoch 18/8192\n",
      "test_loss:  210.006685092\n",
      "24s - loss: 0.0987 - val_loss: 1.1040\n",
      "Epoch 19/8192\n",
      "test_loss:  121.527629975\n",
      "23s - loss: 0.1398 - val_loss: 1.0359\n",
      "Epoch 20/8192\n",
      "test_loss:  165.861766237\n",
      "24s - loss: 0.1078 - val_loss: 0.9075\n",
      "Epoch 21/8192\n",
      "test_loss:  179.897275273\n",
      "26s - loss: 0.1414 - val_loss: 1.6929\n",
      "Epoch 22/8192\n",
      "test_loss:  227.393320481\n",
      "25s - loss: 0.1295 - val_loss: 1.3845\n",
      "Epoch 23/8192\n",
      "test_loss:  809.877286645\n",
      "25s - loss: 0.2229 - val_loss: 7.0660\n",
      "Epoch 24/8192\n",
      "test_loss:  815.075353838\n",
      "25s - loss: 0.1939 - val_loss: 4.6351\n",
      "Epoch 25/8192\n",
      "test_loss:  976.966313872\n",
      "25s - loss: 0.2699 - val_loss: 4.4159\n",
      "Epoch 26/8192\n",
      "test_loss:  1037.94614232\n",
      "25s - loss: 0.1782 - val_loss: 4.1940\n",
      "Epoch 27/8192\n",
      "test_loss:  1356.20710082\n",
      "25s - loss: 0.1478 - val_loss: 8.5937\n",
      "Epoch 28/8192\n",
      "test_loss:  1124.33318658\n",
      "24s - loss: 0.1441 - val_loss: 5.1116\n",
      "Epoch 29/8192\n",
      "test_loss:  1124.08457749\n",
      "25s - loss: 0.1260 - val_loss: 5.6481\n",
      "Epoch 30/8192\n",
      "test_loss:  1356.63429632\n",
      "25s - loss: 0.1096 - val_loss: 8.7395\n",
      "Epoch 31/8192\n",
      "test_loss:  1356.95290824\n",
      "25s - loss: 0.1025 - val_loss: 7.6043\n",
      "Epoch 32/8192\n",
      "test_loss:  1535.74252362\n",
      "24s - loss: 0.1042 - val_loss: 7.6276\n",
      "Epoch 33/8192\n",
      "test_loss:  1892.92828931\n",
      "25s - loss: 0.1157 - val_loss: 11.3309\n",
      "Epoch 34/8192\n",
      "test_loss:  2082.89889111\n",
      "23s - loss: 0.1914 - val_loss: 11.1928\n",
      "Epoch 00033: early stopping\n",
      "Train on 115710 samples, validate on 12857 samples\n",
      "Epoch 1/8192\n",
      "test_loss:  0.537842091483\n",
      "26s - loss: 0.2567 - val_loss: 0.2757\n",
      "Epoch 2/8192\n",
      "test_loss:  0.532857699341\n",
      "25s - loss: 0.2681 - val_loss: 0.2834\n",
      "Epoch 3/8192\n",
      "test_loss:  0.530604954132\n",
      "26s - loss: 0.2064 - val_loss: 0.2289\n",
      "Epoch 4/8192\n",
      "test_loss:  1.14308800466\n",
      "25s - loss: 0.2329 - val_loss: 0.3860\n",
      "Epoch 5/8192\n",
      "test_loss:  0.64676178045\n",
      "25s - loss: 0.2453 - val_loss: 0.2764\n",
      "Epoch 6/8192\n",
      "test_loss:  0.523257468527\n",
      "25s - loss: 0.3063 - val_loss: 0.2685\n",
      "Epoch 7/8192\n",
      "test_loss:  0.534908442403\n",
      "25s - loss: 0.2860 - val_loss: 0.3080\n",
      "Epoch 8/8192\n",
      "test_loss:  0.490092141978\n",
      "24s - loss: 0.2080 - val_loss: 0.2578\n",
      "Epoch 9/8192\n",
      "test_loss:  0.505239690955\n",
      "25s - loss: 0.2201 - val_loss: 0.2655\n",
      "Epoch 10/8192\n",
      "test_loss:  0.472081021164\n",
      "25s - loss: 0.1848 - val_loss: 0.2515\n",
      "Epoch 11/8192\n",
      "test_loss:  0.502503452455\n",
      "25s - loss: 0.1683 - val_loss: 0.2838\n",
      "Epoch 12/8192\n",
      "test_loss:  0.453320987291\n",
      "25s - loss: 0.1663 - val_loss: 0.2451\n",
      "Epoch 13/8192\n",
      "test_loss:  0.439694644606\n",
      "26s - loss: 0.1343 - val_loss: 0.2275\n",
      "Epoch 14/8192\n",
      "test_loss:  0.427043293843\n",
      "25s - loss: 0.1258 - val_loss: 0.2905\n",
      "Epoch 15/8192\n",
      "test_loss:  0.423834840028\n",
      "21s - loss: 0.1189 - val_loss: 0.2045\n",
      "Epoch 16/8192\n",
      "test_loss:  0.426702074321\n",
      "24s - loss: 0.1443 - val_loss: 0.2095\n",
      "Epoch 17/8192\n",
      "test_loss:  0.707139260369\n",
      "23s - loss: 0.1806 - val_loss: 0.7429\n",
      "Epoch 18/8192\n",
      "test_loss:  0.465584555888\n",
      "25s - loss: 0.2532 - val_loss: 0.2478\n",
      "Epoch 19/8192\n",
      "test_loss:  0.454729047278\n",
      "26s - loss: 0.1548 - val_loss: 0.2325\n",
      "Epoch 20/8192\n",
      "test_loss:  0.39738247768\n",
      "26s - loss: 0.1611 - val_loss: 0.2031\n",
      "Epoch 21/8192\n",
      "test_loss:  0.466740806239\n",
      "26s - loss: 0.2383 - val_loss: 0.2470\n",
      "Epoch 22/8192\n",
      "test_loss:  0.639841182726\n",
      "25s - loss: 0.2336 - val_loss: 0.2971\n",
      "Epoch 23/8192\n",
      "test_loss:  0.915358809306\n",
      "26s - loss: 0.2397 - val_loss: 0.4684\n",
      "Epoch 24/8192\n",
      "test_loss:  0.656081561906\n",
      "26s - loss: 0.2528 - val_loss: 0.3473\n",
      "Epoch 25/8192\n",
      "test_loss:  0.641730079078\n",
      "25s - loss: 0.2906 - val_loss: 0.3421\n",
      "Epoch 26/8192\n",
      "test_loss:  0.730051668387\n",
      "26s - loss: 0.2345 - val_loss: 0.3558\n",
      "Epoch 27/8192\n",
      "test_loss:  0.555474320211\n",
      "27s - loss: 0.2574 - val_loss: 0.2909\n",
      "Epoch 28/8192\n",
      "test_loss:  0.556502749241\n",
      "26s - loss: 0.2432 - val_loss: 0.3089\n",
      "Epoch 29/8192\n",
      "test_loss:  0.533930007924\n",
      "26s - loss: 0.2119 - val_loss: 0.2858\n",
      "Epoch 30/8192\n",
      "test_loss:  0.530785096368\n",
      "26s - loss: 0.1679 - val_loss: 0.2729\n",
      "Epoch 31/8192\n",
      "test_loss:  0.449842138449\n",
      "24s - loss: 0.1723 - val_loss: 0.2411\n",
      "Epoch 32/8192\n",
      "test_loss:  0.433972645724\n",
      "25s - loss: 0.1393 - val_loss: 0.2268\n",
      "Epoch 33/8192\n",
      "test_loss:  0.442689885055\n",
      "26s - loss: 0.1419 - val_loss: 0.2152\n",
      "Epoch 34/8192\n",
      "test_loss:  0.432324346867\n",
      "25s - loss: 0.1273 - val_loss: 0.2092\n",
      "Epoch 35/8192\n",
      "test_loss:  0.397775395574\n",
      "27s - loss: 0.1393 - val_loss: 0.1902\n",
      "Epoch 36/8192\n",
      "test_loss:  0.402084795003\n",
      "26s - loss: 0.1416 - val_loss: 0.1983\n",
      "Epoch 37/8192\n",
      "test_loss:  0.402494907468\n",
      "27s - loss: 0.1338 - val_loss: 0.2027\n",
      "Epoch 38/8192\n",
      "test_loss:  0.411199811365\n",
      "26s - loss: 0.1439 - val_loss: 0.1994\n",
      "Epoch 39/8192\n",
      "test_loss:  0.402407638857\n",
      "25s - loss: 0.1517 - val_loss: 0.1925\n",
      "Epoch 40/8192\n",
      "test_loss:  0.437474835709\n",
      "25s - loss: 0.1867 - val_loss: 0.2236\n",
      "Epoch 41/8192\n",
      "test_loss:  0.497726565001\n",
      "23s - loss: 0.1638 - val_loss: 0.2370\n",
      "Epoch 42/8192\n",
      "test_loss:  0.502237125831\n",
      "23s - loss: 0.2459 - val_loss: 0.2246\n",
      "Epoch 43/8192\n",
      "test_loss:  0.657992636429\n",
      "25s - loss: 0.1820 - val_loss: 0.3553\n",
      "Epoch 44/8192\n",
      "test_loss:  0.427197531215\n",
      "26s - loss: 0.1641 - val_loss: 0.2071\n",
      "Epoch 45/8192\n",
      "test_loss:  0.396540896341\n",
      "25s - loss: 0.1395 - val_loss: 0.1978\n",
      "Epoch 46/8192\n",
      "test_loss:  0.385415823038\n",
      "27s - loss: 0.1199 - val_loss: 0.1892\n",
      "Epoch 47/8192\n",
      "test_loss:  0.390516812724\n",
      "27s - loss: 0.1172 - val_loss: 0.1872\n",
      "Epoch 48/8192\n",
      "test_loss:  0.405221265249\n",
      "25s - loss: 0.1110 - val_loss: 0.2009\n",
      "Epoch 49/8192\n",
      "test_loss:  0.366957447769\n",
      "25s - loss: 0.1535 - val_loss: 0.1739\n",
      "Epoch 50/8192\n",
      "test_loss:  0.372339896482\n",
      "26s - loss: 0.1198 - val_loss: 0.1687\n",
      "Epoch 51/8192\n",
      "test_loss:  0.37997323368\n",
      "26s - loss: 0.1271 - val_loss: 0.1818\n",
      "Epoch 52/8192\n",
      "test_loss:  0.356414607925\n",
      "26s - loss: 0.1358 - val_loss: 0.1604\n",
      "Epoch 53/8192\n",
      "test_loss:  0.363686602627\n",
      "25s - loss: 0.1366 - val_loss: 0.1753\n",
      "Epoch 54/8192\n",
      "test_loss:  0.384001033607\n",
      "25s - loss: 0.1193 - val_loss: 0.1828\n",
      "Epoch 55/8192\n",
      "test_loss:  0.364952330186\n",
      "26s - loss: 0.1267 - val_loss: 0.1772\n",
      "Epoch 56/8192\n",
      "test_loss:  0.393776506016\n",
      "25s - loss: 0.1750 - val_loss: 0.1837\n",
      "Epoch 57/8192\n",
      "test_loss:  0.392679552905\n",
      "26s - loss: 0.1353 - val_loss: 0.1861\n",
      "Epoch 58/8192\n",
      "test_loss:  0.401339969406\n",
      "25s - loss: 0.1439 - val_loss: 0.1839\n",
      "Epoch 59/8192\n",
      "test_loss:  0.401862188206\n",
      "25s - loss: 0.1705 - val_loss: 0.1938\n",
      "Epoch 60/8192\n",
      "test_loss:  0.533706166635\n",
      "25s - loss: 0.2163 - val_loss: 0.2340\n",
      "Epoch 61/8192\n",
      "test_loss:  3.04568055305\n",
      "25s - loss: 0.1999 - val_loss: 0.6240\n",
      "Epoch 62/8192\n",
      "test_loss:  0.902715648867\n",
      "25s - loss: 0.4437 - val_loss: 0.5114\n",
      "Epoch 63/8192\n",
      "test_loss:  4.3734539307\n",
      "24s - loss: 0.3939 - val_loss: 1.4220\n",
      "Epoch 64/8192\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 8192\n",
    "batch_size = 256\n",
    "from keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=32,\n",
    "                                  verbose=True,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "\n",
    "for indx1, group in enumerate ([autoencoderB,autoencoderC,autoencoderD,autoencoderE,autoencoderF]): \n",
    "    for indx2, autoencoder in enumerate (group):\n",
    "        name = (\"group%s_autoencoder%s\" % (indx1, indx2))\n",
    "        \n",
    "        autoencoder.compile(optimizer='Adam', \n",
    "                            loss='mean_squared_error'\n",
    "                            # metrics=['accuracy']\n",
    "                           )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint((\"/afs/cern.ch/user/f/fsiroky/models_ae/%s.h5\" % name),\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  verbose=False,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"min\")\n",
    "        testerror = AdditionalValidationSets([(X_test, X_test, 'test')])\n",
    "        history = autoencoder.fit(X_train, X_train,\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1,\n",
    "                            verbose=2,\n",
    "                            callbacks=[testerror, early_stopper, checkpoint_callback]).history\n",
    "\n",
    "        #np.save('/eos/cms/store/user/fsiroky/ae_models/%s.npy' % name, history)\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_loss.npy' % name , history['loss'])\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_valloss.npy' % name, history['val_loss'])\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_testloss.npy' % name , testerror.history['test_loss'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "100-80-50-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "50 - 10 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
