{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import setGPU\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from sklearn.utils import shuffle\n",
    "import h5py\n",
    "\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Anomalous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, RobustScaler, normalize, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature names\n",
    "var_names_reduced5 = ['qPFJetPt', 'qPFJetEta', 'qPFJetPhi', 'qPFJet0Pt', 'qPFJet1Pt', 'qPFJet2Pt', 'qPFJet3Pt', 'qPFJet4Pt', 'qPFJet5Pt', 'qPFJet0Eta', 'qPFJet1Eta', 'qPFJet2Eta', 'qPFJet3Eta', 'qPFJet4Eta', 'qPFJet5Eta', 'qPFJet0Phi', 'qPFJet1Phi', 'qPFJet2Phi', 'qPFJet3Phi', 'qPFJet4Phi', 'qPFJet5Phi', 'qPFJet4CHS0Pt', 'qPFJet4CHS1Pt', 'qPFJet4CHS2Pt', 'qPFJet4CHS3Pt', 'qPFJet4CHS4Pt', 'qPFJet4CHS5Pt', 'qPFJet4CHS0Eta', 'qPFJet4CHS1Eta', 'qPFJet4CHS2Eta', 'qPFJet4CHS3Eta', 'qPFJet4CHS4Eta', 'qPFJet4CHS5Eta', 'qPFJet4CHS0Phi', 'qPFJet4CHS1Phi', 'qPFJet4CHS2Phi', 'qPFJet4CHS3Phi', 'qPFJet4CHS4Phi', 'qPFJet4CHS5Phi', 'qPFJet8CHS0Pt', 'qPFJet8CHS1Pt', 'qPFJet8CHS2Pt', 'qPFJet8CHS3Pt', 'qPFJet8CHS4Pt', 'qPFJet8CHS5Pt', 'qPFJet8CHS0Eta', 'qPFJet8CHS1Eta', 'qPFJet8CHS2Eta', 'qPFJet8CHS3Eta', 'qPFJet8CHS4Eta', 'qPFJet8CHS5Eta', 'qPFJet8CHS0Phi', 'qPFJet8CHS1Phi', 'qPFJet8CHS2Phi', 'qPFJet8CHS3Phi', 'qPFJet8CHS4Phi', 'qPFJet8CHS5Phi', 'qPFJetEI0Pt', 'qPFJetEI1Pt', 'qPFJetEI2Pt', 'qPFJetEI3Pt', 'qPFJetEI4Pt', 'qPFJetEI5Pt', 'qPFJetEI0Eta', 'qPFJetEI1Eta', 'qPFJetEI2Eta', 'qPFJetEI3Eta', 'qPFJetEI4Eta', 'qPFJetEI5Eta', 'qPFJetEI0Phi', 'qPFJetEI1Phi', 'qPFJetEI2Phi', 'qPFJetEI3Phi', 'qPFJetEI4Phi', 'qPFJetEI5Phi', 'qPFJet8CHSSD0Pt', 'qPFJet8CHSSD1Pt', 'qPFJet8CHSSD2Pt', 'qPFJet8CHSSD3Pt', 'qPFJet8CHSSD4Pt', 'qPFJet8CHSSD5Pt', 'qPFJet8CHSSD0Eta', 'qPFJet8CHSSD1Eta', 'qPFJet8CHSSD2Eta', 'qPFJet8CHSSD3Eta', 'qPFJet8CHSSD4Eta', 'qPFJet8CHSSD5Eta', 'qPFJet8CHSSD0Phi', 'qPFJet8CHSSD1Phi', 'qPFJet8CHSSD2Phi', 'qPFJet8CHSSD3Phi', 'qPFJet8CHSSD4Phi', 'qPFJet8CHSSD5Phi', 'qPFJetTopCHS0Pt', 'qPFJetTopCHS1Pt', 'qPFJetTopCHS2Pt', 'qPFJetTopCHS3Pt', 'qPFJetTopCHS4Pt', 'qPFJetTopCHS5Pt', 'qPFJetTopCHS0Eta', 'qPFJetTopCHS1Eta', 'qPFJetTopCHS2Eta', 'qPFJetTopCHS3Eta', 'qPFJetTopCHS4Eta', 'qPFJetTopCHS5Eta', 'qPFJetTopCHS0Phi', 'qPFJetTopCHS1Phi', 'qPFJetTopCHS2Phi', 'qPFJetTopCHS3Phi', 'qPFJetTopCHS4Phi', 'qPFJetTopCHS5Phi', 'qCalJet0Pt', 'qCalJet1Pt', 'qCalJet2Pt', 'qCalJet3Pt', 'qCalJet4Pt', 'qCalJet5Pt', 'qCalJet0Eta', 'qCalJet1Eta', 'qCalJet2Eta', 'qCalJet3Eta', 'qCalJet4Eta', 'qCalJet5Eta', 'qCalJet0Phi', 'qCalJet1Phi', 'qCalJet2Phi', 'qCalJet3Phi', 'qCalJet4Phi', 'qCalJet5Phi', 'qCalJet0En', 'qCalJet1En', 'qCalJet2En', 'qCalJet3En', 'qCalJet4En', 'qCalJet5En', 'qPho0Pt', 'qPho1Pt', 'qPho2Pt', 'qPho3Pt', 'qPho4Pt', 'qPho5Pt', 'qPho0Eta', 'qPho1Eta', 'qPho2Eta', 'qPho3Eta', 'qPho4Eta', 'qPho5Eta', 'qPho0Phi', 'qPho1Phi', 'qPho2Phi', 'qPho3Phi', 'qPho4Phi', 'qPho5Phi', 'qPho0En', 'qPho1En', 'qPho2En', 'qPho3En', 'qPho4En', 'qPho5En', 'qgedPho0Pt', 'qgedPho1Pt', 'qgedPho2Pt', 'qgedPho3Pt', 'qgedPho4Pt', 'qgedPho5Pt', 'qgedPho0Eta', 'qgedPho1Eta', 'qgedPho2Eta', 'qgedPho3Eta', 'qgedPho4Eta', 'qgedPho5Eta', 'qgedPho0Phi', 'qgedPho1Phi', 'qgedPho2Phi', 'qgedPho3Phi', 'qgedPho4Phi', 'qgedPho5Phi', 'qgedPho0En', 'qgedPho1En', 'qgedPho2En', 'qgedPho3En', 'qgedPho4En', 'qgedPho5En', 'qMu0Pt', 'qMu1Pt', 'qMu2Pt', 'qMu3Pt', 'qMu4Pt', 'qMu5Pt', 'qMu0Eta', 'qMu1Eta', 'qMu2Eta', 'qMu3Eta', 'qMu4Eta', 'qMu5Eta', 'qMu0Phi', 'qMu1Phi', 'qMu2Phi', 'qMu3Phi', 'qMu4Phi', 'qMu5Phi', 'qMu0En', 'qMu1En', 'qMu2En', 'qMu3En', 'qMu4En', 'qMu5En', 'qMuCosm0Pt', 'qMuCosm1Pt', 'qMuCosm2Pt', 'qMuCosm3Pt', 'qMuCosm4Pt', 'qMuCosm5Pt', 'qMuCosm0Eta', 'qMuCosm1Eta', 'qMuCosm2Eta', 'qMuCosm3Eta', 'qMuCosm4Eta', 'qMuCosm5Eta', 'qMuCosm0Phi', 'qMuCosm1Phi', 'qMuCosm2Phi', 'qMuCosm3Phi', 'qMuCosm4Phi', 'qMuCosm5Phi', 'qMuCosm0En', 'qMuCosm1En', 'qMuCosm2En', 'qMuCosm3En', 'qMuCosm4En', 'qMuCosm5En', 'qMuCosmLeg0Pt', 'qMuCosmLeg1Pt', 'qMuCosmLeg2Pt', 'qMuCosmLeg3Pt', 'qMuCosmLeg4Pt', 'qMuCosmLeg5Pt', 'qMuCosmLeg0Eta', 'qMuCosmLeg1Eta', 'qMuCosmLeg2Eta', 'qMuCosmLeg3Eta', 'qMuCosmLeg4Eta', 'qMuCosmLeg5Eta', 'qMuCosmLeg0Phi', 'qMuCosmLeg1Phi', 'qMuCosmLeg2Phi', 'qMuCosmLeg3Phi', 'qMuCosmLeg4Phi', 'qMuCosmLeg5Phi', 'qMuCosmLeg0En', 'qMuCosmLeg1En', 'qMuCosmLeg2En', 'qMuCosmLeg3En', 'qMuCosmLeg4En', 'qMuCosmLeg5En', 'qPFJet4CHSPt', 'qPFJet4CHSEta', 'qPFJet4CHSPhi', 'qPFJet8CHSPt', 'qPFJet8CHSEta', 'qPFJet8CHSPhi', 'qPFJetEIPt', 'qPFJetEIEta', 'qPFJetEIPhi', 'qPFJet8CHSSDPt', 'qPFJet8CHSSDEta', 'qPFJet8CHSSDPhi', 'qPFJetTopCHSPt', 'qPFJetTopCHSEta', 'qPFJetTopCHSPhi', 'qPFChMetPt', 'qPFChMetPhi', 'qPFMetPt', 'qPFMetPhi', 'qNVtx', 'qCalJetPt', 'qCalJetEta', 'qCalJetPhi', 'qCalJetEn', 'qCalMETPt', 'qCalMETPhi', 'qCalMETEn', 'qCalMETBEPt', 'qCalMETBEPhi', 'qCalMETBEEn', 'qCalMETBEFOPt', 'qCalMETBEFOPhi', 'qCalMETBEFOEn', 'qCalMETMPt', 'qCalMETMPhi', 'qCalMETMEn', 'qSCEn', 'qSCEta', 'qSCPhi', 'qSCEtaWidth', 'qSCPhiWidth', 'qSCEnhfEM', 'qSCEtahfEM', 'qSCPhihfEM', 'qSCEn5x5', 'qSCEta5x5', 'qSCPhi5x5', 'qSCEtaWidth5x5', 'qSCPhiWidth5x5', 'qCCEn', 'qCCEta', 'qCCPhi', 'qCCEn5x5', 'qCCEta5x5', 'qCCPhi5x5', 'qPhoPt', 'qPhoEta', 'qPhoPhi', 'qPhoEn_', 'qPhoe1x5_', 'qPhoe2x5_', 'qPhoe3x3_', 'qPhoe5x5_', 'qPhomaxenxtal_', 'qPhosigmaeta_', 'qPhosigmaIeta_', 'qPhor1x5_', 'qPhor2x5_', 'qPhor9_', 'qgedPhoPt', 'qgedPhoEta', 'qgedPhoPhi', 'qgedPhoEn_', 'qgedPhoe1x5_', 'qgedPhoe2x5_', 'qgedPhoe3x3_', 'qgedPhoe5x5_', 'qgedPhomaxenxtal_', 'qgedPhosigmaeta_', 'qgedPhosigmaIeta_', 'qgedPhor1x5_', 'qgedPhor2x5_', 'qgedPhor9_', 'qMuPt', 'qMuEta', 'qMuPhi', 'qMuEn_', 'qMuCh_', 'qMuChi2_', 'qMuCosmPt', 'qMuCosmEta', 'qMuCosmPhi', 'qMuCosmEn_', 'qMuCosmCh_', 'qMuCosmChi2_', 'qMuCosmLegPt', 'qMuCosmLegEta', 'qMuCosmLegPhi', 'qMuCosmLegEn_', 'qMuCosmLegCh_', 'qMuCosmLegChi2_', 'qSigmaIEta', 'qSigmaIPhi', 'qr9', 'qHadOEm', 'qdrSumPt', 'qdrSumEt', 'qeSCOP', 'qecEn', 'qUNSigmaIEta', 'qUNSigmaIPhi', 'qUNr9', 'qUNHadOEm', 'qUNdrSumPt', 'qUNdrSumEt', 'qUNeSCOP', 'qUNecEn', 'qEBenergy', 'qEBtime', 'qEBchi2', 'qEBiEta', 'qEBiPhi', 'qEEenergy', 'qEEtime', 'qEEchi2', 'qEEix', 'qEEiy', 'qESenergy', 'qEStime', 'qESix', 'qESiy', 'qHBHEenergy', 'qHBHEtime', 'qHBHEauxe', 'qHBHEieta', 'qHBHEiphi', 'qHFenergy', 'qHFtime', 'qHFieta', 'qHFiphi', 'qPreShEn', 'qPreShEta', 'qPreShPhi', 'qPreShYEn', 'qPreShYEta', 'qPreShYPhi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Authenticate in order to get permission for eos\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load h5 files\n",
    "\n",
    "\n",
    "#Choose where to load the files from\n",
    "# b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "# b_h5       = '/eos/cms/store/user/fsiroky/lumih5/'\n",
    "b_h5 = '/eos/cms/store/user/fsiroky/consistentlumih5/'\n",
    "# b_h5   = '/afs/cern.ch/user/f/fsiroky/public/'\n",
    "# b_h5 = '/mnt/hdf5test/'\n",
    "# b_h5   = '/home/test_local/'\n",
    "\n",
    "pds  = {1: 'BTagCSV', 2: 'BTagMu', 3: 'Charmonium', 4:'DisplacedJet', 5: 'DoubleEG',\n",
    "        6: 'DoubleMuon', 7: 'DoubleMuonLowMass',\n",
    "       # 8: 'FSQJets', 9: 'HighMultiplicityEOF', #NOT ENOUGH DATA, NOTEBOOK FAILES\n",
    "        10: 'HTMHT', 11: 'JetHT', 12: 'MET',\n",
    "       # 13: 'MinimumBias', #NOT ENOUGH DATA\n",
    "        14: 'MuonEG', 15: 'MuOnia',\n",
    "       # 16: 'NoBPTX',\n",
    "        17: 'SingleElectron', 18: 'SingleMuon', 19: 'SinglePhoton', 20: 'Tau', 21: 'ZeroBias'\n",
    "}\n",
    "\n",
    "      \n",
    "def get_jets(bg_files, bg_jets, sig_files, sig_jets):\n",
    "    #Use np.empty([0,2802]) for both good and bad jets, if you use b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "    good_jets = np.empty([0,2813])\n",
    "    bad_jets  = np.empty([0,2813])\n",
    "                   # Control which time intervals files per PD to load with range in the for loop\n",
    "    for i in range(0,len(bg_files)):   #0\n",
    "        try:\n",
    "            bg_jetfile  = h5py.File(bg_files[i],'r')\n",
    "            bg_jet      = bg_jetfile[bg_jets[i]][:]\n",
    "            sig_jetfile = h5py.File(sig_files[i],'r')\n",
    "            sig_jet     = sig_jetfile[sig_jets[i]][:]\n",
    "            # print(bad_jets.shape, bg_jet.shape)\n",
    "            bad_jets    = np.concatenate((bad_jets, bg_jet), axis=0)\n",
    "            good_jets = np.concatenate((good_jets, sig_jet), axis=0)\n",
    "            print( \"Number of good lumis: \", len(sig_jet), \" Number of bad lumis: \", len(bg_jet)) \n",
    "\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have \", bg_jets[i], error )\n",
    "            continue\n",
    "    return good_jets, bad_jets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis:  17238  Number of bad lumis:  492\n",
      "Number of good lumis:  26782  Number of bad lumis:  121\n",
      "Number of good lumis:  15558  Number of bad lumis:  638\n",
      "Number of good lumis:  17901  Number of bad lumis:  70\n",
      "Number of good lumis:  40180  Number of bad lumis:  201\n",
      "Number of good lumis:  41347  Number of bad lumis:  3156\n"
     ]
    }
   ],
   "source": [
    "#Choose which PD to load\n",
    "nbr = 11 #Jvariable\n",
    "\n",
    "bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "#Load good and bad jets\n",
    "good_jets, bad_jets = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 3 #Charmonium\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets2, bad_jets2 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 15 #\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets3, bad_jets3 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 14\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets4, bad_jets4 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "#Assign good jets class label 0\n",
    "df1 = pd.DataFrame(good_jets)\n",
    "# cutted_df = df1.iloc[0:25000, :]   #Temporarily to make training faster\n",
    "# df1 = cutted_df                   #Temporarily to make training faster\n",
    "df1['class'] = 0\n",
    "\n",
    "#Assign bad_jets class label  1\n",
    "df2 = pd.DataFrame(bad_jets)\n",
    "# cutted_df = df2.iloc[0:, :]    #Temporarily to make training faster\n",
    "# df2 = cutted_df                   #Temporarily to make training faster\n",
    "df2['class'] = 1\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df3 = pd.DataFrame(good_jets2)\n",
    "\n",
    "# df3['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df4 = pd.DataFrame(bad_jets2)\n",
    "\n",
    "# df4['class'] = 1\n",
    "\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df5 = pd.DataFrame(good_jets3)\n",
    "\n",
    "# df5['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df6 = pd.DataFrame(bad_jets3)\n",
    "\n",
    "# df6['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df7 = pd.DataFrame(good_jets4)\n",
    "# df7['class'] = 0\n",
    "\n",
    "# df8 = pd.DataFrame(bad_jets4)\n",
    "# df8['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# del(good_jets)\n",
    "# del(bad_jets)\n",
    "#Concatenate them\n",
    "frames  = [df1,df2] \n",
    "#frames = [df1,df2,df3,df4,df5,df6]\n",
    "# frames = [df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "data   = pd.concat(frames)\n",
    "del(frames)\n",
    "# del(df1)\n",
    "# del(df2)\n",
    "\n",
    "data.drop(2805+7, axis=1, inplace=True) #Drop per_pd flags\n",
    "\n",
    "data = data.sort_values([2800+7,2801+7], ascending=[True,True]) #Sort by runID and then by lumiID\n",
    "data = data.reset_index(drop=True)  #Reset index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = data.reindex(index=range(0,len(data)))\n",
    "#Shuffle them randomly\n",
    "# data = shuffle(data)\n",
    "# data = data.reset_index(drop=True)\n",
    "\n",
    "#Save labels and delete them from df not to cheat during training\n",
    "# labels = data['class'].astype(int)\n",
    "# del data['class']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Relabelling incorrect \"Fede json\" with updated one by current choice\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    outcome = 5\n",
    "    for k,v in json_file.items():\n",
    "        if (int(k) == orig_runid):\n",
    "            for d in v: #Checks each inner loop of the json per runID\n",
    "                for i in range (d[0], d[1]+1):\n",
    "#                     print(\"key of json is \", k, \" value of json is \", v)\n",
    "# #                     print(v[0][0], \"and\", v[0][1])\n",
    "#                     print(\"current inner list is\", d, \"and range is\", d[0], \" to \", d[1])\n",
    "#                     print(\"i is \", i)\n",
    "                    if i == orig_lumid:\n",
    "#                         print(\"Flagging as bad\")\n",
    "                        outcome =0  #0 means good lumi! (to be compatible with code anomaly_detection.ipynb[mse ae])\n",
    "                        return(outcome)\n",
    "                \n",
    "            \n",
    "        \n",
    "    outcome = 1 #1 means bad lumisection! (to be compatible with code anomaly_detection.ipynb [mse autoencoder])\n",
    "    return(outcome)\n",
    "\n",
    "json_file_path = '/afs/cern.ch/user/f/fsiroky/public/Cert_271036-284044_13TeV_PromptReco_Collisions16_JSON.txt'\n",
    "\n",
    "def add_flags_from_json(output_json, data):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([data.shape[0],1])\n",
    "    for i in range(0, data.shape[0]):\n",
    "        orig_runid = data[2800+7][i]\n",
    "        orig_runid = int(orig_runid)\n",
    "        orig_lumid = data[2801+7][i]\n",
    "        orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    data['preco_json'] = new_json_class #PromptReco GOLDEN json\n",
    "    return data\n",
    "\n",
    "new_data = add_flags_from_json(json_file_path, data)\n",
    " \n",
    "del(new_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO!\n",
    "#Check how many good lumis and anomalous ones we have\n",
    "\n",
    "# print(\"Laaalelaaa\", data)\n",
    "\n",
    "# anomalies = data[data['class'] == 1]\n",
    "# normal    = data[data['class'] == 0]\n",
    "\n",
    "# print(\"Number of anomalies: \", anomalies.shape)\n",
    "# del(anomalies)\n",
    "\n",
    "# print(\"Number of normals: \", normal.shape)\n",
    "# del(normal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save of RunIDs and LumiIDs done\n",
      "RunID and LumiID dropped\n"
     ]
    }
   ],
   "source": [
    "#Save runIDs and lumiIDs and instantaneous luminosities for later, because now we drop them before training\n",
    "\n",
    "runIDs  = data[2800+7].astype(int)\n",
    "lumiIDs = data[2801+7].astype(int)\n",
    "lumisections = data[2802+7].astype(float)\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datarunIDs.npy',  runIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datalumiIDs.npy', lumiIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/lumisections.npy', lumisections)\n",
    "\n",
    "\n",
    "print(\"Save of RunIDs and LumiIDs done\")\n",
    "\n",
    "# print(data)\n",
    "data.drop(2800+7, axis=1, inplace=True) #drop RunID before normalizing and training\n",
    "data.drop(2801+7, axis=1, inplace=True) #drop LumiID before normalizing and training\n",
    "print(\"RunID and LumiID dropped\")\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the data to make training better\n",
    "\n",
    "cutted_data = data.iloc[:, 0:2803+7]\n",
    "#classes     = data.iloc[:, 2805:2806] \n",
    "classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "\n",
    "# print(classes.shape)\n",
    "np_scaled = StandardScaler().fit_transform(cutted_data.values)\n",
    "# np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "datas = pd.DataFrame(np_scaled)\n",
    "# datas = pd.DataFrame(np_scaled, index=cutted_data.index, columns=cutted_data.columns)\n",
    "\n",
    "# print(\"2222\",datas)\n",
    "\n",
    "# del(np_scaled)\n",
    "del(cutted_data)\n",
    "# print(\"Datas first: \", datas)\n",
    "datas[2803+7] = runIDs   #Append runID back after scaling\n",
    "datas[2804+7] = lumiIDs  #Append lumiID back after scaling\n",
    "datas['qlabel'] = classes  #qlabel is goldenJSON now\n",
    "\n",
    "# print(\"After scale\", datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163684, 2813)\n"
     ]
    }
   ],
   "source": [
    "#TEST/TRAIN SPLIT\n",
    "\n",
    "# X_train, X_test = train_test_split(datas, test_size=0.15, random_state=RANDOM_SEED) # This works when we split rndmly\n",
    "split_nbr = round(datas.shape[0]*0.20)  #0.10 means 10% to the validation set\n",
    "\n",
    "print(datas.shape)\n",
    "X_train = datas.iloc[0:(datas.shape[0] - split_nbr) ,:]\n",
    "X_test  = datas.iloc[(datas.shape[0] - split_nbr): (datas.shape[0]) ,:]\n",
    "last_train_idx = X_train.shape[0]\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/last_train_idx.npy', last_train_idx)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "del(datas)\n",
    "X_train = X_train[X_train['qlabel']== 0]\n",
    "# print(X_train)\n",
    "X_train = X_train.drop(['qlabel'], axis=1)\n",
    "\n",
    "ae_lumis = X_train[2800+7].astype(float)\n",
    "# print(\"ae lumis\", ae_lumis, \"ae_lumis shape\", ae_lumis.shape)\n",
    "# print(\"XTEEEEST before PerPD json beginn\")\n",
    "# print(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "json_file_path_PD = '/afs/cern.ch/user/f/fsiroky/Documents/gen_config/jsons/JetHT.json'\n",
    "\n",
    "def add_flags_from_json_PD(output_json, X_test):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([X_test.shape[0],1])\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        orig_runid = X_test[2803+7][i+last_train_idx]\n",
    "        # orig_runid = int(orig_runid)\n",
    "        orig_lumid = X_test[2804+7][i+last_train_idx]\n",
    "        # orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    X_test['PD_json'] = new_json_class\n",
    "    return X_test\n",
    "\n",
    "new_data = add_flags_from_json_PD(json_file_path_PD, X_test)\n",
    "\n",
    "del(new_data)\n",
    "# print(\"Now new X_test label\")\n",
    "# print(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#y_test = X_test['qlabel']\n",
    "\n",
    "y_test = X_test['PD_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis in X_test:  32076\n",
      "Number of bad lumis in X_test:  661\n"
     ]
    }
   ],
   "source": [
    "#Dropping labels before training and saving Test set lumisections\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of good lumis in X_test: \", len(X_test[y_test==0]))\n",
    "print(\"Number of bad lumis in X_test: \",  len(X_test[y_test==1]))\n",
    "\n",
    "X_test.drop(['qlabel'], axis=1, inplace=True)\n",
    "X_test.drop(['PD_json'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "X_train.drop(2803+7, axis=1, inplace=True) #drop RunID before training\n",
    "X_train.drop(2804+7, axis=1, inplace=True) #drop LumiID before training\n",
    "X_test.drop(2803+7, axis=1, inplace=True) #drop RunID before training\n",
    "X_test.drop(2804+7, axis=1, inplace=True) #drop LumiID before training\n",
    "\n",
    "\n",
    "# print(\"X_test before saving: \", X_test)\n",
    "\n",
    "luminosity_vals = lumisections.iloc[:int(last_train_idx)].values\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/X_testfor3pds_model.npy', X_test)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/y_testfor3pds_model.npy', y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #TRAINING\n",
    "\n",
    "\n",
    "\n",
    "# from keras.layers import concatenate\n",
    "\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "# # def custom_activation(x):\n",
    "# #     return ((((x**2+1)**(.5) - 1) / 2 ) + x)\n",
    "\n",
    "# # get_custom_objects().update({'custom_activation': custom_activation})\n",
    "\n",
    "# input_dim = X_train.shape[1]\n",
    "# encoding_dim = 1000\n",
    "\n",
    "\n",
    "# input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # encoder = Dense(2600, #activation=\"custom_activation\",\n",
    "# # # kernel_regularizer=regularizers.l2(0.005),\n",
    "# #                 activity_regularizer=regularizers.l1(10e-5) \n",
    "# #                               )(input_layer)\n",
    "# # encoder = prellll(encoder)\n",
    "\n",
    "# # encoder = prellll(encoder)\n",
    "# # luminosity_neuron = Input(shape=(1,))\n",
    "\n",
    "# # luminosity_neuron_dense = Dense(1,)(luminosity_neuron)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # encoded = Dense(2200, #activation=\"relu\", \n",
    "# # # kernel_regularizer=regularizers.l2(0.005),\n",
    "# #                 # activity_regularizer=regularizers.l1(10e-5)    \n",
    "# #                 )(encoder)\n",
    "# # encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "# # encoded = Dense(2600, activation='relu')(encoder)\n",
    "\n",
    "# # x = concatenate([encoded, luminosity_neuron_dense])\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# encoded = Dense(encoding_dim, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "#                 # activity_regularizer=regularizers.l1(10e-5) \n",
    "#                    )(input_layer)\n",
    "# encoded = prellll(encoded)\n",
    "\n",
    "# # luminosity_neuron = Input(shape=(1,), name='l_neu')\n",
    "# # decoded = Dense(2600, activation='relu')(encoded)\n",
    "\n",
    "# # x = concatenate([decoded, luminosity_neuron])\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # decoded = Dense(2200, # activation='relu',\n",
    "# #     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# # )(encoded)\n",
    "# # decoded = prellll(decoded)\n",
    "\n",
    "\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # decoded = Dense(2600, # activation='relu',\n",
    "# #     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# # )(encoded)\n",
    "# # decoded = prellll(decoded)\n",
    "\n",
    "# # encoder = Dense(int(encoding_dim / 1.2), activation=\"relu\")(encoder)\n",
    "\n",
    "# # encoder = Dense(int(encoding_dim / 1.5), activation=\"relu\")(encoder)\n",
    "\n",
    "# # decoder = Dense(2000, activation='relu')(encoded)\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "\n",
    "# decoder = Dense(input_dim)(encoded)\n",
    "# decoder = prellll(decoder)\n",
    "# # decoder = Dense(input_dim)(encoded)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, encoding_dim, activation, activation2, regularizer):\n",
    "    input_layer = Input(shape=(input_dim, ), name=\"Input\")\n",
    "    encoded = Dense(encoding_dim, kernel_regularizer=regularizer, name=\"First_Hidden\")(input_layer)\n",
    "    encoded = activation(encoded)\n",
    "    decoder = Dense(input_dim, name=\"Output\")(encoded)\n",
    "    decoder = activation2(decoder)\n",
    "    return Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_foo(input_dim, encoding_dim, activation, activation2, reg_val):\n",
    "    models = []\n",
    "    for x in [None, regularizers.l2(reg_val), regularizers.l1(reg_val)]:\n",
    "        models.append(get_model(X_train.shape[1], encoding_dim, activation, activation2, x))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# MODEL A\n",
    "activation = LeakyReLU(alpha=0.3, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.3, name=\"Second_Activation\")\n",
    "autoencoderA = get_model_foo(X_train.shape[1], 100, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL B\n",
    "activation = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "activation2 = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "autoencoderB = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL C\n",
    "activation = LeakyReLU(alpha=0.1, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.1, name=\"Second_Activation\")\n",
    "autoencoderC = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL D\n",
    "activation = LeakyReLU(alpha=0.6, name=\"First_Activation\")\n",
    "activation2 = LeakyReLU(alpha=0.6, name=\"Second_Activation\")\n",
    "autoencoderD = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL E\n",
    "from keras.layers import Activation\n",
    "activation = Activation(\"linear\", name=\"First_Activation\")\n",
    "activation2 = Activation(\"linear\", name=\"Second_Activation\")\n",
    "autoencoderE = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# MODEL F\n",
    "activation = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "activation2 = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "autoencoderF = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in autoencoderA:\n",
    "    x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=256):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "            \n",
    "            valuename = validation_set_name + '_loss'\n",
    "            print(\"test_loss: \",results)\n",
    "            self.history.setdefault(valuename, []).append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 8192\n",
    "batch_size = 256\n",
    "from keras.optimizers import Adam, Nadam\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=32,\n",
    "                                  verbose=True,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "\n",
    "for indx1, group in enumerate ([autoencoderA,autoencoderB,autoencoderC,autoencoderD,autoencoderE,autoencoderF]): \n",
    "    for indx2, autoencoder in enumerate (group):\n",
    "        name = (\"group%s_autoencoder%s\" % (indx1, indx2))\n",
    "        \n",
    "        autoencoder.compile(optimizer='Adam', \n",
    "                            loss='mean_squared_error'\n",
    "                            # metrics=['accuracy']\n",
    "                           )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint((\"/afs/cern.ch/user/f/fsiroky/models_ae/%s.h5\" % name),\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  verbose=False,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"min\")\n",
    "        testerror = AdditionalValidationSets([(X_test, X_test, 'test')])\n",
    "        history = autoencoder.fit(X_train, X_train,\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.2,\n",
    "                            verbose=2,\n",
    "                            callbacks=[testerror, early_stopper, checkpoint_callback]).history\n",
    "\n",
    "        #np.save('/eos/cms/store/user/fsiroky/ae_models/%s.npy' % name, history)\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_loss.npy' % name , history['loss'])\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_valloss.npy' % name, history['val_loss'])\n",
    "        np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_testloss.npy' % name , testerror.history['test_loss'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "100-80-50-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "50 - 10 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SINGLE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102853 samples, validate on 25714 samples\n",
      "Epoch 1/8192\n",
      "test_loss:  3.28310281395\n",
      "19s - loss: 2.1511 - val_loss: 1.8171\n",
      "Epoch 2/8192\n",
      "test_loss:  2.60847417938\n",
      "18s - loss: 1.1788 - val_loss: 1.3913\n",
      "Epoch 3/8192\n",
      "test_loss:  2.31701817369\n",
      "18s - loss: 0.9332 - val_loss: 1.2350\n",
      "Epoch 4/8192\n",
      "test_loss:  2.18149373171\n",
      "18s - loss: 0.8325 - val_loss: 1.1616\n",
      "Epoch 5/8192\n",
      "test_loss:  2.07925740822\n",
      "18s - loss: 0.7808 - val_loss: 1.1177\n",
      "Epoch 6/8192\n",
      "test_loss:  2.00921631839\n",
      "18s - loss: 0.7498 - val_loss: 1.0883\n",
      "Epoch 7/8192\n",
      "test_loss:  1.95592985356\n",
      "17s - loss: 0.7317 - val_loss: 1.0685\n",
      "Epoch 8/8192\n",
      "test_loss:  1.91281090374\n",
      "17s - loss: 0.7201 - val_loss: 1.0505\n",
      "Epoch 9/8192\n",
      "test_loss:  1.88054616389\n",
      "18s - loss: 0.7111 - val_loss: 1.0398\n",
      "Epoch 10/8192\n",
      "test_loss:  1.84670284214\n",
      "17s - loss: 0.7048 - val_loss: 1.0290\n",
      "Epoch 11/8192\n",
      "test_loss:  1.8207351599\n",
      "17s - loss: 0.7000 - val_loss: 1.0227\n",
      "Epoch 12/8192\n",
      "test_loss:  1.79910972067\n",
      "18s - loss: 0.6966 - val_loss: 1.0151\n",
      "Epoch 13/8192\n",
      "test_loss:  1.78140354066\n",
      "18s - loss: 0.6930 - val_loss: 1.0086\n",
      "Epoch 14/8192\n",
      "test_loss:  1.76698458889\n",
      "18s - loss: 0.6904 - val_loss: 1.0035\n",
      "Epoch 15/8192\n",
      "test_loss:  1.75042161167\n",
      "18s - loss: 0.6882 - val_loss: 0.9996\n",
      "Epoch 16/8192\n",
      "test_loss:  1.73598701543\n",
      "18s - loss: 0.6862 - val_loss: 0.9956\n",
      "Epoch 17/8192\n",
      "test_loss:  1.72784179912\n",
      "17s - loss: 0.6847 - val_loss: 0.9914\n",
      "Epoch 18/8192\n",
      "test_loss:  1.71542977006\n",
      "16s - loss: 0.6831 - val_loss: 0.9873\n",
      "Epoch 19/8192\n",
      "test_loss:  1.70227602809\n",
      "18s - loss: 0.6817 - val_loss: 0.9849\n",
      "Epoch 20/8192\n",
      "test_loss:  1.70017696554\n",
      "17s - loss: 0.6805 - val_loss: 0.9831\n",
      "Epoch 21/8192\n",
      "test_loss:  1.68874136021\n",
      "17s - loss: 0.6796 - val_loss: 0.9790\n",
      "Epoch 22/8192\n",
      "test_loss:  1.68227225864\n",
      "16s - loss: 0.6785 - val_loss: 0.9770\n",
      "Epoch 23/8192\n",
      "test_loss:  1.67502583761\n",
      "17s - loss: 0.6777 - val_loss: 0.9759\n",
      "Epoch 24/8192\n",
      "test_loss:  1.66924433598\n",
      "16s - loss: 0.6768 - val_loss: 0.9731\n",
      "Epoch 25/8192\n",
      "test_loss:  1.66245734055\n",
      "16s - loss: 0.6761 - val_loss: 0.9711\n",
      "Epoch 26/8192\n",
      "test_loss:  1.65965989067\n",
      "16s - loss: 0.6755 - val_loss: 0.9701\n",
      "Epoch 27/8192\n",
      "test_loss:  1.65222186758\n",
      "16s - loss: 0.6748 - val_loss: 0.9679\n",
      "Epoch 28/8192\n",
      "test_loss:  1.6483393887\n",
      "16s - loss: 0.6741 - val_loss: 0.9661\n",
      "Epoch 29/8192\n",
      "test_loss:  1.64420434024\n",
      "16s - loss: 0.6736 - val_loss: 0.9646\n",
      "Epoch 30/8192\n",
      "test_loss:  1.63804032914\n",
      "16s - loss: 0.6731 - val_loss: 0.9634\n",
      "Epoch 31/8192\n",
      "test_loss:  1.63270327885\n",
      "17s - loss: 0.6728 - val_loss: 0.9622\n",
      "Epoch 32/8192\n",
      "test_loss:  1.63012104125\n",
      "18s - loss: 0.6723 - val_loss: 0.9610\n",
      "Epoch 33/8192\n",
      "test_loss:  1.62689016327\n",
      "19s - loss: 0.6720 - val_loss: 0.9596\n",
      "Epoch 34/8192\n",
      "test_loss:  1.62394067291\n",
      "18s - loss: 0.6714 - val_loss: 0.9587\n",
      "Epoch 35/8192\n",
      "test_loss:  1.62111016475\n",
      "18s - loss: 0.6712 - val_loss: 0.9571\n",
      "Epoch 36/8192\n",
      "test_loss:  1.61651649118\n",
      "18s - loss: 0.6707 - val_loss: 0.9566\n",
      "Epoch 37/8192\n",
      "test_loss:  1.61627384382\n",
      "18s - loss: 0.6706 - val_loss: 0.9555\n",
      "Epoch 38/8192\n",
      "test_loss:  1.61253180937\n",
      "18s - loss: 0.6702 - val_loss: 0.9545\n",
      "Epoch 39/8192\n",
      "test_loss:  1.60897036498\n",
      "17s - loss: 0.6698 - val_loss: 0.9535\n",
      "Epoch 40/8192\n",
      "test_loss:  1.60396887765\n",
      "17s - loss: 0.6696 - val_loss: 0.9528\n",
      "Epoch 41/8192\n",
      "test_loss:  1.60431293756\n",
      "18s - loss: 0.6694 - val_loss: 0.9520\n",
      "Epoch 42/8192\n",
      "test_loss:  1.60300816916\n",
      "18s - loss: 0.6690 - val_loss: 0.9512\n",
      "Epoch 43/8192\n",
      "test_loss:  1.59827651896\n",
      "18s - loss: 0.6689 - val_loss: 0.9501\n",
      "Epoch 44/8192\n",
      "test_loss:  1.59634104109\n",
      "18s - loss: 0.6686 - val_loss: 0.9497\n",
      "Epoch 45/8192\n",
      "test_loss:  1.59449499498\n",
      "17s - loss: 0.6684 - val_loss: 0.9493\n",
      "Epoch 46/8192\n",
      "test_loss:  1.59111622629\n",
      "17s - loss: 0.6685 - val_loss: 0.9483\n",
      "Epoch 47/8192\n",
      "test_loss:  1.58869837339\n",
      "18s - loss: 0.6680 - val_loss: 0.9478\n",
      "Epoch 48/8192\n",
      "test_loss:  1.58729486398\n",
      "17s - loss: 0.6679 - val_loss: 0.9471\n",
      "Epoch 49/8192\n",
      "test_loss:  1.58635735756\n",
      "18s - loss: 0.6676 - val_loss: 0.9468\n",
      "Epoch 50/8192\n",
      "test_loss:  1.58512121612\n",
      "18s - loss: 0.6675 - val_loss: 0.9461\n",
      "Epoch 51/8192\n",
      "test_loss:  1.58279434284\n",
      "18s - loss: 0.6672 - val_loss: 0.9455\n",
      "Epoch 52/8192\n",
      "test_loss:  1.581541892\n",
      "18s - loss: 0.6673 - val_loss: 0.9450\n",
      "Epoch 53/8192\n",
      "test_loss:  1.58044215545\n",
      "18s - loss: 0.6669 - val_loss: 0.9445\n",
      "Epoch 54/8192\n",
      "test_loss:  1.57878941679\n",
      "17s - loss: 0.6669 - val_loss: 0.9442\n",
      "Epoch 55/8192\n",
      "test_loss:  1.57715916211\n",
      "17s - loss: 0.6666 - val_loss: 0.9436\n",
      "Epoch 56/8192\n",
      "test_loss:  1.57466767624\n",
      "18s - loss: 0.6666 - val_loss: 0.9434\n",
      "Epoch 57/8192\n",
      "test_loss:  1.57352486263\n",
      "18s - loss: 0.6664 - val_loss: 0.9429\n",
      "Epoch 58/8192\n",
      "test_loss:  1.57344450708\n",
      "18s - loss: 0.6663 - val_loss: 0.9422\n",
      "Epoch 59/8192\n",
      "test_loss:  1.57219344306\n",
      "18s - loss: 0.6661 - val_loss: 0.9419\n",
      "Epoch 60/8192\n",
      "test_loss:  1.57039324748\n",
      "17s - loss: 0.6661 - val_loss: 0.9418\n",
      "Epoch 61/8192\n",
      "test_loss:  1.56939053172\n",
      "18s - loss: 0.6660 - val_loss: 0.9412\n",
      "Epoch 62/8192\n",
      "test_loss:  1.56789626568\n",
      "17s - loss: 0.6660 - val_loss: 0.9407\n",
      "Epoch 63/8192\n",
      "test_loss:  1.56561670334\n",
      "17s - loss: 0.6657 - val_loss: 0.9404\n",
      "Epoch 64/8192\n",
      "test_loss:  1.56634463214\n",
      "17s - loss: 0.6657 - val_loss: 0.9401\n",
      "Epoch 65/8192\n",
      "test_loss:  1.56476139776\n",
      "17s - loss: 0.6655 - val_loss: 0.9396\n",
      "Epoch 66/8192\n",
      "test_loss:  1.5630675064\n",
      "18s - loss: 0.6655 - val_loss: 0.9395\n",
      "Epoch 67/8192\n",
      "test_loss:  1.56209366619\n",
      "18s - loss: 0.6654 - val_loss: 0.9392\n",
      "Epoch 68/8192\n",
      "test_loss:  1.55968866092\n",
      "17s - loss: 0.6655 - val_loss: 0.9388\n",
      "Epoch 69/8192\n",
      "test_loss:  1.55913590308\n",
      "18s - loss: 0.6650 - val_loss: 0.9385\n",
      "Epoch 70/8192\n",
      "test_loss:  1.55829946825\n",
      "17s - loss: 0.6651 - val_loss: 0.9383\n",
      "Epoch 71/8192\n",
      "test_loss:  1.55762590733\n",
      "17s - loss: 0.6650 - val_loss: 0.9379\n",
      "Epoch 72/8192\n",
      "test_loss:  1.5567066939\n",
      "17s - loss: 0.6649 - val_loss: 0.9376\n",
      "Epoch 73/8192\n",
      "test_loss:  1.55574128577\n",
      "18s - loss: 0.6649 - val_loss: 0.9374\n",
      "Epoch 74/8192\n",
      "test_loss:  1.55427366279\n",
      "18s - loss: 0.6649 - val_loss: 0.9374\n",
      "Epoch 75/8192\n",
      "test_loss:  1.55395408838\n",
      "17s - loss: 0.6648 - val_loss: 0.9368\n",
      "Epoch 76/8192\n",
      "test_loss:  1.55343541265\n",
      "17s - loss: 0.6646 - val_loss: 0.9366\n",
      "Epoch 77/8192\n",
      "test_loss:  1.55215992682\n",
      "18s - loss: 0.6645 - val_loss: 0.9363\n",
      "Epoch 78/8192\n",
      "test_loss:  1.55119793146\n",
      "17s - loss: 0.6646 - val_loss: 0.9361\n",
      "Epoch 79/8192\n",
      "test_loss:  1.55098621382\n",
      "17s - loss: 0.6643 - val_loss: 0.9358\n",
      "Epoch 80/8192\n",
      "test_loss:  1.55021208388\n",
      "17s - loss: 0.6645 - val_loss: 0.9358\n",
      "Epoch 81/8192\n",
      "test_loss:  1.54934120452\n",
      "17s - loss: 0.6643 - val_loss: 0.9353\n",
      "Epoch 82/8192\n",
      "test_loss:  1.54775954082\n",
      "18s - loss: 0.6642 - val_loss: 0.9351\n",
      "Epoch 83/8192\n",
      "test_loss:  1.54762922787\n",
      "18s - loss: 0.6643 - val_loss: 0.9348\n",
      "Epoch 84/8192\n",
      "test_loss:  1.54695268523\n",
      "18s - loss: 0.6640 - val_loss: 0.9346\n",
      "Epoch 85/8192\n",
      "test_loss:  1.54700440086\n",
      "18s - loss: 0.6639 - val_loss: 0.9345\n",
      "Epoch 86/8192\n",
      "test_loss:  1.54583935609\n",
      "17s - loss: 0.6640 - val_loss: 0.9343\n",
      "Epoch 87/8192\n",
      "test_loss:  1.54568356452\n",
      "18s - loss: 0.6638 - val_loss: 0.9342\n",
      "Epoch 88/8192\n",
      "test_loss:  1.54562405519\n",
      "18s - loss: 0.6638 - val_loss: 0.9340\n",
      "Epoch 89/8192\n",
      "test_loss:  1.54298457003\n",
      "18s - loss: 0.6638 - val_loss: 0.9332\n",
      "Epoch 90/8192\n",
      "test_loss:  1.54171532632\n",
      "18s - loss: 0.6639 - val_loss: 0.9330\n",
      "Epoch 91/8192\n",
      "test_loss:  1.54257412342\n",
      "18s - loss: 0.6639 - val_loss: 0.9330\n",
      "Epoch 92/8192\n",
      "test_loss:  1.54358665419\n",
      "18s - loss: 0.6638 - val_loss: 0.9343\n",
      "Epoch 93/8192\n",
      "test_loss:  1.53918383423\n",
      "18s - loss: 0.6642 - val_loss: 0.9324\n",
      "Epoch 94/8192\n",
      "test_loss:  1.5439464541\n",
      "17s - loss: 0.6642 - val_loss: 0.9369\n",
      "Epoch 95/8192\n",
      "test_loss:  1.54327816997\n",
      "17s - loss: 0.6648 - val_loss: 0.9364\n",
      "Epoch 96/8192\n",
      "test_loss:  1.5334569075\n",
      "17s - loss: 0.6654 - val_loss: 0.9295\n",
      "Epoch 97/8192\n",
      "test_loss:  1.56226129975\n",
      "17s - loss: 0.6658 - val_loss: 0.9451\n",
      "Epoch 98/8192\n",
      "test_loss:  1.55660775396\n",
      "18s - loss: 0.6657 - val_loss: 0.9442\n",
      "Epoch 99/8192\n",
      "test_loss:  1.55432140893\n",
      "18s - loss: 0.6655 - val_loss: 0.9436\n",
      "Epoch 100/8192\n",
      "test_loss:  1.5300292415\n",
      "18s - loss: 0.6651 - val_loss: 0.9280\n",
      "Epoch 101/8192\n",
      "test_loss:  1.52974295824\n",
      "18s - loss: 0.6644 - val_loss: 0.9287\n",
      "Epoch 102/8192\n",
      "test_loss:  1.53658276047\n",
      "18s - loss: 0.6637 - val_loss: 0.9260\n",
      "Epoch 103/8192\n",
      "test_loss:  1.55224771836\n",
      "17s - loss: 0.6625 - val_loss: 0.9346\n",
      "Epoch 104/8192\n",
      "test_loss:  1.54970239275\n",
      "17s - loss: 0.6621 - val_loss: 0.9345\n",
      "Epoch 105/8192\n",
      "test_loss:  1.55254571567\n",
      "17s - loss: 0.6620 - val_loss: 0.9353\n",
      "Epoch 106/8192\n",
      "test_loss:  1.53891608765\n",
      "17s - loss: 0.6616 - val_loss: 0.9354\n",
      "Epoch 107/8192\n",
      "test_loss:  1.55319909613\n",
      "18s - loss: 0.6618 - val_loss: 0.9328\n",
      "Epoch 108/8192\n",
      "test_loss:  1.52473929579\n",
      "17s - loss: 0.6624 - val_loss: 0.9201\n",
      "Epoch 109/8192\n",
      "test_loss:  1.52734233148\n",
      "18s - loss: 0.6620 - val_loss: 0.9284\n",
      "Epoch 110/8192\n",
      "test_loss:  1.53920768109\n",
      "17s - loss: 0.6613 - val_loss: 0.9344\n",
      "Epoch 111/8192\n",
      "test_loss:  1.52705169197\n",
      "18s - loss: 0.6621 - val_loss: 0.9197\n",
      "Epoch 112/8192\n",
      "test_loss:  1.52672084419\n",
      "18s - loss: 0.6618 - val_loss: 0.9292\n",
      "Epoch 113/8192\n",
      "test_loss:  1.52925838436\n",
      "17s - loss: 0.6619 - val_loss: 0.9307\n",
      "Epoch 114/8192\n",
      "test_loss:  1.52570832854\n",
      "17s - loss: 0.6613 - val_loss: 0.9267\n",
      "Epoch 115/8192\n",
      "test_loss:  1.53406179443\n",
      "18s - loss: 0.6613 - val_loss: 0.9266\n",
      "Epoch 116/8192\n",
      "test_loss:  1.51625164455\n",
      "18s - loss: 0.6615 - val_loss: 0.9165\n",
      "Epoch 117/8192\n",
      "test_loss:  1.56329378149\n",
      "17s - loss: 0.6617 - val_loss: 0.9484\n",
      "Epoch 118/8192\n",
      "test_loss:  1.54463909043\n",
      "18s - loss: 0.6620 - val_loss: 0.9366\n",
      "Epoch 119/8192\n",
      "test_loss:  1.52970380022\n",
      "17s - loss: 0.6616 - val_loss: 0.9302\n",
      "Epoch 120/8192\n",
      "test_loss:  1.54041307751\n",
      "17s - loss: 0.6607 - val_loss: 0.9273\n",
      "Epoch 121/8192\n",
      "test_loss:  1.52526831438\n",
      "17s - loss: 0.6612 - val_loss: 0.9250\n",
      "Epoch 122/8192\n",
      "test_loss:  1.518743522\n",
      "17s - loss: 0.6612 - val_loss: 0.9243\n",
      "Epoch 123/8192\n",
      "test_loss:  1.51303482287\n",
      "17s - loss: 0.6610 - val_loss: 0.9209\n",
      "Epoch 124/8192\n",
      "test_loss:  1.5129154016\n",
      "17s - loss: 0.6605 - val_loss: 0.9168\n",
      "Epoch 125/8192\n",
      "test_loss:  1.52830097962\n",
      "18s - loss: 0.6612 - val_loss: 0.9255\n",
      "Epoch 126/8192\n",
      "test_loss:  1.52089145818\n",
      "17s - loss: 0.6604 - val_loss: 0.9280\n",
      "Epoch 127/8192\n",
      "test_loss:  1.51491223253\n",
      "17s - loss: 0.6604 - val_loss: 0.9160\n",
      "Epoch 128/8192\n",
      "test_loss:  1.53291887576\n",
      "17s - loss: 0.6600 - val_loss: 0.9244\n",
      "Epoch 129/8192\n",
      "test_loss:  1.51650124799\n",
      "17s - loss: 0.6599 - val_loss: 0.9233\n",
      "Epoch 130/8192\n",
      "test_loss:  1.54825892667\n",
      "17s - loss: 0.6596 - val_loss: 0.9289\n",
      "Epoch 131/8192\n",
      "test_loss:  1.52204139652\n",
      "18s - loss: 0.6590 - val_loss: 0.9154\n",
      "Epoch 132/8192\n",
      "test_loss:  1.53241664065\n",
      "18s - loss: 0.6587 - val_loss: 0.9250\n",
      "Epoch 133/8192\n",
      "test_loss:  1.54691045001\n",
      "17s - loss: 0.6592 - val_loss: 0.9313\n",
      "Epoch 134/8192\n",
      "test_loss:  1.53178566865\n",
      "18s - loss: 0.6581 - val_loss: 0.9336\n",
      "Epoch 135/8192\n",
      "test_loss:  1.52713021784\n",
      "18s - loss: 0.6583 - val_loss: 0.9296\n",
      "Epoch 136/8192\n",
      "test_loss:  1.54386309064\n",
      "18s - loss: 0.6576 - val_loss: 0.9250\n",
      "Epoch 137/8192\n",
      "test_loss:  1.547479811\n",
      "18s - loss: 0.6567 - val_loss: 0.9266\n",
      "Epoch 138/8192\n",
      "test_loss:  1.54103996355\n",
      "18s - loss: 0.6552 - val_loss: 0.9311\n",
      "Epoch 139/8192\n",
      "test_loss:  1.52890865452\n",
      "18s - loss: 0.6530 - val_loss: 0.9217\n",
      "Epoch 140/8192\n",
      "test_loss:  1.55013832441\n",
      "17s - loss: 0.6512 - val_loss: 0.9270\n",
      "Epoch 141/8192\n",
      "test_loss:  1.50339547412\n",
      "17s - loss: 0.6501 - val_loss: 0.9031\n",
      "Epoch 142/8192\n",
      "test_loss:  1.50168449575\n",
      "17s - loss: 0.6482 - val_loss: 0.9004\n",
      "Epoch 143/8192\n",
      "test_loss:  1.5287520871\n",
      "17s - loss: 0.6454 - val_loss: 0.9201\n",
      "Epoch 144/8192\n",
      "test_loss:  1.49907667596\n",
      "17s - loss: 0.6452 - val_loss: 0.9018\n",
      "Epoch 145/8192\n",
      "test_loss:  1.53393389002\n",
      "18s - loss: 0.6430 - val_loss: 0.9163\n",
      "Epoch 146/8192\n",
      "test_loss:  1.51720743014\n",
      "18s - loss: 0.6411 - val_loss: 0.9074\n",
      "Epoch 147/8192\n",
      "test_loss:  1.52051767218\n",
      "18s - loss: 0.6409 - val_loss: 0.9105\n",
      "Epoch 148/8192\n",
      "test_loss:  1.53090798132\n",
      "17s - loss: 0.6400 - val_loss: 0.9180\n",
      "Epoch 149/8192\n",
      "test_loss:  1.54540003055\n",
      "18s - loss: 0.6390 - val_loss: 0.9315\n",
      "Epoch 150/8192\n",
      "test_loss:  1.50555389457\n",
      "18s - loss: 0.6383 - val_loss: 0.9015\n",
      "Epoch 151/8192\n",
      "test_loss:  1.51093756659\n",
      "18s - loss: 0.6369 - val_loss: 0.9029\n",
      "Epoch 152/8192\n",
      "test_loss:  1.5321970357\n",
      "17s - loss: 0.6360 - val_loss: 0.9203\n",
      "Epoch 153/8192\n",
      "test_loss:  1.50867291658\n",
      "17s - loss: 0.6360 - val_loss: 0.8996\n",
      "Epoch 154/8192\n",
      "test_loss:  1.50121974355\n",
      "17s - loss: 0.6345 - val_loss: 0.8936\n",
      "Epoch 155/8192\n",
      "test_loss:  1.51186890541\n",
      "18s - loss: 0.6338 - val_loss: 0.9016\n",
      "Epoch 156/8192\n",
      "test_loss:  1.48818736623\n",
      "17s - loss: 0.6333 - val_loss: 0.8850\n",
      "Epoch 157/8192\n",
      "test_loss:  1.48895333274\n",
      "18s - loss: 0.6328 - val_loss: 0.8804\n",
      "Epoch 158/8192\n",
      "test_loss:  1.50219039851\n",
      "18s - loss: 0.6323 - val_loss: 0.8927\n",
      "Epoch 159/8192\n",
      "test_loss:  1.49500672977\n",
      "18s - loss: 0.6316 - val_loss: 0.8916\n",
      "Epoch 160/8192\n",
      "test_loss:  1.49430534517\n",
      "18s - loss: 0.6311 - val_loss: 0.8915\n",
      "Epoch 161/8192\n",
      "test_loss:  1.48202314477\n",
      "17s - loss: 0.6303 - val_loss: 0.8908\n",
      "Epoch 162/8192\n",
      "test_loss:  1.48509437539\n",
      "18s - loss: 0.6289 - val_loss: 0.8824\n",
      "Epoch 163/8192\n",
      "test_loss:  1.47923756623\n",
      "18s - loss: 0.6278 - val_loss: 0.8789\n",
      "Epoch 164/8192\n",
      "test_loss:  1.49493303456\n",
      "18s - loss: 0.6274 - val_loss: 0.8920\n",
      "Epoch 165/8192\n",
      "test_loss:  1.46536975227\n",
      "18s - loss: 0.6261 - val_loss: 0.8743\n",
      "Epoch 166/8192\n",
      "test_loss:  1.46233138355\n",
      "18s - loss: 0.6248 - val_loss: 0.8700\n",
      "Epoch 167/8192\n",
      "test_loss:  1.47551575276\n",
      "17s - loss: 0.6247 - val_loss: 0.8736\n",
      "Epoch 168/8192\n",
      "test_loss:  1.48303362053\n",
      "18s - loss: 0.6248 - val_loss: 0.8840\n",
      "Epoch 169/8192\n",
      "test_loss:  1.48655767184\n",
      "18s - loss: 0.6221 - val_loss: 0.8866\n",
      "Epoch 170/8192\n",
      "test_loss:  1.47183855763\n",
      "17s - loss: 0.6209 - val_loss: 0.8754\n",
      "Epoch 171/8192\n",
      "test_loss:  1.47582599147\n",
      "18s - loss: 0.6200 - val_loss: 0.8818\n",
      "Epoch 172/8192\n",
      "test_loss:  1.47982045943\n",
      "17s - loss: 0.6177 - val_loss: 0.8808\n",
      "Epoch 173/8192\n",
      "test_loss:  1.457573713\n",
      "17s - loss: 0.6163 - val_loss: 0.8713\n",
      "Epoch 174/8192\n",
      "test_loss:  1.47815110708\n",
      "17s - loss: 0.6135 - val_loss: 0.8893\n",
      "Epoch 175/8192\n",
      "test_loss:  1.46734463202\n",
      "17s - loss: 0.6110 - val_loss: 0.8774\n",
      "Epoch 176/8192\n",
      "test_loss:  1.45951572244\n",
      "18s - loss: 0.6090 - val_loss: 0.8679\n",
      "Epoch 177/8192\n",
      "test_loss:  1.45262425281\n",
      "17s - loss: 0.6058 - val_loss: 0.8648\n",
      "Epoch 178/8192\n",
      "test_loss:  1.44340024794\n",
      "17s - loss: 0.6029 - val_loss: 0.8559\n",
      "Epoch 179/8192\n",
      "test_loss:  1.43394582429\n",
      "17s - loss: 0.5997 - val_loss: 0.8492\n",
      "Epoch 180/8192\n",
      "test_loss:  1.43353080582\n",
      "17s - loss: 0.5971 - val_loss: 0.8507\n",
      "Epoch 181/8192\n",
      "test_loss:  1.42601377995\n",
      "18s - loss: 0.5944 - val_loss: 0.8403\n",
      "Epoch 182/8192\n",
      "test_loss:  1.4455459709\n",
      "17s - loss: 0.5916 - val_loss: 0.8492\n",
      "Epoch 183/8192\n",
      "test_loss:  1.44031633448\n",
      "17s - loss: 0.5899 - val_loss: 0.8444\n",
      "Epoch 184/8192\n",
      "test_loss:  1.43805570412\n",
      "17s - loss: 0.5870 - val_loss: 0.8465\n",
      "Epoch 185/8192\n",
      "test_loss:  1.44069615833\n",
      "18s - loss: 0.5851 - val_loss: 0.8454\n",
      "Epoch 186/8192\n",
      "test_loss:  1.42495349905\n",
      "18s - loss: 0.5836 - val_loss: 0.8310\n",
      "Epoch 187/8192\n",
      "test_loss:  1.43622324043\n",
      "17s - loss: 0.5827 - val_loss: 0.8382\n",
      "Epoch 188/8192\n",
      "test_loss:  1.43156321589\n",
      "18s - loss: 0.5801 - val_loss: 0.8256\n",
      "Epoch 189/8192\n",
      "test_loss:  1.43131436178\n",
      "17s - loss: 0.5785 - val_loss: 0.8340\n",
      "Epoch 190/8192\n",
      "test_loss:  1.44093938029\n",
      "18s - loss: 0.5779 - val_loss: 0.8339\n",
      "Epoch 191/8192\n",
      "test_loss:  1.44512508435\n",
      "17s - loss: 0.5755 - val_loss: 0.8314\n",
      "Epoch 192/8192\n",
      "test_loss:  1.44049329454\n",
      "17s - loss: 0.5760 - val_loss: 0.8284\n",
      "Epoch 193/8192\n",
      "test_loss:  1.43254119729\n",
      "18s - loss: 0.5750 - val_loss: 0.8197\n",
      "Epoch 194/8192\n",
      "test_loss:  1.5284653691\n",
      "18s - loss: 0.5742 - val_loss: 0.9003\n",
      "Epoch 195/8192\n",
      "test_loss:  1.43217634297\n",
      "18s - loss: 0.5728 - val_loss: 0.8185\n",
      "Epoch 196/8192\n",
      "test_loss:  1.44982626451\n",
      "17s - loss: 0.5717 - val_loss: 0.8225\n",
      "Epoch 197/8192\n",
      "test_loss:  1.42544570654\n",
      "18s - loss: 0.5711 - val_loss: 0.8106\n",
      "Epoch 198/8192\n",
      "test_loss:  1.43047924175\n",
      "17s - loss: 0.5700 - val_loss: 0.8212\n",
      "Epoch 199/8192\n",
      "test_loss:  1.44089034239\n",
      "17s - loss: 0.5701 - val_loss: 0.8185\n",
      "Epoch 200/8192\n",
      "test_loss:  1.54038283246\n",
      "18s - loss: 0.5694 - val_loss: 0.8774\n",
      "Epoch 201/8192\n",
      "test_loss:  1.45027723733\n",
      "18s - loss: 0.5715 - val_loss: 0.8231\n",
      "Epoch 202/8192\n",
      "test_loss:  1.49062553252\n",
      "18s - loss: 0.5682 - val_loss: 0.8428\n",
      "Epoch 203/8192\n",
      "test_loss:  1.48428087556\n",
      "17s - loss: 0.5674 - val_loss: 0.8455\n",
      "Epoch 204/8192\n",
      "test_loss:  1.42414480612\n",
      "18s - loss: 0.5665 - val_loss: 0.8103\n",
      "Epoch 205/8192\n",
      "test_loss:  1.45192559221\n",
      "17s - loss: 0.5682 - val_loss: 0.8153\n",
      "Epoch 206/8192\n",
      "test_loss:  1.44968321654\n",
      "17s - loss: 0.5662 - val_loss: 0.8144\n",
      "Epoch 207/8192\n",
      "test_loss:  1.45323331997\n",
      "17s - loss: 0.5661 - val_loss: 0.8236\n",
      "Epoch 208/8192\n",
      "test_loss:  1.4148730664\n",
      "18s - loss: 0.5659 - val_loss: 0.8057\n",
      "Epoch 209/8192\n",
      "test_loss:  1.44309171642\n",
      "18s - loss: 0.5659 - val_loss: 0.8176\n",
      "Epoch 210/8192\n",
      "test_loss:  1.41985546779\n",
      "17s - loss: 0.5646 - val_loss: 0.8131\n",
      "Epoch 211/8192\n",
      "test_loss:  1.44099783112\n",
      "18s - loss: 0.5634 - val_loss: 0.8223\n",
      "Epoch 212/8192\n",
      "test_loss:  1.43186717929\n",
      "18s - loss: 0.5636 - val_loss: 0.8223\n",
      "Epoch 213/8192\n",
      "test_loss:  1.44584935143\n",
      "18s - loss: 0.5644 - val_loss: 0.8174\n",
      "Epoch 214/8192\n",
      "test_loss:  1.43077778786\n",
      "17s - loss: 0.5623 - val_loss: 0.8133\n",
      "Epoch 215/8192\n",
      "test_loss:  1.40145619928\n",
      "17s - loss: 0.5622 - val_loss: 0.7963\n",
      "Epoch 216/8192\n",
      "test_loss:  1.39991847995\n",
      "17s - loss: 0.5615 - val_loss: 0.7964\n",
      "Epoch 217/8192\n",
      "test_loss:  1.42212662011\n",
      "17s - loss: 0.5615 - val_loss: 0.8197\n",
      "Epoch 218/8192\n",
      "test_loss:  1.42725065799\n",
      "17s - loss: 0.5602 - val_loss: 0.8456\n",
      "Epoch 219/8192\n",
      "test_loss:  1.39927193105\n",
      "18s - loss: 0.5611 - val_loss: 0.8073\n",
      "Epoch 220/8192\n",
      "test_loss:  1.395768031\n",
      "17s - loss: 0.5612 - val_loss: 0.7958\n",
      "Epoch 221/8192\n",
      "test_loss:  1.39366517485\n",
      "18s - loss: 0.5596 - val_loss: 0.7907\n",
      "Epoch 222/8192\n",
      "test_loss:  1.40748517792\n",
      "17s - loss: 0.5597 - val_loss: 0.8104\n",
      "Epoch 223/8192\n",
      "test_loss:  1.39584771994\n",
      "17s - loss: 0.5593 - val_loss: 0.8161\n",
      "Epoch 224/8192\n",
      "test_loss:  1.39552105705\n",
      "17s - loss: 0.5587 - val_loss: 0.7982\n",
      "Epoch 225/8192\n",
      "test_loss:  1.39269813355\n",
      "17s - loss: 0.5576 - val_loss: 0.7963\n",
      "Epoch 226/8192\n",
      "test_loss:  1.40026350061\n",
      "18s - loss: 0.5583 - val_loss: 0.7966\n",
      "Epoch 227/8192\n",
      "test_loss:  1.37859360747\n",
      "17s - loss: 0.5572 - val_loss: 0.7948\n",
      "Epoch 228/8192\n",
      "test_loss:  1.40243522597\n",
      "17s - loss: 0.5564 - val_loss: 0.8039\n",
      "Epoch 229/8192\n",
      "test_loss:  1.39678071116\n",
      "17s - loss: 0.5569 - val_loss: 0.8070\n",
      "Epoch 230/8192\n",
      "test_loss:  1.38919630721\n",
      "17s - loss: 0.5560 - val_loss: 0.8025\n",
      "Epoch 231/8192\n",
      "test_loss:  1.40986611141\n",
      "17s - loss: 0.5552 - val_loss: 0.8097\n",
      "Epoch 232/8192\n",
      "test_loss:  1.3945102771\n",
      "17s - loss: 0.5551 - val_loss: 0.8085\n",
      "Epoch 233/8192\n",
      "test_loss:  1.3819780874\n",
      "17s - loss: 0.5553 - val_loss: 0.8036\n",
      "Epoch 234/8192\n",
      "test_loss:  1.36209826201\n",
      "18s - loss: 0.5528 - val_loss: 0.7894\n",
      "Epoch 235/8192\n",
      "test_loss:  1.38935717007\n",
      "18s - loss: 0.5533 - val_loss: 0.8048\n",
      "Epoch 236/8192\n",
      "test_loss:  1.3813404029\n",
      "17s - loss: 0.5539 - val_loss: 0.8091\n",
      "Epoch 237/8192\n",
      "test_loss:  1.38455201\n",
      "17s - loss: 0.5526 - val_loss: 0.8008\n",
      "Epoch 238/8192\n",
      "test_loss:  1.41397490032\n",
      "18s - loss: 0.5515 - val_loss: 0.8053\n",
      "Epoch 239/8192\n",
      "test_loss:  1.38918181204\n",
      "18s - loss: 0.5504 - val_loss: 0.8110\n",
      "Epoch 240/8192\n",
      "test_loss:  1.39094500899\n",
      "17s - loss: 0.5509 - val_loss: 0.8261\n",
      "Epoch 241/8192\n",
      "test_loss:  1.37382294277\n",
      "17s - loss: 0.5499 - val_loss: 0.8025\n",
      "Epoch 242/8192\n",
      "test_loss:  1.39443183466\n",
      "18s - loss: 0.5500 - val_loss: 0.8044\n",
      "Epoch 243/8192\n",
      "test_loss:  1.37869495945\n",
      "18s - loss: 0.5485 - val_loss: 0.8080\n",
      "Epoch 244/8192\n",
      "test_loss:  1.34822216829\n",
      "17s - loss: 0.5481 - val_loss: 0.7909\n",
      "Epoch 245/8192\n",
      "test_loss:  1.35968471026\n",
      "17s - loss: 0.5478 - val_loss: 0.7937\n",
      "Epoch 246/8192\n",
      "test_loss:  1.34135943262\n",
      "17s - loss: 0.5472 - val_loss: 0.7769\n",
      "Epoch 247/8192\n",
      "test_loss:  1.36135338945\n",
      "17s - loss: 0.5466 - val_loss: 0.7935\n",
      "Epoch 248/8192\n",
      "test_loss:  1.33808471799\n",
      "17s - loss: 0.5460 - val_loss: 0.7756\n",
      "Epoch 249/8192\n",
      "test_loss:  1.3501473353\n",
      "17s - loss: 0.5464 - val_loss: 0.7901\n",
      "Epoch 250/8192\n",
      "test_loss:  1.35137885813\n",
      "17s - loss: 0.5448 - val_loss: 0.7875\n",
      "Epoch 251/8192\n",
      "test_loss:  1.36237477707\n",
      "17s - loss: 0.5448 - val_loss: 0.8009\n",
      "Epoch 252/8192\n",
      "test_loss:  1.35610008024\n",
      "18s - loss: 0.5444 - val_loss: 0.7909\n",
      "Epoch 253/8192\n",
      "test_loss:  1.34616060819\n",
      "18s - loss: 0.5445 - val_loss: 0.7892\n",
      "Epoch 254/8192\n",
      "test_loss:  1.37424828045\n",
      "17s - loss: 0.5424 - val_loss: 0.8082\n",
      "Epoch 255/8192\n",
      "test_loss:  1.35689706714\n",
      "17s - loss: 0.5421 - val_loss: 0.7944\n",
      "Epoch 256/8192\n",
      "test_loss:  1.36234937026\n",
      "17s - loss: 0.5416 - val_loss: 0.7968\n",
      "Epoch 257/8192\n",
      "test_loss:  1.34755151858\n",
      "17s - loss: 0.5421 - val_loss: 0.7880\n",
      "Epoch 258/8192\n",
      "test_loss:  1.33634698183\n",
      "18s - loss: 0.5399 - val_loss: 0.7839\n",
      "Epoch 259/8192\n",
      "test_loss:  1.3358531653\n",
      "17s - loss: 0.5406 - val_loss: 0.7875\n",
      "Epoch 260/8192\n",
      "test_loss:  1.36195877781\n",
      "17s - loss: 0.5404 - val_loss: 0.7995\n",
      "Epoch 261/8192\n",
      "test_loss:  1.36224767335\n",
      "18s - loss: 0.5390 - val_loss: 0.8005\n",
      "Epoch 262/8192\n",
      "test_loss:  1.4024808766\n",
      "18s - loss: 0.5385 - val_loss: 0.8342\n",
      "Epoch 263/8192\n",
      "test_loss:  1.33008204271\n",
      "17s - loss: 0.5384 - val_loss: 0.7813\n",
      "Epoch 264/8192\n",
      "test_loss:  1.32689833904\n",
      "17s - loss: 0.5390 - val_loss: 0.7793\n",
      "Epoch 265/8192\n",
      "test_loss:  1.31816831965\n",
      "17s - loss: 0.5386 - val_loss: 0.7831\n",
      "Epoch 266/8192\n",
      "test_loss:  1.32176884835\n",
      "18s - loss: 0.5359 - val_loss: 0.7812\n",
      "Epoch 267/8192\n",
      "test_loss:  1.32959875512\n",
      "17s - loss: 0.5363 - val_loss: 0.7858\n",
      "Epoch 268/8192\n",
      "test_loss:  1.32100778762\n",
      "17s - loss: 0.5370 - val_loss: 0.7733\n",
      "Epoch 269/8192\n",
      "test_loss:  1.33358039375\n",
      "18s - loss: 0.5359 - val_loss: 0.7829\n",
      "Epoch 270/8192\n",
      "test_loss:  1.33186881949\n",
      "17s - loss: 0.5367 - val_loss: 0.7882\n",
      "Epoch 271/8192\n",
      "test_loss:  1.33431625813\n",
      "17s - loss: 0.5348 - val_loss: 0.7894\n",
      "Epoch 272/8192\n",
      "test_loss:  1.32952480632\n",
      "17s - loss: 0.5344 - val_loss: 0.7830\n",
      "Epoch 273/8192\n",
      "test_loss:  1.32378518511\n",
      "17s - loss: 0.5350 - val_loss: 0.7838\n",
      "Epoch 274/8192\n",
      "test_loss:  1.30452019713\n",
      "17s - loss: 0.5344 - val_loss: 0.7701\n",
      "Epoch 275/8192\n",
      "test_loss:  1.32243494133\n",
      "17s - loss: 0.5342 - val_loss: 0.7848\n",
      "Epoch 276/8192\n",
      "test_loss:  1.31048921823\n",
      "17s - loss: 0.5331 - val_loss: 0.7734\n",
      "Epoch 277/8192\n",
      "test_loss:  1.33955169991\n",
      "18s - loss: 0.5332 - val_loss: 0.7942\n",
      "Epoch 278/8192\n",
      "test_loss:  1.32442144946\n",
      "17s - loss: 0.5333 - val_loss: 0.7868\n",
      "Epoch 279/8192\n",
      "test_loss:  1.30363064217\n",
      "18s - loss: 0.5321 - val_loss: 0.7647\n",
      "Epoch 280/8192\n",
      "test_loss:  1.30558418805\n",
      "17s - loss: 0.5319 - val_loss: 0.7711\n",
      "Epoch 281/8192\n",
      "test_loss:  1.31895186281\n",
      "17s - loss: 0.5330 - val_loss: 0.7807\n",
      "Epoch 282/8192\n",
      "test_loss:  1.31226443237\n",
      "17s - loss: 0.5321 - val_loss: 0.7799\n",
      "Epoch 283/8192\n",
      "test_loss:  1.30975135086\n",
      "18s - loss: 0.5310 - val_loss: 0.7779\n",
      "Epoch 284/8192\n",
      "test_loss:  1.31310519476\n",
      "18s - loss: 0.5309 - val_loss: 0.7799\n",
      "Epoch 285/8192\n",
      "test_loss:  1.31405339877\n",
      "16s - loss: 0.5314 - val_loss: 0.7807\n",
      "Epoch 286/8192\n",
      "test_loss:  1.31675165341\n",
      "17s - loss: 0.5303 - val_loss: 0.7808\n",
      "Epoch 287/8192\n",
      "test_loss:  1.31264682725\n",
      "18s - loss: 0.5305 - val_loss: 0.7861\n",
      "Epoch 288/8192\n",
      "test_loss:  1.32455266404\n",
      "18s - loss: 0.5308 - val_loss: 0.7939\n",
      "Epoch 289/8192\n",
      "test_loss:  1.30768995556\n",
      "17s - loss: 0.5294 - val_loss: 0.7751\n",
      "Epoch 290/8192\n",
      "test_loss:  1.28687831705\n",
      "17s - loss: 0.5295 - val_loss: 0.7631\n",
      "Epoch 291/8192\n",
      "test_loss:  1.29430051867\n",
      "17s - loss: 0.5288 - val_loss: 0.7675\n",
      "Epoch 292/8192\n",
      "test_loss:  1.28780666868\n",
      "18s - loss: 0.5288 - val_loss: 0.7598\n",
      "Epoch 293/8192\n",
      "test_loss:  1.31311467056\n",
      "18s - loss: 0.5297 - val_loss: 0.7782\n",
      "Epoch 294/8192\n",
      "test_loss:  1.29393823943\n",
      "18s - loss: 0.5278 - val_loss: 0.7681\n",
      "Epoch 295/8192\n",
      "test_loss:  1.3229034078\n",
      "18s - loss: 0.5288 - val_loss: 0.7865\n",
      "Epoch 296/8192\n",
      "test_loss:  1.27959281397\n",
      "18s - loss: 0.5282 - val_loss: 0.7647\n",
      "Epoch 297/8192\n",
      "test_loss:  1.31346498265\n",
      "18s - loss: 0.5275 - val_loss: 0.7714\n",
      "Epoch 298/8192\n",
      "test_loss:  1.29509989298\n",
      "18s - loss: 0.5273 - val_loss: 0.7713\n",
      "Epoch 299/8192\n",
      "test_loss:  1.30793324484\n",
      "17s - loss: 0.5271 - val_loss: 0.7742\n",
      "Epoch 300/8192\n",
      "test_loss:  1.29690766132\n",
      "18s - loss: 0.5270 - val_loss: 0.7758\n",
      "Epoch 301/8192\n",
      "test_loss:  1.28828638108\n",
      "18s - loss: 0.5263 - val_loss: 0.7656\n",
      "Epoch 302/8192\n",
      "test_loss:  1.28798810121\n",
      "18s - loss: 0.5272 - val_loss: 0.7721\n",
      "Epoch 303/8192\n",
      "test_loss:  1.27820481442\n",
      "17s - loss: 0.5260 - val_loss: 0.7599\n",
      "Epoch 304/8192\n",
      "test_loss:  1.31067190449\n",
      "17s - loss: 0.5268 - val_loss: 0.7849\n",
      "Epoch 305/8192\n",
      "test_loss:  1.30828062608\n",
      "17s - loss: 0.5269 - val_loss: 0.7781\n",
      "Epoch 306/8192\n",
      "test_loss:  1.30516810737\n",
      "17s - loss: 0.5255 - val_loss: 0.7846\n",
      "Epoch 307/8192\n",
      "test_loss:  1.28837608546\n",
      "17s - loss: 0.5264 - val_loss: 0.7621\n",
      "Epoch 308/8192\n",
      "test_loss:  1.29576351116\n",
      "17s - loss: 0.5263 - val_loss: 0.7719\n",
      "Epoch 309/8192\n",
      "test_loss:  1.29312641767\n",
      "18s - loss: 0.5247 - val_loss: 0.7716\n",
      "Epoch 310/8192\n",
      "test_loss:  1.29962902472\n",
      "17s - loss: 0.5250 - val_loss: 0.7777\n",
      "Epoch 311/8192\n",
      "test_loss:  1.29694801818\n",
      "17s - loss: 0.5246 - val_loss: 0.7709\n",
      "Epoch 312/8192\n",
      "test_loss:  1.27288954469\n",
      "17s - loss: 0.5245 - val_loss: 0.7559\n",
      "Epoch 313/8192\n",
      "test_loss:  1.29198252954\n",
      "15s - loss: 0.5251 - val_loss: 0.7531\n",
      "Epoch 314/8192\n",
      "test_loss:  1.29624203045\n",
      "15s - loss: 0.5242 - val_loss: 0.7715\n",
      "Epoch 315/8192\n",
      "test_loss:  1.2764893573\n",
      "15s - loss: 0.5248 - val_loss: 0.7642\n",
      "Epoch 316/8192\n",
      "test_loss:  1.29717825088\n",
      "17s - loss: 0.5238 - val_loss: 0.7676\n",
      "Epoch 317/8192\n",
      "test_loss:  1.30732199999\n",
      "17s - loss: 0.5234 - val_loss: 0.7787\n",
      "Epoch 318/8192\n",
      "test_loss:  1.28738164093\n",
      "18s - loss: 0.5243 - val_loss: 0.7706\n",
      "Epoch 319/8192\n",
      "test_loss:  1.33883170914\n",
      "17s - loss: 0.5226 - val_loss: 0.7966\n",
      "Epoch 320/8192\n",
      "test_loss:  1.31230525302\n",
      "17s - loss: 0.5235 - val_loss: 0.7878\n",
      "Epoch 321/8192\n",
      "test_loss:  1.26589624496\n",
      "17s - loss: 0.5235 - val_loss: 0.7562\n",
      "Epoch 322/8192\n",
      "test_loss:  1.2751639162\n",
      "18s - loss: 0.5227 - val_loss: 0.7576\n",
      "Epoch 323/8192\n",
      "test_loss:  1.26546856168\n",
      "17s - loss: 0.5227 - val_loss: 0.7585\n",
      "Epoch 324/8192\n",
      "test_loss:  1.2736832761\n",
      "17s - loss: 0.5230 - val_loss: 0.7650\n",
      "Epoch 325/8192\n",
      "test_loss:  1.28309855414\n",
      "17s - loss: 0.5238 - val_loss: 0.7684\n",
      "Epoch 326/8192\n",
      "test_loss:  1.27576323786\n",
      "18s - loss: 0.5239 - val_loss: 0.7639\n",
      "Epoch 327/8192\n",
      "test_loss:  1.29261386638\n",
      "18s - loss: 0.5216 - val_loss: 0.7708\n",
      "Epoch 328/8192\n",
      "test_loss:  1.25519116518\n",
      "18s - loss: 0.5221 - val_loss: 0.7519\n",
      "Epoch 329/8192\n",
      "test_loss:  1.27066692149\n",
      "18s - loss: 0.5223 - val_loss: 0.7664\n",
      "Epoch 330/8192\n",
      "test_loss:  1.35241844689\n",
      "17s - loss: 0.5221 - val_loss: 0.8147\n",
      "Epoch 331/8192\n",
      "test_loss:  1.25927508225\n",
      "18s - loss: 0.5217 - val_loss: 0.7603\n",
      "Epoch 332/8192\n",
      "test_loss:  1.25355983875\n",
      "17s - loss: 0.5211 - val_loss: 0.7508\n",
      "Epoch 333/8192\n",
      "test_loss:  1.26941550802\n",
      "17s - loss: 0.5213 - val_loss: 0.7635\n",
      "Epoch 334/8192\n",
      "test_loss:  1.24550895913\n",
      "17s - loss: 0.5219 - val_loss: 0.7491\n",
      "Epoch 335/8192\n",
      "test_loss:  1.2794010001\n",
      "17s - loss: 0.5204 - val_loss: 0.7719\n",
      "Epoch 336/8192\n",
      "test_loss:  1.27395070241\n",
      "17s - loss: 0.5199 - val_loss: 0.7655\n",
      "Epoch 337/8192\n",
      "test_loss:  1.26857330634\n",
      "17s - loss: 0.5212 - val_loss: 0.7565\n",
      "Epoch 338/8192\n",
      "test_loss:  1.2519156604\n",
      "17s - loss: 0.5206 - val_loss: 0.7506\n",
      "Epoch 339/8192\n",
      "test_loss:  1.28319583231\n",
      "17s - loss: 0.5206 - val_loss: 0.7759\n",
      "Epoch 340/8192\n",
      "test_loss:  1.268172264\n",
      "17s - loss: 0.5208 - val_loss: 0.7617\n",
      "Epoch 341/8192\n",
      "test_loss:  1.31375265315\n",
      "17s - loss: 0.5195 - val_loss: 0.7904\n",
      "Epoch 342/8192\n",
      "test_loss:  1.24835943462\n",
      "18s - loss: 0.5206 - val_loss: 0.7478\n",
      "Epoch 343/8192\n",
      "test_loss:  1.27622915634\n",
      "18s - loss: 0.5191 - val_loss: 0.7658\n",
      "Epoch 344/8192\n",
      "test_loss:  1.2824392066\n",
      "17s - loss: 0.5189 - val_loss: 0.7668\n",
      "Epoch 345/8192\n",
      "test_loss:  1.24879757854\n",
      "17s - loss: 0.5205 - val_loss: 0.7494\n",
      "Epoch 346/8192\n",
      "test_loss:  1.25811536797\n",
      "17s - loss: 0.5189 - val_loss: 0.7497\n",
      "Epoch 347/8192\n",
      "test_loss:  1.2646314184\n",
      "18s - loss: 0.5183 - val_loss: 0.7616\n",
      "Epoch 348/8192\n",
      "test_loss:  1.27750932111\n",
      "18s - loss: 0.5193 - val_loss: 0.7639\n",
      "Epoch 349/8192\n",
      "test_loss:  1.25749408692\n",
      "18s - loss: 0.5193 - val_loss: 0.7612\n",
      "Epoch 350/8192\n",
      "test_loss:  1.26221223992\n",
      "17s - loss: 0.5186 - val_loss: 0.7562\n",
      "Epoch 351/8192\n",
      "test_loss:  1.23905939171\n",
      "17s - loss: 0.5189 - val_loss: 0.7478\n",
      "Epoch 352/8192\n",
      "test_loss:  1.26975228724\n",
      "18s - loss: 0.5184 - val_loss: 0.7622\n",
      "Epoch 353/8192\n",
      "test_loss:  1.27561439642\n",
      "17s - loss: 0.5180 - val_loss: 0.7666\n",
      "Epoch 354/8192\n",
      "test_loss:  1.2558837626\n",
      "17s - loss: 0.5181 - val_loss: 0.7522\n",
      "Epoch 355/8192\n",
      "test_loss:  1.24084601798\n",
      "18s - loss: 0.5176 - val_loss: 0.7561\n",
      "Epoch 356/8192\n",
      "test_loss:  1.27359580215\n",
      "18s - loss: 0.5175 - val_loss: 0.7605\n",
      "Epoch 357/8192\n",
      "test_loss:  1.27348740821\n",
      "18s - loss: 0.5173 - val_loss: 0.7766\n",
      "Epoch 358/8192\n",
      "test_loss:  1.24244223313\n",
      "17s - loss: 0.5182 - val_loss: 0.7543\n",
      "Epoch 359/8192\n",
      "test_loss:  1.27285433205\n",
      "17s - loss: 0.5173 - val_loss: 0.7651\n",
      "Epoch 360/8192\n",
      "test_loss:  1.27562042114\n",
      "17s - loss: 0.5178 - val_loss: 0.7655\n",
      "Epoch 361/8192\n",
      "test_loss:  1.27092160953\n",
      "17s - loss: 0.5167 - val_loss: 0.7705\n",
      "Epoch 362/8192\n",
      "test_loss:  1.25089773912\n",
      "17s - loss: 0.5163 - val_loss: 0.7617\n",
      "Epoch 363/8192\n",
      "test_loss:  1.25863687689\n",
      "17s - loss: 0.5172 - val_loss: 0.7545\n",
      "Epoch 364/8192\n",
      "test_loss:  1.26036642846\n",
      "17s - loss: 0.5159 - val_loss: 0.7649\n",
      "Epoch 365/8192\n",
      "test_loss:  1.256918505\n",
      "17s - loss: 0.5162 - val_loss: 0.7637\n",
      "Epoch 366/8192\n",
      "test_loss:  1.28376210724\n",
      "17s - loss: 0.5168 - val_loss: 0.7633\n",
      "Epoch 367/8192\n",
      "test_loss:  1.27181743784\n",
      "17s - loss: 0.5164 - val_loss: 0.7670\n",
      "Epoch 368/8192\n",
      "test_loss:  1.26592873638\n",
      "18s - loss: 0.5164 - val_loss: 0.7543\n",
      "Epoch 369/8192\n",
      "test_loss:  1.25783705647\n",
      "17s - loss: 0.5170 - val_loss: 0.7678\n",
      "Epoch 370/8192\n",
      "test_loss:  1.25622700126\n",
      "17s - loss: 0.5163 - val_loss: 0.7633\n",
      "Epoch 371/8192\n",
      "test_loss:  1.24109373594\n",
      "18s - loss: 0.5153 - val_loss: 0.7473\n",
      "Epoch 372/8192\n",
      "test_loss:  1.24768375532\n",
      "18s - loss: 0.5156 - val_loss: 0.7552\n",
      "Epoch 373/8192\n",
      "test_loss:  1.25781995363\n",
      "17s - loss: 0.5158 - val_loss: 0.7625\n",
      "Epoch 374/8192\n",
      "test_loss:  1.2564615728\n",
      "17s - loss: 0.5165 - val_loss: 0.7639\n",
      "Epoch 375/8192\n",
      "test_loss:  1.26949186677\n",
      "18s - loss: 0.5158 - val_loss: 0.7554\n",
      "Epoch 376/8192\n",
      "test_loss:  1.24888241441\n",
      "17s - loss: 0.5160 - val_loss: 0.7518\n",
      "Epoch 377/8192\n",
      "test_loss:  1.22137536232\n",
      "17s - loss: 0.5154 - val_loss: 0.7369\n",
      "Epoch 378/8192\n",
      "test_loss:  1.24438235673\n",
      "17s - loss: 0.5143 - val_loss: 0.7502\n",
      "Epoch 379/8192\n",
      "test_loss:  1.24529829883\n",
      "17s - loss: 0.5140 - val_loss: 0.7554\n",
      "Epoch 380/8192\n",
      "test_loss:  1.31945015278\n",
      "17s - loss: 0.5146 - val_loss: 0.8053\n",
      "Epoch 381/8192\n",
      "test_loss:  1.2531832736\n",
      "17s - loss: 0.5143 - val_loss: 0.7611\n",
      "Epoch 382/8192\n",
      "test_loss:  1.26278793859\n",
      "18s - loss: 0.5151 - val_loss: 0.7553\n",
      "Epoch 383/8192\n",
      "test_loss:  1.24226431492\n",
      "18s - loss: 0.5140 - val_loss: 0.7522\n",
      "Epoch 384/8192\n",
      "test_loss:  1.26756837038\n",
      "17s - loss: 0.5142 - val_loss: 0.7613\n",
      "Epoch 385/8192\n",
      "test_loss:  1.24586307934\n",
      "17s - loss: 0.5142 - val_loss: 0.7614\n",
      "Epoch 386/8192\n",
      "test_loss:  1.24551660519\n",
      "17s - loss: 0.5131 - val_loss: 0.7615\n",
      "Epoch 387/8192\n",
      "test_loss:  1.24732950487\n",
      "17s - loss: 0.5142 - val_loss: 0.7563\n",
      "Epoch 388/8192\n",
      "test_loss:  1.22692942996\n",
      "17s - loss: 0.5137 - val_loss: 0.7392\n",
      "Epoch 389/8192\n",
      "test_loss:  1.25362715522\n",
      "17s - loss: 0.5128 - val_loss: 0.7541\n",
      "Epoch 390/8192\n",
      "test_loss:  1.24268622663\n",
      "18s - loss: 0.5135 - val_loss: 0.7532\n",
      "Epoch 391/8192\n",
      "test_loss:  1.25057055843\n",
      "17s - loss: 0.5133 - val_loss: 0.7607\n",
      "Epoch 392/8192\n",
      "test_loss:  1.24900168296\n",
      "17s - loss: 0.5126 - val_loss: 0.7570\n",
      "Epoch 393/8192\n",
      "test_loss:  1.24312505354\n",
      "18s - loss: 0.5137 - val_loss: 0.7549\n",
      "Epoch 394/8192\n",
      "test_loss:  1.30908965488\n",
      "18s - loss: 0.5121 - val_loss: 0.7922\n",
      "Epoch 395/8192\n",
      "test_loss:  1.22726091157\n",
      "18s - loss: 0.5131 - val_loss: 0.7440\n",
      "Epoch 396/8192\n",
      "test_loss:  1.23349902121\n",
      "18s - loss: 0.5127 - val_loss: 0.7491\n",
      "Epoch 397/8192\n",
      "test_loss:  1.2386967584\n",
      "18s - loss: 0.5127 - val_loss: 0.7508\n",
      "Epoch 398/8192\n",
      "test_loss:  1.2421216471\n",
      "18s - loss: 0.5119 - val_loss: 0.7530\n",
      "Epoch 399/8192\n",
      "test_loss:  1.2485178213\n",
      "18s - loss: 0.5124 - val_loss: 0.7616\n",
      "Epoch 400/8192\n",
      "test_loss:  1.24373940279\n",
      "17s - loss: 0.5120 - val_loss: 0.7556\n",
      "Epoch 401/8192\n",
      "test_loss:  1.23345207819\n",
      "18s - loss: 0.5107 - val_loss: 0.7464\n",
      "Epoch 402/8192\n",
      "test_loss:  1.22122029627\n",
      "18s - loss: 0.5122 - val_loss: 0.7380\n",
      "Epoch 403/8192\n",
      "test_loss:  1.23573258303\n",
      "17s - loss: 0.5108 - val_loss: 0.7454\n",
      "Epoch 404/8192\n",
      "test_loss:  1.22762584351\n",
      "18s - loss: 0.5120 - val_loss: 0.7353\n",
      "Epoch 405/8192\n",
      "test_loss:  1.24823398279\n",
      "18s - loss: 0.5108 - val_loss: 0.7483\n",
      "Epoch 406/8192\n",
      "test_loss:  1.25671525274\n",
      "18s - loss: 0.5111 - val_loss: 0.7509\n",
      "Epoch 407/8192\n",
      "test_loss:  1.2520556263\n",
      "17s - loss: 0.5103 - val_loss: 0.7543\n",
      "Epoch 408/8192\n",
      "test_loss:  1.25007228695\n",
      "18s - loss: 0.5101 - val_loss: 0.7451\n",
      "Epoch 409/8192\n",
      "test_loss:  1.24313501417\n",
      "17s - loss: 0.5105 - val_loss: 0.7451\n",
      "Epoch 410/8192\n",
      "test_loss:  1.26054816067\n",
      "17s - loss: 0.5097 - val_loss: 0.7561\n",
      "Epoch 411/8192\n",
      "test_loss:  1.22247532156\n",
      "17s - loss: 0.5112 - val_loss: 0.7370\n",
      "Epoch 412/8192\n",
      "test_loss:  1.2353971147\n",
      "17s - loss: 0.5101 - val_loss: 0.7410\n",
      "Epoch 413/8192\n",
      "test_loss:  1.27146559147\n",
      "18s - loss: 0.5097 - val_loss: 0.7716\n",
      "Epoch 414/8192\n",
      "test_loss:  1.24382055497\n",
      "17s - loss: 0.5107 - val_loss: 0.7528\n",
      "Epoch 415/8192\n",
      "test_loss:  1.23247841731\n",
      "17s - loss: 0.5098 - val_loss: 0.7573\n",
      "Epoch 416/8192\n",
      "test_loss:  1.2412381592\n",
      "17s - loss: 0.5111 - val_loss: 0.7638\n",
      "Epoch 417/8192\n",
      "test_loss:  1.23829641145\n",
      "16s - loss: 0.5111 - val_loss: 0.7399\n",
      "Epoch 418/8192\n",
      "test_loss:  1.23206308266\n",
      "17s - loss: 0.5087 - val_loss: 0.7493\n",
      "Epoch 419/8192\n",
      "test_loss:  1.2614641988\n",
      "17s - loss: 0.5100 - val_loss: 0.7664\n",
      "Epoch 420/8192\n",
      "test_loss:  1.23091413624\n",
      "17s - loss: 0.5096 - val_loss: 0.7427\n",
      "Epoch 421/8192\n",
      "test_loss:  1.22709387314\n",
      "17s - loss: 0.5083 - val_loss: 0.7453\n",
      "Epoch 422/8192\n",
      "test_loss:  1.23494744944\n",
      "17s - loss: 0.5094 - val_loss: 0.7443\n",
      "Epoch 423/8192\n",
      "test_loss:  1.25644135772\n",
      "17s - loss: 0.5088 - val_loss: 0.7583\n",
      "Epoch 424/8192\n",
      "test_loss:  1.2204713913\n",
      "18s - loss: 0.5080 - val_loss: 0.7382\n",
      "Epoch 425/8192\n",
      "test_loss:  1.23506180296\n",
      "17s - loss: 0.5079 - val_loss: 0.7476\n",
      "Epoch 426/8192\n",
      "test_loss:  1.22056205353\n",
      "18s - loss: 0.5084 - val_loss: 0.7380\n",
      "Epoch 427/8192\n",
      "test_loss:  1.25265434794\n",
      "17s - loss: 0.5080 - val_loss: 0.7441\n",
      "Epoch 428/8192\n",
      "test_loss:  1.2174095159\n",
      "17s - loss: 0.5086 - val_loss: 0.7404\n",
      "Epoch 429/8192\n",
      "test_loss:  1.24561216999\n",
      "17s - loss: 0.5065 - val_loss: 0.7635\n",
      "Epoch 430/8192\n",
      "test_loss:  1.25047841339\n",
      "17s - loss: 0.5076 - val_loss: 0.7609\n",
      "Epoch 431/8192\n",
      "test_loss:  1.24249065951\n",
      "18s - loss: 0.5070 - val_loss: 0.7477\n",
      "Epoch 432/8192\n",
      "test_loss:  1.23005097934\n",
      "17s - loss: 0.5073 - val_loss: 0.7507\n",
      "Epoch 433/8192\n",
      "test_loss:  1.2207680789\n",
      "17s - loss: 0.5068 - val_loss: 0.7460\n",
      "Epoch 434/8192\n",
      "test_loss:  1.21570539649\n",
      "18s - loss: 0.5064 - val_loss: 0.7394\n",
      "Epoch 435/8192\n",
      "test_loss:  1.24107507591\n",
      "18s - loss: 0.5074 - val_loss: 0.7539\n",
      "Epoch 436/8192\n",
      "test_loss:  1.21770607184\n",
      "17s - loss: 0.5080 - val_loss: 0.7457\n",
      "Epoch 437/8192\n",
      "test_loss:  1.20406216926\n",
      "17s - loss: 0.5065 - val_loss: 0.7299\n",
      "Epoch 438/8192\n",
      "test_loss:  1.2538247054\n",
      "17s - loss: 0.5065 - val_loss: 0.7577\n",
      "Epoch 439/8192\n",
      "test_loss:  1.32844454241\n",
      "17s - loss: 0.5060 - val_loss: 0.8180\n",
      "Epoch 440/8192\n",
      "test_loss:  1.22466379831\n",
      "18s - loss: 0.5067 - val_loss: 0.7518\n",
      "Epoch 441/8192\n",
      "test_loss:  1.21215405524\n",
      "17s - loss: 0.5059 - val_loss: 0.7356\n",
      "Epoch 442/8192\n",
      "test_loss:  1.23482334733\n",
      "17s - loss: 0.5064 - val_loss: 0.7470\n",
      "Epoch 443/8192\n",
      "test_loss:  1.20787415278\n",
      "17s - loss: 0.5051 - val_loss: 0.7335\n",
      "Epoch 444/8192\n",
      "test_loss:  1.21157802892\n",
      "18s - loss: 0.5054 - val_loss: 0.7361\n",
      "Epoch 445/8192\n",
      "test_loss:  1.21323232652\n",
      "18s - loss: 0.5054 - val_loss: 0.7422\n",
      "Epoch 446/8192\n",
      "test_loss:  1.23032790325\n",
      "17s - loss: 0.5059 - val_loss: 0.7479\n",
      "Epoch 447/8192\n",
      "test_loss:  1.19402398475\n",
      "18s - loss: 0.5052 - val_loss: 0.7277\n",
      "Epoch 448/8192\n",
      "test_loss:  1.23572364283\n",
      "17s - loss: 0.5047 - val_loss: 0.7456\n",
      "Epoch 449/8192\n",
      "test_loss:  1.22057992711\n",
      "17s - loss: 0.5047 - val_loss: 0.7324\n",
      "Epoch 450/8192\n",
      "test_loss:  1.22837467531\n",
      "17s - loss: 0.5045 - val_loss: 0.7458\n",
      "Epoch 451/8192\n",
      "test_loss:  1.23067496424\n",
      "18s - loss: 0.5052 - val_loss: 0.7451\n",
      "Epoch 452/8192\n",
      "test_loss:  1.23523117569\n",
      "18s - loss: 0.5049 - val_loss: 0.7342\n",
      "Epoch 453/8192\n",
      "test_loss:  1.20028059523\n",
      "17s - loss: 0.5038 - val_loss: 0.7288\n",
      "Epoch 454/8192\n",
      "test_loss:  1.21835476593\n",
      "18s - loss: 0.5041 - val_loss: 0.7379\n",
      "Epoch 455/8192\n",
      "test_loss:  1.21286502485\n",
      "18s - loss: 0.5046 - val_loss: 0.7339\n",
      "Epoch 456/8192\n",
      "test_loss:  1.24256394635\n",
      "18s - loss: 0.5047 - val_loss: 0.7405\n",
      "Epoch 457/8192\n",
      "test_loss:  1.2088548422\n",
      "18s - loss: 0.5047 - val_loss: 0.7231\n",
      "Epoch 458/8192\n",
      "test_loss:  1.22340862424\n",
      "18s - loss: 0.5051 - val_loss: 0.7351\n",
      "Epoch 459/8192\n",
      "test_loss:  1.20672361573\n",
      "17s - loss: 0.5040 - val_loss: 0.7375\n",
      "Epoch 460/8192\n",
      "test_loss:  1.22662256538\n",
      "18s - loss: 0.5035 - val_loss: 0.7480\n",
      "Epoch 461/8192\n",
      "test_loss:  1.21401735525\n",
      "17s - loss: 0.5032 - val_loss: 0.7248\n",
      "Epoch 462/8192\n",
      "test_loss:  1.20348616787\n",
      "18s - loss: 0.5029 - val_loss: 0.7352\n",
      "Epoch 463/8192\n",
      "test_loss:  1.20456081766\n",
      "17s - loss: 0.5026 - val_loss: 0.7333\n",
      "Epoch 464/8192\n",
      "test_loss:  1.21271812916\n",
      "17s - loss: 0.5026 - val_loss: 0.7363\n",
      "Epoch 465/8192\n",
      "test_loss:  1.22538865987\n",
      "17s - loss: 0.5026 - val_loss: 0.7381\n",
      "Epoch 466/8192\n",
      "test_loss:  1.20888476355\n",
      "17s - loss: 0.5028 - val_loss: 0.7304\n",
      "Epoch 467/8192\n",
      "test_loss:  1.19486918221\n",
      "17s - loss: 0.5016 - val_loss: 0.7253\n",
      "Epoch 468/8192\n",
      "test_loss:  1.22159124236\n",
      "18s - loss: 0.5023 - val_loss: 0.7410\n",
      "Epoch 469/8192\n",
      "test_loss:  1.2259505359\n",
      "17s - loss: 0.5024 - val_loss: 0.7402\n",
      "Epoch 470/8192\n",
      "test_loss:  1.20155918571\n",
      "18s - loss: 0.5010 - val_loss: 0.7254\n",
      "Epoch 471/8192\n",
      "test_loss:  1.19757320305\n",
      "18s - loss: 0.5013 - val_loss: 0.7348\n",
      "Epoch 472/8192\n",
      "test_loss:  1.22670886857\n",
      "18s - loss: 0.5022 - val_loss: 0.7417\n",
      "Epoch 473/8192\n",
      "test_loss:  1.21509258169\n",
      "17s - loss: 0.5014 - val_loss: 0.7316\n",
      "Epoch 474/8192\n",
      "test_loss:  1.20323761762\n",
      "17s - loss: 0.5000 - val_loss: 0.7268\n",
      "Epoch 475/8192\n",
      "test_loss:  1.23071534452\n",
      "17s - loss: 0.5006 - val_loss: 0.7439\n",
      "Epoch 476/8192\n",
      "test_loss:  1.20485134728\n",
      "18s - loss: 0.5010 - val_loss: 0.7227\n",
      "Epoch 477/8192\n",
      "test_loss:  1.22960450585\n",
      "17s - loss: 0.5006 - val_loss: 0.7604\n",
      "Epoch 478/8192\n",
      "test_loss:  1.2064142783\n",
      "17s - loss: 0.5004 - val_loss: 0.7250\n",
      "Epoch 479/8192\n",
      "test_loss:  1.19511968773\n",
      "18s - loss: 0.5000 - val_loss: 0.7125\n",
      "Epoch 480/8192\n",
      "test_loss:  1.23424179997\n",
      "18s - loss: 0.4997 - val_loss: 0.7404\n",
      "Epoch 481/8192\n",
      "test_loss:  1.23996015508\n",
      "18s - loss: 0.5007 - val_loss: 0.7341\n",
      "Epoch 482/8192\n",
      "test_loss:  1.23625619821\n",
      "18s - loss: 0.4997 - val_loss: 0.7271\n",
      "Epoch 483/8192\n",
      "test_loss:  1.22820246077\n",
      "17s - loss: 0.4994 - val_loss: 0.7361\n",
      "Epoch 484/8192\n",
      "test_loss:  1.2124799036\n",
      "17s - loss: 0.4981 - val_loss: 0.7247\n",
      "Epoch 485/8192\n",
      "test_loss:  1.21098021929\n",
      "17s - loss: 0.4989 - val_loss: 0.7268\n",
      "Epoch 486/8192\n",
      "test_loss:  1.22873798416\n",
      "17s - loss: 0.4991 - val_loss: 0.7287\n",
      "Epoch 487/8192\n",
      "test_loss:  1.20745465529\n",
      "18s - loss: 0.4981 - val_loss: 0.7150\n",
      "Epoch 488/8192\n",
      "test_loss:  1.22083929743\n",
      "17s - loss: 0.4976 - val_loss: 0.7282\n",
      "Epoch 489/8192\n",
      "test_loss:  1.2291516656\n",
      "18s - loss: 0.4975 - val_loss: 0.7283\n",
      "Epoch 490/8192\n",
      "test_loss:  1.20776152145\n",
      "17s - loss: 0.4973 - val_loss: 0.7156\n",
      "Epoch 491/8192\n",
      "test_loss:  1.23123141035\n",
      "17s - loss: 0.4968 - val_loss: 0.7442\n",
      "Epoch 492/8192\n",
      "test_loss:  1.24054022767\n",
      "17s - loss: 0.4964 - val_loss: 0.7254\n",
      "Epoch 493/8192\n",
      "test_loss:  1.22888935677\n",
      "17s - loss: 0.4952 - val_loss: 0.7144\n",
      "Epoch 494/8192\n",
      "test_loss:  1.21744335317\n",
      "18s - loss: 0.4962 - val_loss: 0.7109\n",
      "Epoch 495/8192\n",
      "test_loss:  1.2265955898\n",
      "18s - loss: 0.4942 - val_loss: 0.7156\n",
      "Epoch 496/8192\n",
      "test_loss:  1.25817270683\n",
      "17s - loss: 0.4936 - val_loss: 0.7251\n",
      "Epoch 497/8192\n",
      "test_loss:  1.22344276044\n",
      "17s - loss: 0.4935 - val_loss: 0.7072\n",
      "Epoch 498/8192\n",
      "test_loss:  1.31413226745\n",
      "17s - loss: 0.4942 - val_loss: 0.7403\n",
      "Epoch 499/8192\n",
      "test_loss:  1.2473058605\n",
      "18s - loss: 0.4933 - val_loss: 0.7251\n",
      "Epoch 500/8192\n",
      "test_loss:  1.22997282572\n",
      "18s - loss: 0.4930 - val_loss: 0.7049\n",
      "Epoch 501/8192\n",
      "test_loss:  1.23723628859\n",
      "17s - loss: 0.4926 - val_loss: 0.7121\n",
      "Epoch 502/8192\n",
      "test_loss:  1.23538101604\n",
      "17s - loss: 0.4920 - val_loss: 0.7104\n",
      "Epoch 503/8192\n",
      "test_loss:  1.24062241704\n",
      "17s - loss: 0.4918 - val_loss: 0.7072\n",
      "Epoch 504/8192\n",
      "test_loss:  1.26993588078\n",
      "18s - loss: 0.4909 - val_loss: 0.7201\n",
      "Epoch 505/8192\n",
      "test_loss:  1.24506771096\n",
      "18s - loss: 0.4921 - val_loss: 0.7192\n",
      "Epoch 506/8192\n",
      "test_loss:  1.25909133852\n",
      "17s - loss: 0.4912 - val_loss: 0.7024\n",
      "Epoch 507/8192\n",
      "test_loss:  1.25720460667\n",
      "17s - loss: 0.4905 - val_loss: 0.7045\n",
      "Epoch 508/8192\n",
      "test_loss:  1.25844780747\n",
      "18s - loss: 0.4909 - val_loss: 0.7092\n",
      "Epoch 509/8192\n",
      "test_loss:  1.27296074346\n",
      "18s - loss: 0.4906 - val_loss: 0.6976\n",
      "Epoch 510/8192\n",
      "test_loss:  1.25043953797\n",
      "17s - loss: 0.4899 - val_loss: 0.7014\n",
      "Epoch 511/8192\n",
      "test_loss:  1.26735481545\n",
      "17s - loss: 0.4887 - val_loss: 0.7144\n",
      "Epoch 512/8192\n",
      "test_loss:  1.22867510892\n",
      "18s - loss: 0.4899 - val_loss: 0.6893\n",
      "Epoch 513/8192\n",
      "test_loss:  1.25208398609\n",
      "18s - loss: 0.4901 - val_loss: 0.6989\n",
      "Epoch 514/8192\n",
      "test_loss:  1.25154499412\n",
      "17s - loss: 0.4890 - val_loss: 0.7106\n",
      "Epoch 515/8192\n",
      "test_loss:  1.25190007481\n",
      "17s - loss: 0.4885 - val_loss: 0.7176\n",
      "Epoch 516/8192\n",
      "test_loss:  1.23877062546\n",
      "17s - loss: 0.4886 - val_loss: 0.6994\n",
      "Epoch 517/8192\n",
      "test_loss:  1.25614145549\n",
      "17s - loss: 0.4878 - val_loss: 0.7043\n",
      "Epoch 518/8192\n",
      "test_loss:  1.26551944363\n",
      "17s - loss: 0.4881 - val_loss: 0.6994\n",
      "Epoch 519/8192\n",
      "test_loss:  1.24671279384\n",
      "17s - loss: 0.4895 - val_loss: 0.7063\n",
      "Epoch 520/8192\n",
      "test_loss:  1.25658036131\n",
      "17s - loss: 0.4876 - val_loss: 0.7088\n",
      "Epoch 521/8192\n",
      "test_loss:  1.25947782418\n",
      "18s - loss: 0.4893 - val_loss: 0.7083\n",
      "Epoch 522/8192\n",
      "test_loss:  1.22997645071\n",
      "17s - loss: 0.4893 - val_loss: 0.6838\n",
      "Epoch 523/8192\n",
      "test_loss:  1.24497532882\n",
      "17s - loss: 0.4879 - val_loss: 0.6977\n",
      "Epoch 524/8192\n",
      "test_loss:  1.23729911099\n",
      "18s - loss: 0.4878 - val_loss: 0.6929\n",
      "Epoch 525/8192\n",
      "test_loss:  1.24297389088\n",
      "16s - loss: 0.4887 - val_loss: 0.6990\n",
      "Epoch 526/8192\n",
      "test_loss:  1.23438922779\n",
      "17s - loss: 0.4863 - val_loss: 0.6870\n",
      "Epoch 527/8192\n",
      "test_loss:  1.22389332828\n",
      "18s - loss: 0.4896 - val_loss: 0.6756\n",
      "Epoch 528/8192\n",
      "test_loss:  1.27228604651\n",
      "17s - loss: 0.4863 - val_loss: 0.7056\n",
      "Epoch 529/8192\n",
      "test_loss:  1.22721915232\n",
      "18s - loss: 0.4865 - val_loss: 0.6845\n",
      "Epoch 530/8192\n",
      "test_loss:  1.23316881367\n",
      "17s - loss: 0.4868 - val_loss: 0.6953\n",
      "Epoch 531/8192\n",
      "test_loss:  1.26785411916\n",
      "18s - loss: 0.4863 - val_loss: 0.7151\n",
      "Epoch 532/8192\n",
      "test_loss:  1.25159104832\n",
      "17s - loss: 0.4863 - val_loss: 0.6873\n",
      "Epoch 533/8192\n",
      "test_loss:  1.2284031997\n",
      "18s - loss: 0.4860 - val_loss: 0.6988\n",
      "Epoch 534/8192\n",
      "test_loss:  1.23807860781\n",
      "18s - loss: 0.4877 - val_loss: 0.6934\n",
      "Epoch 535/8192\n",
      "test_loss:  1.23945492168\n",
      "17s - loss: 0.4862 - val_loss: 0.7106\n",
      "Epoch 536/8192\n",
      "test_loss:  1.25678086869\n",
      "17s - loss: 0.4867 - val_loss: 0.6984\n",
      "Epoch 537/8192\n",
      "test_loss:  1.22300178702\n",
      "17s - loss: 0.4866 - val_loss: 0.6846\n",
      "Epoch 538/8192\n",
      "test_loss:  1.25290141148\n",
      "18s - loss: 0.4868 - val_loss: 0.6998\n",
      "Epoch 539/8192\n",
      "test_loss:  1.25072384343\n",
      "17s - loss: 0.4866 - val_loss: 0.6928\n",
      "Epoch 540/8192\n",
      "test_loss:  1.29003587425\n",
      "18s - loss: 0.4864 - val_loss: 0.7053\n",
      "Epoch 541/8192\n",
      "test_loss:  1.23574863918\n",
      "17s - loss: 0.4859 - val_loss: 0.6914\n",
      "Epoch 542/8192\n",
      "test_loss:  1.23216001697\n",
      "17s - loss: 0.4857 - val_loss: 0.6870\n",
      "Epoch 543/8192\n",
      "test_loss:  1.23423649901\n",
      "17s - loss: 0.4854 - val_loss: 0.6881\n",
      "Epoch 544/8192\n",
      "test_loss:  1.23549599308\n",
      "17s - loss: 0.4850 - val_loss: 0.7103\n",
      "Epoch 545/8192\n",
      "test_loss:  1.23881406047\n",
      "17s - loss: 0.4858 - val_loss: 0.6964\n",
      "Epoch 546/8192\n",
      "test_loss:  1.25559806296\n",
      "18s - loss: 0.4855 - val_loss: 0.7104\n",
      "Epoch 547/8192\n",
      "test_loss:  1.25497517994\n",
      "17s - loss: 0.4869 - val_loss: 0.7013\n",
      "Epoch 548/8192\n",
      "test_loss:  1.2621543168\n",
      "17s - loss: 0.4857 - val_loss: 0.7024\n",
      "Epoch 549/8192\n",
      "test_loss:  1.24420948188\n",
      "17s - loss: 0.4846 - val_loss: 0.6917\n",
      "Epoch 550/8192\n",
      "test_loss:  1.24592154335\n",
      "16s - loss: 0.4845 - val_loss: 0.6883\n",
      "Epoch 551/8192\n",
      "test_loss:  1.25523880338\n",
      "17s - loss: 0.4852 - val_loss: 0.6911\n",
      "Epoch 552/8192\n",
      "test_loss:  1.24408287065\n",
      "17s - loss: 0.4845 - val_loss: 0.6929\n",
      "Epoch 553/8192\n",
      "test_loss:  1.24747415641\n",
      "17s - loss: 0.4853 - val_loss: 0.6889\n",
      "Epoch 554/8192\n",
      "test_loss:  1.24346719646\n",
      "18s - loss: 0.4840 - val_loss: 0.6841\n",
      "Epoch 555/8192\n",
      "test_loss:  1.217758362\n",
      "17s - loss: 0.4840 - val_loss: 0.6741\n",
      "Epoch 556/8192\n",
      "test_loss:  1.25682186034\n",
      "16s - loss: 0.4840 - val_loss: 0.6950\n",
      "Epoch 557/8192\n",
      "test_loss:  1.26338121644\n",
      "18s - loss: 0.4840 - val_loss: 0.7157\n",
      "Epoch 558/8192\n",
      "test_loss:  1.25125059793\n",
      "17s - loss: 0.4836 - val_loss: 0.6921\n",
      "Epoch 559/8192\n",
      "test_loss:  1.25041303085\n",
      "17s - loss: 0.4840 - val_loss: 0.6994\n",
      "Epoch 560/8192\n",
      "test_loss:  1.24657410643\n",
      "18s - loss: 0.4830 - val_loss: 0.6936\n",
      "Epoch 561/8192\n",
      "test_loss:  1.24349446369\n",
      "17s - loss: 0.4841 - val_loss: 0.6895\n",
      "Epoch 562/8192\n",
      "test_loss:  1.24134391568\n",
      "18s - loss: 0.4832 - val_loss: 0.6943\n",
      "Epoch 563/8192\n",
      "test_loss:  1.24785010147\n",
      "18s - loss: 0.4823 - val_loss: 0.6868\n",
      "Epoch 564/8192\n",
      "test_loss:  1.24913649931\n",
      "18s - loss: 0.4835 - val_loss: 0.6933\n",
      "Epoch 565/8192\n",
      "test_loss:  1.21195579041\n",
      "17s - loss: 0.4829 - val_loss: 0.6775\n",
      "Epoch 566/8192\n",
      "test_loss:  1.23782391172\n",
      "17s - loss: 0.4831 - val_loss: 0.6924\n",
      "Epoch 567/8192\n",
      "test_loss:  1.32434125694\n",
      "17s - loss: 0.4832 - val_loss: 0.7612\n",
      "Epoch 568/8192\n",
      "test_loss:  1.23823598414\n",
      "17s - loss: 0.4824 - val_loss: 0.6918\n",
      "Epoch 569/8192\n",
      "test_loss:  1.22871746647\n",
      "17s - loss: 0.4828 - val_loss: 0.6716\n",
      "Epoch 570/8192\n",
      "test_loss:  1.2494010658\n",
      "17s - loss: 0.4834 - val_loss: 0.6904\n",
      "Epoch 571/8192\n",
      "test_loss:  1.23428333627\n",
      "17s - loss: 0.4822 - val_loss: 0.7032\n",
      "Epoch 572/8192\n",
      "test_loss:  1.23804788797\n",
      "17s - loss: 0.4825 - val_loss: 0.6677\n",
      "Epoch 573/8192\n",
      "test_loss:  1.24968271108\n",
      "17s - loss: 0.4817 - val_loss: 0.6865\n",
      "Epoch 574/8192\n",
      "test_loss:  1.24662597582\n",
      "16s - loss: 0.4816 - val_loss: 0.6917\n",
      "Epoch 575/8192\n",
      "test_loss:  1.24013852629\n",
      "17s - loss: 0.4818 - val_loss: 0.6765\n",
      "Epoch 576/8192\n",
      "test_loss:  1.22810173524\n",
      "17s - loss: 0.4814 - val_loss: 0.6720\n",
      "Epoch 577/8192\n",
      "test_loss:  1.25073265914\n",
      "17s - loss: 0.4821 - val_loss: 0.6838\n",
      "Epoch 578/8192\n",
      "test_loss:  1.22123515412\n",
      "17s - loss: 0.4813 - val_loss: 0.6605\n",
      "Epoch 579/8192\n",
      "test_loss:  1.21180384993\n",
      "17s - loss: 0.4814 - val_loss: 0.6578\n",
      "Epoch 580/8192\n",
      "test_loss:  1.25807792247\n",
      "18s - loss: 0.4809 - val_loss: 0.6928\n",
      "Epoch 581/8192\n",
      "test_loss:  1.2766601123\n",
      "17s - loss: 0.4814 - val_loss: 0.6965\n",
      "Epoch 582/8192\n",
      "test_loss:  1.22145730669\n",
      "18s - loss: 0.4814 - val_loss: 0.6661\n",
      "Epoch 583/8192\n",
      "test_loss:  1.24432256732\n",
      "17s - loss: 0.4797 - val_loss: 0.6589\n",
      "Epoch 584/8192\n",
      "test_loss:  1.23211375742\n",
      "17s - loss: 0.4798 - val_loss: 0.6675\n",
      "Epoch 585/8192\n",
      "test_loss:  1.20829126655\n",
      "17s - loss: 0.4809 - val_loss: 0.6499\n",
      "Epoch 586/8192\n",
      "test_loss:  1.33741710064\n",
      "18s - loss: 0.4805 - val_loss: 0.6910\n",
      "Epoch 587/8192\n",
      "test_loss:  1.24273596614\n",
      "17s - loss: 0.4804 - val_loss: 0.6799\n",
      "Epoch 588/8192\n",
      "test_loss:  1.21595011838\n",
      "17s - loss: 0.4791 - val_loss: 0.6512\n",
      "Epoch 589/8192\n",
      "test_loss:  1.24863185723\n",
      "17s - loss: 0.4793 - val_loss: 0.6740\n",
      "Epoch 590/8192\n",
      "test_loss:  1.22780228484\n",
      "17s - loss: 0.4809 - val_loss: 0.6799\n",
      "Epoch 591/8192\n",
      "test_loss:  1.250226442\n",
      "17s - loss: 0.4791 - val_loss: 0.6954\n",
      "Epoch 592/8192\n",
      "test_loss:  1.24106447913\n",
      "17s - loss: 0.4788 - val_loss: 0.6754\n",
      "Epoch 593/8192\n",
      "test_loss:  1.24682637623\n",
      "17s - loss: 0.4807 - val_loss: 0.6846\n",
      "Epoch 594/8192\n",
      "test_loss:  1.23832425253\n",
      "18s - loss: 0.4793 - val_loss: 0.6739\n",
      "Epoch 595/8192\n",
      "test_loss:  1.25932760053\n",
      "17s - loss: 0.4801 - val_loss: 0.6698\n",
      "Epoch 596/8192\n",
      "test_loss:  1.23027812857\n",
      "17s - loss: 0.4794 - val_loss: 0.6602\n",
      "Epoch 597/8192\n",
      "test_loss:  1.22033261312\n",
      "17s - loss: 0.4790 - val_loss: 0.6798\n",
      "Epoch 598/8192\n",
      "test_loss:  1.21675763707\n",
      "17s - loss: 0.4786 - val_loss: 0.6514\n",
      "Epoch 599/8192\n",
      "test_loss:  1.23729430893\n",
      "18s - loss: 0.4785 - val_loss: 0.6657\n",
      "Epoch 600/8192\n",
      "test_loss:  1.22862033131\n",
      "17s - loss: 0.4788 - val_loss: 0.6638\n",
      "Epoch 601/8192\n",
      "test_loss:  1.21511706199\n",
      "17s - loss: 0.4782 - val_loss: 0.6533\n",
      "Epoch 602/8192\n",
      "test_loss:  1.22012977054\n",
      "18s - loss: 0.4781 - val_loss: 0.6591\n",
      "Epoch 603/8192\n",
      "test_loss:  1.21590607096\n",
      "17s - loss: 0.4784 - val_loss: 0.6588\n",
      "Epoch 604/8192\n",
      "test_loss:  1.20913994099\n",
      "18s - loss: 0.4789 - val_loss: 0.6489\n",
      "Epoch 605/8192\n",
      "test_loss:  1.23037224791\n",
      "17s - loss: 0.4782 - val_loss: 0.6825\n",
      "Epoch 606/8192\n",
      "test_loss:  1.23792407153\n",
      "17s - loss: 0.4766 - val_loss: 0.6683\n",
      "Epoch 607/8192\n",
      "test_loss:  1.23758854655\n",
      "17s - loss: 0.4783 - val_loss: 0.6686\n",
      "Epoch 608/8192\n",
      "test_loss:  1.20392009815\n",
      "17s - loss: 0.4774 - val_loss: 0.6780\n",
      "Epoch 609/8192\n",
      "test_loss:  1.24257009433\n",
      "17s - loss: 0.4782 - val_loss: 0.6681\n",
      "Epoch 610/8192\n",
      "test_loss:  1.23801081526\n",
      "17s - loss: 0.4768 - val_loss: 0.6693\n",
      "Epoch 611/8192\n",
      "test_loss:  1.24181368002\n",
      "17s - loss: 0.4773 - val_loss: 0.6686\n",
      "Epoch 612/8192\n",
      "test_loss:  1.23744771171\n",
      "17s - loss: 0.4771 - val_loss: 0.6639\n",
      "Epoch 613/8192\n",
      "test_loss:  1.2232232312\n",
      "18s - loss: 0.4781 - val_loss: 0.6532\n",
      "Epoch 614/8192\n",
      "test_loss:  1.21710978508\n",
      "17s - loss: 0.4773 - val_loss: 0.6551\n",
      "Epoch 615/8192\n",
      "test_loss:  1.20458092688\n",
      "18s - loss: 0.4771 - val_loss: 0.6442\n",
      "Epoch 616/8192\n",
      "test_loss:  1.21253599994\n",
      "17s - loss: 0.4763 - val_loss: 0.6540\n",
      "Epoch 617/8192\n",
      "test_loss:  1.21464025198\n",
      "17s - loss: 0.4778 - val_loss: 0.6560\n",
      "Epoch 618/8192\n",
      "test_loss:  1.19746751199\n",
      "18s - loss: 0.4760 - val_loss: 0.6360\n",
      "Epoch 619/8192\n",
      "test_loss:  1.20444931332\n",
      "18s - loss: 0.4770 - val_loss: 0.6422\n",
      "Epoch 620/8192\n",
      "test_loss:  1.20820524068\n",
      "17s - loss: 0.4765 - val_loss: 0.6382\n",
      "Epoch 621/8192\n",
      "test_loss:  1.20030288824\n",
      "17s - loss: 0.4768 - val_loss: 0.6366\n",
      "Epoch 622/8192\n",
      "test_loss:  1.20678722026\n",
      "17s - loss: 0.4758 - val_loss: 0.6490\n",
      "Epoch 623/8192\n",
      "test_loss:  1.19467996654\n",
      "17s - loss: 0.4761 - val_loss: 0.6478\n",
      "Epoch 624/8192\n",
      "test_loss:  1.23829644888\n",
      "17s - loss: 0.4760 - val_loss: 0.6547\n",
      "Epoch 625/8192\n",
      "test_loss:  1.20895159667\n",
      "17s - loss: 0.4768 - val_loss: 0.6441\n",
      "Epoch 626/8192\n",
      "test_loss:  1.22465788243\n",
      "18s - loss: 0.4756 - val_loss: 0.6447\n",
      "Epoch 627/8192\n",
      "test_loss:  1.21859967691\n",
      "17s - loss: 0.4745 - val_loss: 0.6523\n",
      "Epoch 628/8192\n",
      "test_loss:  1.22691092684\n",
      "17s - loss: 0.4750 - val_loss: 0.6485\n",
      "Epoch 629/8192\n",
      "test_loss:  1.22766780585\n",
      "17s - loss: 0.4759 - val_loss: 0.6502\n",
      "Epoch 630/8192\n",
      "test_loss:  1.22707249668\n",
      "17s - loss: 0.4743 - val_loss: 0.6544\n",
      "Epoch 631/8192\n",
      "test_loss:  1.23888668558\n",
      "16s - loss: 0.4756 - val_loss: 0.6493\n",
      "Epoch 632/8192\n",
      "test_loss:  1.2216855533\n",
      "17s - loss: 0.4746 - val_loss: 0.6524\n",
      "Epoch 633/8192\n",
      "test_loss:  1.21909593095\n",
      "16s - loss: 0.4740 - val_loss: 0.6464\n",
      "Epoch 634/8192\n",
      "test_loss:  1.27163612869\n",
      "17s - loss: 0.4750 - val_loss: 0.6439\n",
      "Epoch 635/8192\n",
      "test_loss:  1.21360864506\n",
      "17s - loss: 0.4751 - val_loss: 0.6414\n",
      "Epoch 636/8192\n",
      "test_loss:  1.23984703565\n",
      "18s - loss: 0.4748 - val_loss: 0.6635\n",
      "Epoch 637/8192\n",
      "test_loss:  1.20531748243\n",
      "18s - loss: 0.4744 - val_loss: 0.6328\n",
      "Epoch 638/8192\n",
      "test_loss:  1.1966425891\n",
      "17s - loss: 0.4741 - val_loss: 0.6336\n",
      "Epoch 639/8192\n",
      "test_loss:  1.20988480201\n",
      "17s - loss: 0.4751 - val_loss: 0.6415\n",
      "Epoch 640/8192\n",
      "test_loss:  1.22752995723\n",
      "18s - loss: 0.4741 - val_loss: 0.6586\n",
      "Epoch 641/8192\n",
      "test_loss:  1.20194604052\n",
      "18s - loss: 0.4741 - val_loss: 0.6301\n",
      "Epoch 642/8192\n",
      "test_loss:  1.35440534995\n",
      "17s - loss: 0.4732 - val_loss: 0.7251\n",
      "Epoch 643/8192\n",
      "test_loss:  1.20028258601\n",
      "17s - loss: 0.4752 - val_loss: 0.6254\n",
      "Epoch 644/8192\n",
      "test_loss:  1.22637530561\n",
      "17s - loss: 0.4745 - val_loss: 0.6391\n",
      "Epoch 645/8192\n",
      "test_loss:  1.21808853425\n",
      "18s - loss: 0.4737 - val_loss: 0.6381\n",
      "Epoch 646/8192\n",
      "test_loss:  1.21621113937\n",
      "18s - loss: 0.4739 - val_loss: 0.6395\n",
      "Epoch 647/8192\n",
      "test_loss:  1.2087043104\n",
      "17s - loss: 0.4732 - val_loss: 0.6263\n",
      "Epoch 648/8192\n",
      "test_loss:  1.21798059071\n",
      "17s - loss: 0.4738 - val_loss: 0.6350\n",
      "Epoch 649/8192\n",
      "test_loss:  1.21021271458\n",
      "17s - loss: 0.4736 - val_loss: 0.6294\n",
      "Epoch 650/8192\n",
      "test_loss:  1.21589849695\n",
      "17s - loss: 0.4743 - val_loss: 0.6276\n",
      "Epoch 651/8192\n",
      "test_loss:  1.20462370165\n",
      "18s - loss: 0.4721 - val_loss: 0.6199\n",
      "Epoch 652/8192\n",
      "test_loss:  1.22925295825\n",
      "17s - loss: 0.4731 - val_loss: 0.6472\n",
      "Epoch 653/8192\n",
      "test_loss:  1.23015161804\n",
      "18s - loss: 0.4732 - val_loss: 0.6376\n",
      "Epoch 654/8192\n",
      "test_loss:  1.23046742933\n",
      "17s - loss: 0.4723 - val_loss: 0.6384\n",
      "Epoch 655/8192\n",
      "test_loss:  1.24460331464\n",
      "17s - loss: 0.4736 - val_loss: 0.6622\n",
      "Epoch 656/8192\n",
      "test_loss:  1.23332860137\n",
      "17s - loss: 0.4727 - val_loss: 0.6559\n",
      "Epoch 657/8192\n",
      "test_loss:  1.22562184937\n",
      "17s - loss: 0.4716 - val_loss: 0.6379\n",
      "Epoch 658/8192\n",
      "test_loss:  1.24860723151\n",
      "17s - loss: 0.4720 - val_loss: 0.6482\n",
      "Epoch 659/8192\n",
      "test_loss:  1.26183501235\n",
      "17s - loss: 0.4718 - val_loss: 0.6508\n",
      "Epoch 660/8192\n",
      "test_loss:  1.20901516581\n",
      "17s - loss: 0.4714 - val_loss: 0.6288\n",
      "Epoch 661/8192\n",
      "test_loss:  1.19995308762\n",
      "18s - loss: 0.4718 - val_loss: 0.6363\n",
      "Epoch 662/8192\n",
      "test_loss:  1.22434880573\n",
      "17s - loss: 0.4721 - val_loss: 0.6369\n",
      "Epoch 663/8192\n",
      "test_loss:  1.20615431104\n",
      "17s - loss: 0.4728 - val_loss: 0.6396\n",
      "Epoch 664/8192\n",
      "test_loss:  1.21753743678\n",
      "17s - loss: 0.4724 - val_loss: 0.6257\n",
      "Epoch 665/8192\n",
      "test_loss:  1.21565618123\n",
      "18s - loss: 0.4712 - val_loss: 0.6352\n",
      "Epoch 666/8192\n",
      "test_loss:  1.22555791104\n",
      "18s - loss: 0.4726 - val_loss: 0.6494\n",
      "Epoch 667/8192\n",
      "test_loss:  1.21584128153\n",
      "17s - loss: 0.4732 - val_loss: 0.6415\n",
      "Epoch 668/8192\n",
      "test_loss:  1.21959538424\n",
      "18s - loss: 0.4716 - val_loss: 0.6339\n",
      "Epoch 669/8192\n",
      "test_loss:  1.21870430717\n",
      "17s - loss: 0.4720 - val_loss: 0.6372\n",
      "Epoch 670/8192\n",
      "test_loss:  1.19783964307\n",
      "17s - loss: 0.4721 - val_loss: 0.6234\n",
      "Epoch 671/8192\n",
      "test_loss:  1.23476731492\n",
      "18s - loss: 0.4715 - val_loss: 0.6444\n",
      "Epoch 672/8192\n",
      "test_loss:  1.24241455253\n",
      "18s - loss: 0.4707 - val_loss: 0.6457\n",
      "Epoch 673/8192\n",
      "test_loss:  1.2175532711\n",
      "17s - loss: 0.4709 - val_loss: 0.6228\n",
      "Epoch 674/8192\n",
      "test_loss:  1.23271884799\n",
      "18s - loss: 0.4720 - val_loss: 0.6313\n",
      "Epoch 675/8192\n",
      "test_loss:  1.23334009451\n",
      "17s - loss: 0.4708 - val_loss: 0.6364\n",
      "Epoch 676/8192\n",
      "test_loss:  1.20011646892\n",
      "17s - loss: 0.4709 - val_loss: 0.6117\n",
      "Epoch 677/8192\n",
      "test_loss:  1.19675915114\n",
      "17s - loss: 0.4718 - val_loss: 0.6450\n",
      "Epoch 678/8192\n",
      "test_loss:  1.21570968826\n",
      "17s - loss: 0.4703 - val_loss: 0.6276\n",
      "Epoch 679/8192\n",
      "test_loss:  1.22788787527\n",
      "17s - loss: 0.4714 - val_loss: 0.6188\n",
      "Epoch 680/8192\n",
      "test_loss:  1.23979040383\n",
      "17s - loss: 0.4717 - val_loss: 0.6441\n",
      "Epoch 681/8192\n",
      "test_loss:  1.23212882619\n",
      "17s - loss: 0.4694 - val_loss: 0.6351\n",
      "Epoch 682/8192\n",
      "test_loss:  1.20127903482\n",
      "17s - loss: 0.4710 - val_loss: 0.6147\n",
      "Epoch 683/8192\n",
      "test_loss:  1.24067510836\n",
      "17s - loss: 0.4699 - val_loss: 0.6338\n",
      "Epoch 684/8192\n",
      "test_loss:  1.20402745972\n",
      "17s - loss: 0.4705 - val_loss: 0.6095\n",
      "Epoch 685/8192\n",
      "test_loss:  1.2184190002\n",
      "18s - loss: 0.4703 - val_loss: 0.6252\n",
      "Epoch 686/8192\n",
      "test_loss:  1.20118597946\n",
      "18s - loss: 0.4693 - val_loss: 0.6174\n",
      "Epoch 687/8192\n",
      "test_loss:  1.23719798513\n",
      "17s - loss: 0.4697 - val_loss: 0.6334\n",
      "Epoch 688/8192\n",
      "test_loss:  1.24288562971\n",
      "17s - loss: 0.4699 - val_loss: 0.6336\n",
      "Epoch 689/8192\n",
      "test_loss:  1.19211994234\n",
      "17s - loss: 0.4696 - val_loss: 0.6165\n",
      "Epoch 690/8192\n",
      "test_loss:  1.21326937637\n",
      "17s - loss: 0.4707 - val_loss: 0.6152\n",
      "Epoch 691/8192\n",
      "test_loss:  1.21971891533\n",
      "17s - loss: 0.4689 - val_loss: 0.6296\n",
      "Epoch 692/8192\n",
      "test_loss:  1.20417974603\n",
      "18s - loss: 0.4695 - val_loss: 0.6160\n",
      "Epoch 693/8192\n",
      "test_loss:  1.21913469265\n",
      "17s - loss: 0.4691 - val_loss: 0.6242\n",
      "Epoch 694/8192\n",
      "test_loss:  1.2252407694\n",
      "17s - loss: 0.4704 - val_loss: 0.6431\n",
      "Epoch 695/8192\n",
      "test_loss:  1.20404157951\n",
      "18s - loss: 0.4690 - val_loss: 0.6411\n",
      "Epoch 696/8192\n",
      "test_loss:  1.22934119779\n",
      "17s - loss: 0.4701 - val_loss: 0.6277\n",
      "Epoch 697/8192\n",
      "test_loss:  1.21772327217\n",
      "17s - loss: 0.4689 - val_loss: 0.6106\n",
      "Epoch 698/8192\n",
      "test_loss:  1.2124561268\n",
      "17s - loss: 0.4703 - val_loss: 0.6115\n",
      "Epoch 699/8192\n",
      "test_loss:  1.24144407461\n",
      "17s - loss: 0.4691 - val_loss: 0.6258\n",
      "Epoch 700/8192\n",
      "test_loss:  1.23544591911\n",
      "18s - loss: 0.4706 - val_loss: 0.6484\n",
      "Epoch 701/8192\n",
      "test_loss:  1.20520559081\n",
      "17s - loss: 0.4698 - val_loss: 0.6136\n",
      "Epoch 702/8192\n",
      "test_loss:  1.20119337269\n",
      "17s - loss: 0.4690 - val_loss: 0.6094\n",
      "Epoch 703/8192\n",
      "test_loss:  1.22836822715\n",
      "17s - loss: 0.4680 - val_loss: 0.6260\n",
      "Epoch 704/8192\n",
      "test_loss:  1.22087352195\n",
      "16s - loss: 0.4685 - val_loss: 0.6336\n",
      "Epoch 705/8192\n",
      "test_loss:  1.19652074842\n",
      "17s - loss: 0.4704 - val_loss: 0.6115\n",
      "Epoch 706/8192\n",
      "test_loss:  1.21240221917\n",
      "17s - loss: 0.4688 - val_loss: 0.6086\n",
      "Epoch 707/8192\n",
      "test_loss:  1.20482268816\n",
      "17s - loss: 0.4680 - val_loss: 0.6163\n",
      "Epoch 708/8192\n",
      "test_loss:  1.20059074352\n",
      "17s - loss: 0.4675 - val_loss: 0.6083\n",
      "Epoch 709/8192\n",
      "test_loss:  1.19275518212\n",
      "17s - loss: 0.4684 - val_loss: 0.6145\n",
      "Epoch 710/8192\n",
      "test_loss:  1.23265490971\n",
      "16s - loss: 0.4678 - val_loss: 0.6227\n",
      "Epoch 711/8192\n",
      "test_loss:  1.23375409342\n",
      "18s - loss: 0.4679 - val_loss: 0.6231\n",
      "Epoch 712/8192\n",
      "test_loss:  1.19748084732\n",
      "18s - loss: 0.4694 - val_loss: 0.6208\n",
      "Epoch 713/8192\n",
      "test_loss:  1.19714723284\n",
      "17s - loss: 0.4686 - val_loss: 0.5988\n",
      "Epoch 714/8192\n",
      "test_loss:  1.21677127853\n",
      "17s - loss: 0.4694 - val_loss: 0.6281\n",
      "Epoch 715/8192\n",
      "test_loss:  1.25908483797\n",
      "17s - loss: 0.4678 - val_loss: 0.6235\n",
      "Epoch 716/8192\n",
      "test_loss:  1.20088821781\n",
      "17s - loss: 0.4675 - val_loss: 0.6009\n",
      "Epoch 717/8192\n",
      "test_loss:  1.1953374372\n",
      "17s - loss: 0.4668 - val_loss: 0.6012\n",
      "Epoch 718/8192\n",
      "test_loss:  1.19564888283\n",
      "17s - loss: 0.4685 - val_loss: 0.6170\n",
      "Epoch 719/8192\n",
      "test_loss:  1.20552353942\n",
      "17s - loss: 0.4676 - val_loss: 0.6090\n",
      "Epoch 720/8192\n",
      "test_loss:  1.21383909992\n",
      "17s - loss: 0.4676 - val_loss: 0.6158\n",
      "Epoch 721/8192\n",
      "test_loss:  1.24020435582\n",
      "17s - loss: 0.4675 - val_loss: 0.6422\n",
      "Epoch 722/8192\n",
      "test_loss:  1.24842782481\n",
      "18s - loss: 0.4675 - val_loss: 0.6476\n",
      "Epoch 723/8192\n",
      "test_loss:  1.21481101463\n",
      "17s - loss: 0.4669 - val_loss: 0.6162\n",
      "Epoch 724/8192\n",
      "test_loss:  1.20245153169\n",
      "17s - loss: 0.4667 - val_loss: 0.6096\n",
      "Epoch 725/8192\n",
      "test_loss:  1.21337238421\n",
      "17s - loss: 0.4668 - val_loss: 0.6254\n",
      "Epoch 726/8192\n",
      "test_loss:  1.20655152813\n",
      "17s - loss: 0.4673 - val_loss: 0.6068\n",
      "Epoch 727/8192\n",
      "test_loss:  1.22134328183\n",
      "17s - loss: 0.4671 - val_loss: 0.6314\n",
      "Epoch 728/8192\n",
      "test_loss:  1.20004529682\n",
      "17s - loss: 0.4667 - val_loss: 0.6061\n",
      "Epoch 729/8192\n",
      "test_loss:  1.20731649523\n",
      "17s - loss: 0.4672 - val_loss: 0.6014\n",
      "Epoch 730/8192\n",
      "test_loss:  1.20646540118\n",
      "17s - loss: 0.4670 - val_loss: 0.6188\n",
      "Epoch 731/8192\n",
      "test_loss:  1.20898977458\n",
      "17s - loss: 0.4681 - val_loss: 0.6129\n",
      "Epoch 732/8192\n",
      "test_loss:  1.21085082776\n",
      "17s - loss: 0.4670 - val_loss: 0.6135\n",
      "Epoch 733/8192\n",
      "test_loss:  1.22359241543\n",
      "18s - loss: 0.4668 - val_loss: 0.6183\n",
      "Epoch 734/8192\n",
      "test_loss:  1.19651694365\n",
      "18s - loss: 0.4676 - val_loss: 0.5974\n",
      "Epoch 735/8192\n",
      "test_loss:  1.2041755875\n",
      "17s - loss: 0.4667 - val_loss: 0.6297\n",
      "Epoch 736/8192\n",
      "test_loss:  1.20064091466\n",
      "18s - loss: 0.4658 - val_loss: 0.5968\n",
      "Epoch 737/8192\n",
      "test_loss:  1.19356884485\n",
      "18s - loss: 0.4663 - val_loss: 0.5941\n",
      "Epoch 738/8192\n",
      "test_loss:  1.22361789443\n",
      "18s - loss: 0.4661 - val_loss: 0.6122\n",
      "Epoch 739/8192\n",
      "test_loss:  1.19193263834\n",
      "18s - loss: 0.4658 - val_loss: 0.6104\n",
      "Epoch 740/8192\n",
      "test_loss:  1.21217375616\n",
      "17s - loss: 0.4662 - val_loss: 0.6021\n",
      "Epoch 741/8192\n",
      "test_loss:  1.2076363913\n",
      "17s - loss: 0.4670 - val_loss: 0.6082\n",
      "Epoch 742/8192\n",
      "test_loss:  1.20297637631\n",
      "18s - loss: 0.4671 - val_loss: 0.6111\n",
      "Epoch 743/8192\n",
      "test_loss:  1.23830340375\n",
      "17s - loss: 0.4664 - val_loss: 0.6292\n",
      "Epoch 744/8192\n",
      "test_loss:  1.20296520707\n",
      "17s - loss: 0.4656 - val_loss: 0.6042\n",
      "Epoch 745/8192\n",
      "test_loss:  1.19771160034\n",
      "18s - loss: 0.4666 - val_loss: 0.6136\n",
      "Epoch 746/8192\n",
      "test_loss:  1.20194502533\n",
      "17s - loss: 0.4676 - val_loss: 0.6228\n",
      "Epoch 747/8192\n",
      "test_loss:  1.21347073161\n",
      "17s - loss: 0.4666 - val_loss: 0.6060\n",
      "Epoch 748/8192\n",
      "test_loss:  1.21127322184\n",
      "17s - loss: 0.4671 - val_loss: 0.6012\n",
      "Epoch 749/8192\n",
      "test_loss:  1.22002828094\n",
      "17s - loss: 0.4658 - val_loss: 0.6098\n",
      "Epoch 750/8192\n",
      "test_loss:  1.21492662575\n",
      "17s - loss: 0.4652 - val_loss: 0.6123\n",
      "Epoch 751/8192\n",
      "test_loss:  1.1914865973\n",
      "17s - loss: 0.4659 - val_loss: 0.5902\n",
      "Epoch 752/8192\n",
      "test_loss:  1.22079675271\n",
      "17s - loss: 0.4659 - val_loss: 0.6387\n",
      "Epoch 753/8192\n",
      "test_loss:  1.19352557715\n",
      "17s - loss: 0.4646 - val_loss: 0.5927\n",
      "Epoch 754/8192\n",
      "test_loss:  1.215617277\n",
      "18s - loss: 0.4649 - val_loss: 0.6079\n",
      "Epoch 755/8192\n",
      "test_loss:  1.21496495019\n",
      "17s - loss: 0.4656 - val_loss: 0.6104\n",
      "Epoch 756/8192\n",
      "test_loss:  1.22596554215\n",
      "17s - loss: 0.4651 - val_loss: 0.6133\n",
      "Epoch 757/8192\n",
      "test_loss:  1.23418987845\n",
      "16s - loss: 0.4665 - val_loss: 0.6168\n",
      "Epoch 758/8192\n",
      "test_loss:  1.20086306226\n",
      "17s - loss: 0.4654 - val_loss: 0.6009\n",
      "Epoch 759/8192\n",
      "test_loss:  1.20659685932\n",
      "17s - loss: 0.4658 - val_loss: 0.6077\n",
      "Epoch 760/8192\n",
      "test_loss:  1.21574496123\n",
      "17s - loss: 0.4658 - val_loss: 0.5957\n",
      "Epoch 761/8192\n",
      "test_loss:  1.193703153\n",
      "18s - loss: 0.4649 - val_loss: 0.5936\n",
      "Epoch 762/8192\n",
      "test_loss:  1.22777703752\n",
      "17s - loss: 0.4651 - val_loss: 0.6183\n",
      "Epoch 763/8192\n",
      "test_loss:  1.21620197048\n",
      "17s - loss: 0.4650 - val_loss: 0.6192\n",
      "Epoch 764/8192\n",
      "test_loss:  1.19954561871\n",
      "17s - loss: 0.4652 - val_loss: 0.6005\n",
      "Epoch 765/8192\n",
      "test_loss:  1.24106278882\n",
      "17s - loss: 0.4660 - val_loss: 0.6217\n",
      "Epoch 766/8192\n",
      "test_loss:  1.19966598547\n",
      "17s - loss: 0.4655 - val_loss: 0.5988\n",
      "Epoch 767/8192\n",
      "test_loss:  1.25689295763\n",
      "17s - loss: 0.4642 - val_loss: 0.6439\n",
      "Epoch 768/8192\n",
      "test_loss:  1.22409505773\n",
      "17s - loss: 0.4656 - val_loss: 0.6246\n",
      "Epoch 769/8192\n",
      "test_loss:  1.21829319819\n",
      "17s - loss: 0.4642 - val_loss: 0.6073\n",
      "Epoch 770/8192\n",
      "test_loss:  1.38001164613\n",
      "17s - loss: 0.4644 - val_loss: 0.7161\n",
      "Epoch 771/8192\n",
      "test_loss:  1.30575647264\n",
      "17s - loss: 0.4642 - val_loss: 0.6585\n",
      "Epoch 772/8192\n",
      "test_loss:  1.22681313228\n",
      "17s - loss: 0.4646 - val_loss: 0.6118\n",
      "Epoch 773/8192\n",
      "test_loss:  1.23290986917\n",
      "18s - loss: 0.4635 - val_loss: 0.6221\n",
      "Epoch 774/8192\n",
      "test_loss:  1.24309341775\n",
      "17s - loss: 0.4639 - val_loss: 0.5958\n",
      "Epoch 775/8192\n",
      "test_loss:  1.20349341433\n",
      "18s - loss: 0.4642 - val_loss: 0.5856\n",
      "Epoch 776/8192\n",
      "test_loss:  1.18860259576\n",
      "18s - loss: 0.4644 - val_loss: 0.5836\n",
      "Epoch 777/8192\n",
      "test_loss:  1.21736253153\n",
      "18s - loss: 0.4639 - val_loss: 0.6072\n",
      "Epoch 778/8192\n",
      "test_loss:  1.21659630577\n",
      "17s - loss: 0.4643 - val_loss: 0.6068\n",
      "Epoch 779/8192\n",
      "test_loss:  1.21848957578\n",
      "18s - loss: 0.4635 - val_loss: 0.6111\n",
      "Epoch 780/8192\n",
      "test_loss:  1.21603881372\n",
      "17s - loss: 0.4645 - val_loss: 0.5960\n",
      "Epoch 781/8192\n",
      "test_loss:  1.20021500077\n",
      "17s - loss: 0.4642 - val_loss: 0.6026\n",
      "Epoch 782/8192\n",
      "test_loss:  1.19722229147\n",
      "17s - loss: 0.4643 - val_loss: 0.6042\n",
      "Epoch 783/8192\n",
      "test_loss:  1.22033549464\n",
      "17s - loss: 0.4637 - val_loss: 0.5993\n",
      "Epoch 784/8192\n",
      "test_loss:  1.243254584\n",
      "17s - loss: 0.4632 - val_loss: 0.6005\n",
      "Epoch 785/8192\n",
      "test_loss:  1.19977146805\n",
      "17s - loss: 0.4630 - val_loss: 0.6054\n",
      "Epoch 786/8192\n",
      "test_loss:  1.19737276842\n",
      "18s - loss: 0.4644 - val_loss: 0.5904\n",
      "Epoch 787/8192\n",
      "test_loss:  1.21861404254\n",
      "18s - loss: 0.4650 - val_loss: 0.6046\n",
      "Epoch 788/8192\n",
      "test_loss:  1.21436680613\n",
      "17s - loss: 0.4639 - val_loss: 0.5999\n",
      "Epoch 789/8192\n",
      "test_loss:  1.20754295489\n",
      "17s - loss: 0.4651 - val_loss: 0.5963\n",
      "Epoch 790/8192\n",
      "test_loss:  1.21029804426\n",
      "17s - loss: 0.4635 - val_loss: 0.6151\n",
      "Epoch 791/8192\n",
      "test_loss:  1.19859980011\n",
      "18s - loss: 0.4648 - val_loss: 0.5991\n",
      "Epoch 792/8192\n",
      "test_loss:  1.26832674409\n",
      "18s - loss: 0.4636 - val_loss: 0.6011\n",
      "Epoch 793/8192\n",
      "test_loss:  1.21073197942\n",
      "17s - loss: 0.4630 - val_loss: 0.5963\n",
      "Epoch 794/8192\n",
      "test_loss:  1.22118273805\n",
      "17s - loss: 0.4643 - val_loss: 0.5999\n",
      "Epoch 795/8192\n",
      "test_loss:  1.22219622288\n",
      "17s - loss: 0.4632 - val_loss: 0.6018\n",
      "Epoch 796/8192\n",
      "test_loss:  1.21394940641\n",
      "18s - loss: 0.4646 - val_loss: 0.6121\n",
      "Epoch 797/8192\n",
      "test_loss:  1.20307310401\n",
      "17s - loss: 0.4629 - val_loss: 0.5969\n",
      "Epoch 798/8192\n",
      "test_loss:  1.24358575076\n",
      "17s - loss: 0.4632 - val_loss: 0.6101\n",
      "Epoch 799/8192\n",
      "test_loss:  1.20520237395\n",
      "17s - loss: 0.4626 - val_loss: 0.6127\n",
      "Epoch 800/8192\n",
      "test_loss:  1.21952378\n",
      "17s - loss: 0.4633 - val_loss: 0.5966\n",
      "Epoch 801/8192\n",
      "test_loss:  1.2033057701\n",
      "17s - loss: 0.4631 - val_loss: 0.6062\n",
      "Epoch 802/8192\n",
      "test_loss:  1.20675030674\n",
      "17s - loss: 0.4615 - val_loss: 0.5988\n",
      "Epoch 803/8192\n",
      "test_loss:  1.18449176466\n",
      "17s - loss: 0.4625 - val_loss: 0.6005\n",
      "Epoch 804/8192\n",
      "test_loss:  1.22387705341\n",
      "17s - loss: 0.4622 - val_loss: 0.6178\n",
      "Epoch 805/8192\n",
      "test_loss:  1.20978392033\n",
      "17s - loss: 0.4626 - val_loss: 0.6295\n",
      "Epoch 806/8192\n",
      "test_loss:  1.22574272778\n",
      "17s - loss: 0.4644 - val_loss: 0.6129\n",
      "Epoch 807/8192\n",
      "test_loss:  1.19759299255\n",
      "18s - loss: 0.4619 - val_loss: 0.5882\n",
      "Epoch 808/8192\n",
      "test_loss:  1.22064336775\n",
      "17s - loss: 0.4632 - val_loss: 0.6087\n",
      "Epoch 809/8192\n",
      "test_loss:  1.20713887463\n",
      "17s - loss: 0.4633 - val_loss: 0.6164\n",
      "Epoch 810/8192\n",
      "test_loss:  1.20790883047\n",
      "17s - loss: 0.4638 - val_loss: 0.5995\n",
      "Epoch 811/8192\n",
      "test_loss:  1.21405948773\n",
      "17s - loss: 0.4615 - val_loss: 0.5896\n",
      "Epoch 812/8192\n",
      "test_loss:  1.19678307139\n",
      "17s - loss: 0.4623 - val_loss: 0.6010\n",
      "Epoch 813/8192\n",
      "test_loss:  1.2004174205\n",
      "17s - loss: 0.4618 - val_loss: 0.5943\n",
      "Epoch 814/8192\n",
      "test_loss:  1.20636735585\n",
      "18s - loss: 0.4634 - val_loss: 0.6242\n",
      "Epoch 815/8192\n",
      "test_loss:  1.22908133892\n",
      "18s - loss: 0.4625 - val_loss: 0.6138\n",
      "Epoch 816/8192\n",
      "test_loss:  1.22703816551\n",
      "18s - loss: 0.4625 - val_loss: 0.5881\n",
      "Epoch 817/8192\n",
      "test_loss:  1.21359900318\n",
      "17s - loss: 0.4626 - val_loss: 0.6012\n",
      "Epoch 818/8192\n",
      "test_loss:  1.22702951662\n",
      "17s - loss: 0.4630 - val_loss: 0.6083\n",
      "Epoch 819/8192\n",
      "test_loss:  1.18859506568\n",
      "17s - loss: 0.4620 - val_loss: 0.5896\n",
      "Epoch 820/8192\n",
      "test_loss:  1.21879348446\n",
      "17s - loss: 0.4623 - val_loss: 0.6010\n",
      "Epoch 821/8192\n",
      "test_loss:  1.19338366612\n",
      "17s - loss: 0.4621 - val_loss: 0.5846\n",
      "Epoch 822/8192\n",
      "test_loss:  1.20875694451\n",
      "17s - loss: 0.4621 - val_loss: 0.6087\n",
      "Epoch 823/8192\n",
      "test_loss:  1.19686103432\n",
      "17s - loss: 0.4627 - val_loss: 0.5878\n",
      "Epoch 824/8192\n",
      "test_loss:  1.20448023369\n",
      "18s - loss: 0.4612 - val_loss: 0.5907\n",
      "Epoch 825/8192\n",
      "test_loss:  1.25517056419\n",
      "17s - loss: 0.4625 - val_loss: 0.6167\n",
      "Epoch 826/8192\n",
      "test_loss:  1.20896338848\n",
      "17s - loss: 0.4620 - val_loss: 0.5918\n",
      "Epoch 827/8192\n",
      "test_loss:  1.20247405417\n",
      "17s - loss: 0.4631 - val_loss: 0.5896\n",
      "Epoch 828/8192\n",
      "test_loss:  1.21161082692\n",
      "17s - loss: 0.4618 - val_loss: 0.5832\n",
      "Epoch 829/8192\n",
      "test_loss:  1.19335927519\n",
      "18s - loss: 0.4627 - val_loss: 0.5936\n",
      "Epoch 830/8192\n",
      "test_loss:  1.2153547758\n",
      "18s - loss: 0.4620 - val_loss: 0.6025\n",
      "Epoch 831/8192\n",
      "test_loss:  1.20408364739\n",
      "17s - loss: 0.4622 - val_loss: 0.5975\n",
      "Epoch 832/8192\n",
      "test_loss:  1.21798678266\n",
      "17s - loss: 0.4613 - val_loss: 0.5990\n",
      "Epoch 833/8192\n",
      "test_loss:  1.2159115664\n",
      "18s - loss: 0.4611 - val_loss: 0.6237\n",
      "Epoch 834/8192\n",
      "test_loss:  1.22527405997\n",
      "17s - loss: 0.4621 - val_loss: 0.6041\n",
      "Epoch 835/8192\n",
      "test_loss:  1.22561963504\n",
      "18s - loss: 0.4614 - val_loss: 0.6016\n",
      "Epoch 836/8192\n",
      "test_loss:  1.2235773237\n",
      "17s - loss: 0.4624 - val_loss: 0.5953\n",
      "Epoch 837/8192\n",
      "test_loss:  1.19810828044\n",
      "17s - loss: 0.4624 - val_loss: 0.5997\n",
      "Epoch 838/8192\n",
      "test_loss:  1.20895674777\n",
      "17s - loss: 0.4617 - val_loss: 0.6066\n",
      "Epoch 839/8192\n",
      "test_loss:  1.24555314141\n",
      "17s - loss: 0.4618 - val_loss: 0.6024\n",
      "Epoch 840/8192\n",
      "test_loss:  1.23513441008\n",
      "17s - loss: 0.4634 - val_loss: 0.5993\n",
      "Epoch 841/8192\n",
      "test_loss:  1.20367556005\n",
      "17s - loss: 0.4623 - val_loss: 0.5976\n",
      "Epoch 842/8192\n",
      "test_loss:  1.21120380592\n",
      "17s - loss: 0.4615 - val_loss: 0.5856\n",
      "Epoch 843/8192\n",
      "test_loss:  1.19531668337\n",
      "17s - loss: 0.4609 - val_loss: 0.5868\n",
      "Epoch 844/8192\n",
      "test_loss:  1.1929064156\n",
      "17s - loss: 0.4623 - val_loss: 0.6093\n",
      "Epoch 845/8192\n",
      "test_loss:  1.21966608065\n",
      "17s - loss: 0.4626 - val_loss: 0.5975\n",
      "Epoch 846/8192\n",
      "test_loss:  1.2079038995\n",
      "18s - loss: 0.4609 - val_loss: 0.6120\n",
      "Epoch 847/8192\n",
      "test_loss:  1.21895421004\n",
      "17s - loss: 0.4612 - val_loss: 0.6243\n",
      "Epoch 848/8192\n",
      "test_loss:  1.22560422793\n",
      "17s - loss: 0.4616 - val_loss: 0.5991\n",
      "Epoch 849/8192\n",
      "test_loss:  1.21824725433\n",
      "17s - loss: 0.4620 - val_loss: 0.6398\n",
      "Epoch 850/8192\n",
      "test_loss:  1.20797406858\n",
      "17s - loss: 0.4610 - val_loss: 0.5899\n",
      "Epoch 851/8192\n",
      "test_loss:  1.18684108739\n",
      "17s - loss: 0.4596 - val_loss: 0.6012\n",
      "Epoch 852/8192\n",
      "test_loss:  1.22192924148\n",
      "17s - loss: 0.4616 - val_loss: 0.5938\n",
      "Epoch 853/8192\n",
      "test_loss:  1.19324228638\n",
      "17s - loss: 0.4616 - val_loss: 0.5845\n",
      "Epoch 854/8192\n",
      "test_loss:  1.23151940954\n",
      "17s - loss: 0.4618 - val_loss: 0.6139\n",
      "Epoch 855/8192\n",
      "test_loss:  1.19379943288\n",
      "18s - loss: 0.4604 - val_loss: 0.5842\n",
      "Epoch 856/8192\n",
      "test_loss:  1.23386794797\n",
      "17s - loss: 0.4609 - val_loss: 0.5989\n",
      "Epoch 857/8192\n",
      "test_loss:  1.20434675818\n",
      "17s - loss: 0.4608 - val_loss: 0.6066\n",
      "Epoch 858/8192\n",
      "test_loss:  1.22821981269\n",
      "17s - loss: 0.4610 - val_loss: 0.6066\n",
      "Epoch 859/8192\n",
      "test_loss:  1.22465705614\n",
      "18s - loss: 0.4599 - val_loss: 0.5909\n",
      "Epoch 860/8192\n",
      "test_loss:  1.20672279635\n",
      "17s - loss: 0.4608 - val_loss: 0.5954\n",
      "Epoch 861/8192\n",
      "test_loss:  1.20124952563\n",
      "17s - loss: 0.4601 - val_loss: 0.5914\n",
      "Epoch 862/8192\n",
      "test_loss:  1.18599722471\n",
      "17s - loss: 0.4602 - val_loss: 0.6133\n",
      "Epoch 863/8192\n",
      "test_loss:  1.23900629753\n",
      "17s - loss: 0.4603 - val_loss: 0.6017\n",
      "Epoch 864/8192\n",
      "test_loss:  1.21043469737\n",
      "17s - loss: 0.4603 - val_loss: 0.6064\n",
      "Epoch 865/8192\n",
      "test_loss:  1.22665719145\n",
      "17s - loss: 0.4617 - val_loss: 0.5979\n",
      "Epoch 866/8192\n",
      "test_loss:  1.19736194489\n",
      "17s - loss: 0.4595 - val_loss: 0.6104\n",
      "Epoch 867/8192\n",
      "test_loss:  1.20034990008\n",
      "18s - loss: 0.4595 - val_loss: 0.5869\n",
      "Epoch 868/8192\n",
      "test_loss:  1.19466758704\n",
      "18s - loss: 0.4609 - val_loss: 0.5846\n",
      "Epoch 869/8192\n",
      "test_loss:  1.21899114119\n",
      "18s - loss: 0.4589 - val_loss: 0.5994\n",
      "Epoch 870/8192\n",
      "test_loss:  1.20112981142\n",
      "17s - loss: 0.4602 - val_loss: 0.6009\n",
      "Epoch 871/8192\n",
      "test_loss:  1.22742270014\n",
      "17s - loss: 0.4599 - val_loss: 0.5888\n",
      "Epoch 872/8192\n",
      "test_loss:  1.21824536058\n",
      "18s - loss: 0.4599 - val_loss: 0.6074\n",
      "Epoch 873/8192\n",
      "test_loss:  1.21261128986\n",
      "17s - loss: 0.4592 - val_loss: 0.5899\n",
      "Epoch 874/8192\n",
      "test_loss:  1.22959451802\n",
      "17s - loss: 0.4618 - val_loss: 0.6019\n",
      "Epoch 875/8192\n",
      "test_loss:  1.20323955714\n",
      "17s - loss: 0.4612 - val_loss: 0.6101\n",
      "Epoch 876/8192\n",
      "test_loss:  1.20989123392\n",
      "17s - loss: 0.4599 - val_loss: 0.5953\n",
      "Epoch 877/8192\n",
      "test_loss:  1.21185509532\n",
      "18s - loss: 0.4597 - val_loss: 0.6245\n",
      "Epoch 878/8192\n",
      "test_loss:  1.23883597522\n",
      "17s - loss: 0.4595 - val_loss: 0.6011\n",
      "Epoch 879/8192\n",
      "test_loss:  1.2386508078\n",
      "18s - loss: 0.4593 - val_loss: 0.6456\n",
      "Epoch 880/8192\n",
      "test_loss:  1.20411732025\n",
      "17s - loss: 0.4606 - val_loss: 0.5875\n",
      "Epoch 881/8192\n",
      "test_loss:  1.24382416635\n",
      "17s - loss: 0.4593 - val_loss: 0.6050\n",
      "Epoch 882/8192\n",
      "test_loss:  1.21425617601\n",
      "17s - loss: 0.4607 - val_loss: 0.5935\n",
      "Epoch 883/8192\n",
      "test_loss:  1.19807624989\n",
      "18s - loss: 0.4601 - val_loss: 0.5803\n",
      "Epoch 884/8192\n",
      "test_loss:  1.22500231365\n",
      "17s - loss: 0.4603 - val_loss: 0.6097\n",
      "Epoch 885/8192\n",
      "test_loss:  1.21778599642\n",
      "17s - loss: 0.4598 - val_loss: 0.5912\n",
      "Epoch 886/8192\n",
      "test_loss:  1.21802810675\n",
      "17s - loss: 0.4593 - val_loss: 0.5931\n",
      "Epoch 887/8192\n",
      "test_loss:  1.24648786826\n",
      "17s - loss: 0.4588 - val_loss: 0.6230\n",
      "Epoch 888/8192\n",
      "test_loss:  1.20439406164\n",
      "17s - loss: 0.4602 - val_loss: 0.5851\n",
      "Epoch 889/8192\n",
      "test_loss:  1.21309594934\n",
      "18s - loss: 0.4591 - val_loss: 0.5952\n",
      "Epoch 890/8192\n",
      "test_loss:  1.22310882796\n",
      "17s - loss: 0.4591 - val_loss: 0.5890\n",
      "Epoch 891/8192\n",
      "test_loss:  1.20230157204\n",
      "17s - loss: 0.4597 - val_loss: 0.6020\n",
      "Epoch 892/8192\n",
      "test_loss:  1.2117783643\n",
      "17s - loss: 0.4608 - val_loss: 0.5997\n",
      "Epoch 893/8192\n",
      "test_loss:  1.21522639991\n",
      "17s - loss: 0.4591 - val_loss: 0.5886\n",
      "Epoch 894/8192\n",
      "test_loss:  1.18836398674\n",
      "17s - loss: 0.4593 - val_loss: 0.5705\n",
      "Epoch 895/8192\n",
      "test_loss:  1.20980410456\n",
      "18s - loss: 0.4582 - val_loss: 0.5984\n",
      "Epoch 896/8192\n",
      "test_loss:  1.20699786626\n",
      "17s - loss: 0.4588 - val_loss: 0.5849\n",
      "Epoch 897/8192\n",
      "test_loss:  1.23553413217\n",
      "17s - loss: 0.4593 - val_loss: 0.5950\n",
      "Epoch 898/8192\n",
      "test_loss:  1.22340966831\n",
      "18s - loss: 0.4591 - val_loss: 0.5932\n",
      "Epoch 899/8192\n",
      "test_loss:  1.19851815477\n",
      "17s - loss: 0.4595 - val_loss: 0.5879\n",
      "Epoch 900/8192\n",
      "test_loss:  1.21331207103\n",
      "17s - loss: 0.4586 - val_loss: 0.6242\n",
      "Epoch 901/8192\n",
      "test_loss:  1.22012538387\n",
      "17s - loss: 0.4579 - val_loss: 0.6021\n",
      "Epoch 902/8192\n",
      "test_loss:  1.23327011493\n",
      "17s - loss: 0.4590 - val_loss: 0.5874\n",
      "Epoch 903/8192\n",
      "test_loss:  1.2556396532\n",
      "18s - loss: 0.4593 - val_loss: 0.5901\n",
      "Epoch 904/8192\n",
      "test_loss:  1.21554153341\n",
      "17s - loss: 0.4590 - val_loss: 0.5964\n",
      "Epoch 905/8192\n",
      "test_loss:  1.22826609775\n",
      "18s - loss: 0.4580 - val_loss: 0.5983\n",
      "Epoch 906/8192\n",
      "test_loss:  1.23724245571\n",
      "17s - loss: 0.4584 - val_loss: 0.6169\n",
      "Epoch 907/8192\n",
      "test_loss:  1.21255290177\n",
      "16s - loss: 0.4598 - val_loss: 0.5825\n",
      "Epoch 908/8192\n",
      "test_loss:  1.21539478239\n",
      "17s - loss: 0.4594 - val_loss: 0.5900\n",
      "Epoch 909/8192\n",
      "test_loss:  1.22392219904\n",
      "17s - loss: 0.4589 - val_loss: 0.5905\n",
      "Epoch 910/8192\n",
      "test_loss:  1.23532594442\n",
      "17s - loss: 0.4574 - val_loss: 0.6054\n",
      "Epoch 911/8192\n",
      "test_loss:  1.27574993025\n",
      "17s - loss: 0.4588 - val_loss: 0.6087\n",
      "Epoch 912/8192\n",
      "test_loss:  1.21596731701\n",
      "17s - loss: 0.4599 - val_loss: 0.5784\n",
      "Epoch 913/8192\n",
      "test_loss:  1.20889870723\n",
      "18s - loss: 0.4588 - val_loss: 0.5803\n",
      "Epoch 914/8192\n",
      "test_loss:  1.20909289163\n",
      "17s - loss: 0.4581 - val_loss: 0.6020\n",
      "Epoch 915/8192\n",
      "test_loss:  1.21884941636\n",
      "17s - loss: 0.4585 - val_loss: 0.5920\n",
      "Epoch 916/8192\n",
      "test_loss:  1.21314342339\n",
      "17s - loss: 0.4587 - val_loss: 0.5847\n",
      "Epoch 917/8192\n",
      "test_loss:  1.23972807176\n",
      "17s - loss: 0.4579 - val_loss: 0.6068\n",
      "Epoch 918/8192\n",
      "test_loss:  1.21635317562\n",
      "17s - loss: 0.4574 - val_loss: 0.5765\n",
      "Epoch 919/8192\n",
      "test_loss:  1.24132018596\n",
      "17s - loss: 0.4581 - val_loss: 0.6094\n",
      "Epoch 920/8192\n",
      "test_loss:  1.20439817675\n",
      "17s - loss: 0.4596 - val_loss: 0.6004\n",
      "Epoch 921/8192\n",
      "test_loss:  1.24061855216\n",
      "17s - loss: 0.4604 - val_loss: 0.5949\n",
      "Epoch 922/8192\n",
      "test_loss:  1.21695720509\n",
      "17s - loss: 0.4583 - val_loss: 0.5850\n",
      "Epoch 923/8192\n",
      "test_loss:  1.19513520308\n",
      "17s - loss: 0.4604 - val_loss: 0.5881\n",
      "Epoch 924/8192\n",
      "test_loss:  1.21461218407\n",
      "17s - loss: 0.4591 - val_loss: 0.6224\n",
      "Epoch 925/8192\n",
      "test_loss:  1.20184290759\n",
      "17s - loss: 0.4578 - val_loss: 0.5840\n",
      "Epoch 926/8192\n",
      "test_loss:  1.21983004548\n",
      "17s - loss: 0.4586 - val_loss: 0.6375\n",
      "Epoch 927/8192\n",
      "test_loss:  1.27126292805\n",
      "17s - loss: 0.4587 - val_loss: 0.5993\n",
      "Epoch 928/8192\n",
      "test_loss:  1.28760164305\n",
      "17s - loss: 0.4580 - val_loss: 0.5985\n",
      "Epoch 929/8192\n",
      "test_loss:  1.20456258371\n",
      "17s - loss: 0.4591 - val_loss: 0.5878\n",
      "Epoch 930/8192\n",
      "test_loss:  1.20826802231\n",
      "17s - loss: 0.4592 - val_loss: 0.5776\n",
      "Epoch 931/8192\n",
      "test_loss:  1.20768322034\n",
      "17s - loss: 0.4578 - val_loss: 0.5880\n",
      "Epoch 932/8192\n",
      "test_loss:  1.22526473276\n",
      "17s - loss: 0.4584 - val_loss: 0.6042\n",
      "Epoch 933/8192\n",
      "test_loss:  1.19990229063\n",
      "17s - loss: 0.4578 - val_loss: 0.5773\n",
      "Epoch 934/8192\n",
      "test_loss:  1.22027967712\n",
      "17s - loss: 0.4574 - val_loss: 0.5958\n",
      "Epoch 935/8192\n",
      "test_loss:  1.22108187995\n",
      "17s - loss: 0.4575 - val_loss: 0.5780\n",
      "Epoch 936/8192\n",
      "test_loss:  1.21304098687\n",
      "17s - loss: 0.4594 - val_loss: 0.5821\n",
      "Epoch 937/8192\n",
      "test_loss:  1.23618722778\n",
      "18s - loss: 0.4586 - val_loss: 0.5962\n",
      "Epoch 938/8192\n",
      "test_loss:  1.22864988291\n",
      "17s - loss: 0.4581 - val_loss: 0.6094\n",
      "Epoch 939/8192\n",
      "test_loss:  1.19802807176\n",
      "17s - loss: 0.4583 - val_loss: 0.5875\n",
      "Epoch 940/8192\n",
      "test_loss:  1.23194549104\n",
      "17s - loss: 0.4581 - val_loss: 0.6099\n",
      "Epoch 941/8192\n",
      "test_loss:  1.23666037641\n",
      "18s - loss: 0.4568 - val_loss: 0.6007\n",
      "Epoch 942/8192\n",
      "test_loss:  1.29175439199\n",
      "17s - loss: 0.4573 - val_loss: 0.6197\n",
      "Epoch 943/8192\n",
      "test_loss:  1.21927204653\n",
      "18s - loss: 0.4588 - val_loss: 0.6068\n",
      "Epoch 944/8192\n",
      "test_loss:  1.24418822837\n",
      "17s - loss: 0.4576 - val_loss: 0.5857\n",
      "Epoch 945/8192\n",
      "test_loss:  1.25461837601\n",
      "17s - loss: 0.4579 - val_loss: 0.5979\n",
      "Epoch 946/8192\n",
      "test_loss:  1.22124974532\n",
      "17s - loss: 0.4569 - val_loss: 0.5927\n",
      "Epoch 947/8192\n",
      "test_loss:  1.22548680711\n",
      "17s - loss: 0.4585 - val_loss: 0.6126\n",
      "Epoch 948/8192\n",
      "test_loss:  1.24190149067\n",
      "18s - loss: 0.4577 - val_loss: 0.6198\n",
      "Epoch 949/8192\n",
      "test_loss:  1.26464577171\n",
      "17s - loss: 0.4580 - val_loss: 0.5966\n",
      "Epoch 950/8192\n",
      "test_loss:  1.22073236608\n",
      "17s - loss: 0.4570 - val_loss: 0.5827\n",
      "Epoch 951/8192\n",
      "test_loss:  1.25934051722\n",
      "17s - loss: 0.4571 - val_loss: 0.5892\n",
      "Epoch 952/8192\n",
      "test_loss:  1.21625005924\n",
      "18s - loss: 0.4569 - val_loss: 0.5985\n",
      "Epoch 953/8192\n",
      "test_loss:  1.23798826166\n",
      "17s - loss: 0.4578 - val_loss: 0.5891\n",
      "Epoch 954/8192\n",
      "test_loss:  1.21332986691\n",
      "17s - loss: 0.4588 - val_loss: 0.5920\n",
      "Epoch 955/8192\n",
      "test_loss:  1.22484295766\n",
      "18s - loss: 0.4562 - val_loss: 0.6049\n",
      "Epoch 956/8192\n",
      "test_loss:  1.23271879166\n",
      "17s - loss: 0.4569 - val_loss: 0.5929\n",
      "Epoch 957/8192\n",
      "test_loss:  1.25767750788\n",
      "17s - loss: 0.4558 - val_loss: 0.5921\n",
      "Epoch 958/8192\n",
      "test_loss:  1.2988164744\n",
      "18s - loss: 0.4571 - val_loss: 0.6117\n",
      "Epoch 959/8192\n",
      "test_loss:  1.1864665195\n",
      "17s - loss: 0.4576 - val_loss: 0.6007\n",
      "Epoch 960/8192\n",
      "test_loss:  1.23661273367\n",
      "17s - loss: 0.4565 - val_loss: 0.5984\n",
      "Epoch 961/8192\n",
      "test_loss:  1.21754834615\n",
      "18s - loss: 0.4578 - val_loss: 0.5862\n",
      "Epoch 962/8192\n",
      "test_loss:  1.23262800872\n",
      "18s - loss: 0.4567 - val_loss: 0.5802\n",
      "Epoch 963/8192\n",
      "test_loss:  1.21690181689\n",
      "18s - loss: 0.4574 - val_loss: 0.5759\n",
      "Epoch 964/8192\n",
      "test_loss:  1.23162389785\n",
      "17s - loss: 0.4584 - val_loss: 0.5875\n",
      "Epoch 965/8192\n",
      "test_loss:  1.22201008562\n",
      "18s - loss: 0.4577 - val_loss: 0.5808\n",
      "Epoch 966/8192\n",
      "test_loss:  1.26126184147\n",
      "17s - loss: 0.4572 - val_loss: 0.6442\n",
      "Epoch 967/8192\n",
      "test_loss:  1.20272189039\n",
      "17s - loss: 0.4564 - val_loss: 0.5894\n",
      "Epoch 968/8192\n",
      "test_loss:  1.21304572188\n",
      "18s - loss: 0.4583 - val_loss: 0.5820\n",
      "Epoch 969/8192\n",
      "test_loss:  1.22310640116\n",
      "18s - loss: 0.4573 - val_loss: 0.5744\n",
      "Epoch 970/8192\n",
      "test_loss:  1.21087411374\n",
      "17s - loss: 0.4580 - val_loss: 0.5752\n",
      "Epoch 971/8192\n",
      "test_loss:  1.20034783111\n",
      "17s - loss: 0.4564 - val_loss: 0.6095\n",
      "Epoch 972/8192\n",
      "test_loss:  1.22281242959\n",
      "17s - loss: 0.4580 - val_loss: 0.5864\n",
      "Epoch 973/8192\n",
      "test_loss:  1.21721584673\n",
      "17s - loss: 0.4558 - val_loss: 0.5923\n",
      "Epoch 974/8192\n",
      "test_loss:  1.20514693757\n",
      "17s - loss: 0.4570 - val_loss: 0.5894\n",
      "Epoch 975/8192\n",
      "test_loss:  1.20658783456\n",
      "17s - loss: 0.4563 - val_loss: 0.5792\n",
      "Epoch 976/8192\n",
      "test_loss:  1.20167016803\n",
      "17s - loss: 0.4573 - val_loss: 0.5797\n",
      "Epoch 977/8192\n",
      "test_loss:  1.2448768895\n",
      "17s - loss: 0.4564 - val_loss: 0.6000\n",
      "Epoch 978/8192\n",
      "test_loss:  1.24925434617\n",
      "17s - loss: 0.4568 - val_loss: 0.5965\n",
      "Epoch 979/8192\n",
      "test_loss:  1.25204976239\n",
      "17s - loss: 0.4573 - val_loss: 0.6262\n",
      "Epoch 980/8192\n",
      "test_loss:  1.19382397454\n",
      "17s - loss: 0.4565 - val_loss: 0.5832\n",
      "Epoch 981/8192\n",
      "test_loss:  1.21405204757\n",
      "17s - loss: 0.4567 - val_loss: 0.6234\n",
      "Epoch 982/8192\n",
      "test_loss:  1.23453813149\n",
      "17s - loss: 0.4576 - val_loss: 0.5800\n",
      "Epoch 983/8192\n",
      "test_loss:  1.23212873946\n",
      "18s - loss: 0.4576 - val_loss: 0.5887\n",
      "Epoch 984/8192\n",
      "test_loss:  1.24798671309\n",
      "18s - loss: 0.4565 - val_loss: 0.5927\n",
      "Epoch 985/8192\n",
      "test_loss:  1.22236811642\n",
      "17s - loss: 0.4564 - val_loss: 0.6028\n",
      "Epoch 986/8192\n",
      "test_loss:  1.2191048427\n",
      "17s - loss: 0.4556 - val_loss: 0.5874\n",
      "Epoch 987/8192\n",
      "test_loss:  1.20704886335\n",
      "17s - loss: 0.4570 - val_loss: 0.5776\n",
      "Epoch 988/8192\n",
      "test_loss:  1.23471265339\n",
      "17s - loss: 0.4569 - val_loss: 0.5910\n",
      "Epoch 989/8192\n",
      "test_loss:  1.21963335473\n",
      "17s - loss: 0.4562 - val_loss: 0.5914\n",
      "Epoch 990/8192\n",
      "test_loss:  1.22203039953\n",
      "17s - loss: 0.4566 - val_loss: 0.5893\n",
      "Epoch 991/8192\n",
      "test_loss:  1.22961753814\n",
      "17s - loss: 0.4575 - val_loss: 0.5979\n",
      "Epoch 992/8192\n",
      "test_loss:  1.23406179818\n",
      "17s - loss: 0.4555 - val_loss: 0.6255\n",
      "Epoch 993/8192\n",
      "test_loss:  1.23050621797\n",
      "17s - loss: 0.4568 - val_loss: 0.5867\n",
      "Epoch 994/8192\n",
      "test_loss:  1.22801992162\n",
      "17s - loss: 0.4581 - val_loss: 0.5734\n",
      "Epoch 995/8192\n",
      "test_loss:  1.20705671161\n",
      "18s - loss: 0.4568 - val_loss: 0.6088\n",
      "Epoch 996/8192\n",
      "test_loss:  1.22275982731\n",
      "17s - loss: 0.4573 - val_loss: 0.5895\n",
      "Epoch 997/8192\n",
      "test_loss:  1.21895924627\n",
      "17s - loss: 0.4568 - val_loss: 0.6036\n",
      "Epoch 998/8192\n",
      "test_loss:  1.25450492261\n",
      "17s - loss: 0.4577 - val_loss: 0.5990\n",
      "Epoch 999/8192\n",
      "test_loss:  1.20775871615\n",
      "17s - loss: 0.4557 - val_loss: 0.5981\n",
      "Epoch 1000/8192\n",
      "test_loss:  1.19671993275\n",
      "17s - loss: 0.4568 - val_loss: 0.5718\n",
      "Epoch 1001/8192\n",
      "test_loss:  1.21723927898\n",
      "17s - loss: 0.4572 - val_loss: 0.5954\n",
      "Epoch 1002/8192\n",
      "test_loss:  1.18857320411\n",
      "17s - loss: 0.4554 - val_loss: 0.6027\n",
      "Epoch 1003/8192\n",
      "test_loss:  1.22294328257\n",
      "17s - loss: 0.4568 - val_loss: 0.5941\n",
      "Epoch 1004/8192\n",
      "test_loss:  1.24094208947\n",
      "17s - loss: 0.4575 - val_loss: 0.5874\n",
      "Epoch 1005/8192\n",
      "test_loss:  1.19846214783\n",
      "17s - loss: 0.4569 - val_loss: 0.5840\n",
      "Epoch 1006/8192\n",
      "test_loss:  1.22816998407\n",
      "17s - loss: 0.4558 - val_loss: 0.5829\n",
      "Epoch 1007/8192\n",
      "test_loss:  1.23830635972\n",
      "17s - loss: 0.4552 - val_loss: 0.6094\n",
      "Epoch 1008/8192\n",
      "test_loss:  1.24299540147\n",
      "18s - loss: 0.4557 - val_loss: 0.5851\n",
      "Epoch 1009/8192\n",
      "test_loss:  1.25126781492\n",
      "18s - loss: 0.4562 - val_loss: 0.6010\n",
      "Epoch 1010/8192\n",
      "test_loss:  1.24556815976\n",
      "18s - loss: 0.4564 - val_loss: 0.5702\n",
      "Epoch 1011/8192\n",
      "test_loss:  1.23583350998\n",
      "17s - loss: 0.4559 - val_loss: 0.6099\n",
      "Epoch 1012/8192\n",
      "test_loss:  1.2746724808\n",
      "17s - loss: 0.4566 - val_loss: 0.6300\n",
      "Epoch 1013/8192\n",
      "test_loss:  1.21413504826\n",
      "17s - loss: 0.4558 - val_loss: 0.5813\n",
      "Epoch 1014/8192\n",
      "test_loss:  1.2057808912\n",
      "17s - loss: 0.4540 - val_loss: 0.5861\n",
      "Epoch 1015/8192\n",
      "test_loss:  1.25050148789\n",
      "17s - loss: 0.4560 - val_loss: 0.5963\n",
      "Epoch 1016/8192\n",
      "test_loss:  1.19862813984\n",
      "17s - loss: 0.4551 - val_loss: 0.5946\n",
      "Epoch 1017/8192\n",
      "test_loss:  1.23298210764\n",
      "18s - loss: 0.4561 - val_loss: 0.5742\n",
      "Epoch 1018/8192\n",
      "test_loss:  1.22043508067\n",
      "18s - loss: 0.4553 - val_loss: 0.5709\n",
      "Epoch 1019/8192\n",
      "test_loss:  1.23373354362\n",
      "17s - loss: 0.4566 - val_loss: 0.5991\n",
      "Epoch 1020/8192\n",
      "test_loss:  1.22717184174\n",
      "17s - loss: 0.4569 - val_loss: 0.5777\n",
      "Epoch 1021/8192\n",
      "test_loss:  1.22016759837\n",
      "17s - loss: 0.4553 - val_loss: 0.5925\n",
      "Epoch 1022/8192\n",
      "test_loss:  1.25218395239\n",
      "18s - loss: 0.4561 - val_loss: 0.5873\n",
      "Epoch 1023/8192\n",
      "test_loss:  1.24368294817\n",
      "17s - loss: 0.4564 - val_loss: 0.5922\n",
      "Epoch 1024/8192\n",
      "test_loss:  1.23396686489\n",
      "17s - loss: 0.4565 - val_loss: 0.6111\n",
      "Epoch 1025/8192\n",
      "test_loss:  1.22933539753\n",
      "17s - loss: 0.4559 - val_loss: 0.5868\n",
      "Epoch 1026/8192\n",
      "test_loss:  1.21120244776\n",
      "18s - loss: 0.4549 - val_loss: 0.5787\n",
      "Epoch 1027/8192\n",
      "test_loss:  1.21706524838\n",
      "17s - loss: 0.4549 - val_loss: 0.5769\n",
      "Epoch 1028/8192\n",
      "test_loss:  1.21922436661\n",
      "17s - loss: 0.4561 - val_loss: 0.5711\n",
      "Epoch 1029/8192\n",
      "test_loss:  1.21971946434\n",
      "17s - loss: 0.4559 - val_loss: 0.5917\n",
      "Epoch 1030/8192\n",
      "test_loss:  1.21875223345\n",
      "17s - loss: 0.4571 - val_loss: 0.6036\n",
      "Epoch 1031/8192\n",
      "test_loss:  1.22574590248\n",
      "17s - loss: 0.4562 - val_loss: 0.5793\n",
      "Epoch 1032/8192\n",
      "test_loss:  1.26217360921\n",
      "17s - loss: 0.4564 - val_loss: 0.5782\n",
      "Epoch 1033/8192\n",
      "test_loss:  1.220246177\n",
      "18s - loss: 0.4549 - val_loss: 0.5722\n",
      "Epoch 1034/8192\n",
      "test_loss:  1.27741663313\n",
      "18s - loss: 0.4550 - val_loss: 0.6374\n",
      "Epoch 1035/8192\n",
      "test_loss:  1.20505813014\n",
      "17s - loss: 0.4560 - val_loss: 0.6173\n",
      "Epoch 1036/8192\n",
      "test_loss:  1.21133566805\n",
      "18s - loss: 0.4551 - val_loss: 0.5730\n",
      "Epoch 1037/8192\n",
      "test_loss:  1.2461401291\n",
      "17s - loss: 0.4545 - val_loss: 0.5868\n",
      "Epoch 1038/8192\n",
      "test_loss:  1.2518645557\n",
      "17s - loss: 0.4556 - val_loss: 0.5878\n",
      "Epoch 1039/8192\n",
      "test_loss:  1.23596229573\n",
      "17s - loss: 0.4534 - val_loss: 0.6015\n",
      "Epoch 1040/8192\n",
      "test_loss:  1.22910486228\n",
      "17s - loss: 0.4556 - val_loss: 0.5781\n",
      "Epoch 1041/8192\n",
      "test_loss:  1.23670363832\n",
      "17s - loss: 0.4556 - val_loss: 0.5926\n",
      "Epoch 1042/8192\n",
      "test_loss:  1.23823672838\n",
      "17s - loss: 0.4560 - val_loss: 0.6019\n",
      "Epoch 1043/8192\n",
      "test_loss:  1.20191462724\n",
      "18s - loss: 0.4548 - val_loss: 0.5748\n",
      "Epoch 1044/8192\n",
      "test_loss:  1.22643083253\n",
      "17s - loss: 0.4559 - val_loss: 0.5984\n",
      "Epoch 1045/8192\n",
      "test_loss:  1.24427582941\n",
      "18s - loss: 0.4565 - val_loss: 0.5987\n",
      "Epoch 1046/8192\n",
      "test_loss:  1.22469707782\n",
      "18s - loss: 0.4553 - val_loss: 0.5887\n",
      "Epoch 1047/8192\n",
      "test_loss:  1.20534384308\n",
      "17s - loss: 0.4557 - val_loss: 0.5831\n",
      "Epoch 1048/8192\n",
      "test_loss:  1.20977213241\n",
      "18s - loss: 0.4542 - val_loss: 0.5804\n",
      "Epoch 1049/8192\n",
      "test_loss:  1.21735308574\n",
      "17s - loss: 0.4561 - val_loss: 0.5876\n",
      "Epoch 1050/8192\n",
      "test_loss:  1.22709742706\n",
      "18s - loss: 0.4555 - val_loss: 0.5766\n",
      "Epoch 1051/8192\n",
      "test_loss:  1.22939284438\n",
      "17s - loss: 0.4542 - val_loss: 0.5837\n",
      "Epoch 1052/8192\n",
      "test_loss:  1.21784160448\n",
      "17s - loss: 0.4540 - val_loss: 0.6113\n",
      "Epoch 1053/8192\n",
      "test_loss:  1.22708844826\n",
      "17s - loss: 0.4541 - val_loss: 0.5941\n",
      "Epoch 1054/8192\n",
      "test_loss:  1.23408696887\n",
      "17s - loss: 0.4542 - val_loss: 0.5728\n",
      "Epoch 1055/8192\n",
      "test_loss:  1.23781936735\n",
      "17s - loss: 0.4553 - val_loss: 0.5997\n",
      "Epoch 1056/8192\n",
      "test_loss:  1.24952993311\n",
      "17s - loss: 0.4539 - val_loss: 0.5770\n",
      "Epoch 1057/8192\n",
      "test_loss:  1.20239698915\n",
      "17s - loss: 0.4548 - val_loss: 0.5867\n",
      "Epoch 1058/8192\n",
      "test_loss:  1.20589016569\n",
      "18s - loss: 0.4553 - val_loss: 0.5991\n",
      "Epoch 1059/8192\n",
      "test_loss:  1.23065703159\n",
      "18s - loss: 0.4544 - val_loss: 0.5838\n",
      "Epoch 1060/8192\n",
      "test_loss:  1.23783110356\n",
      "17s - loss: 0.4548 - val_loss: 0.5973\n",
      "Epoch 1061/8192\n",
      "test_loss:  1.22463748647\n",
      "17s - loss: 0.4550 - val_loss: 0.5860\n",
      "Epoch 1062/8192\n",
      "test_loss:  1.2145817716\n",
      "17s - loss: 0.4550 - val_loss: 0.5867\n",
      "Epoch 1063/8192\n",
      "test_loss:  1.2213330672\n",
      "17s - loss: 0.4545 - val_loss: 0.5943\n",
      "Epoch 1064/8192\n",
      "test_loss:  1.24786450733\n",
      "18s - loss: 0.4552 - val_loss: 0.5821\n",
      "Epoch 1065/8192\n",
      "test_loss:  1.24276510005\n",
      "17s - loss: 0.4557 - val_loss: 0.5945\n",
      "Epoch 1066/8192\n",
      "test_loss:  1.259183896\n",
      "17s - loss: 0.4543 - val_loss: 0.5747\n",
      "Epoch 1067/8192\n",
      "test_loss:  1.20733247654\n",
      "18s - loss: 0.4550 - val_loss: 0.5724\n",
      "Epoch 1068/8192\n",
      "test_loss:  1.27783714951\n",
      "18s - loss: 0.4553 - val_loss: 0.5917\n",
      "Epoch 1069/8192\n",
      "test_loss:  1.24792934177\n",
      "17s - loss: 0.4553 - val_loss: 0.5855\n",
      "Epoch 1070/8192\n",
      "test_loss:  1.2392063104\n",
      "18s - loss: 0.4550 - val_loss: 0.5890\n",
      "Epoch 1071/8192\n",
      "test_loss:  1.222465569\n",
      "17s - loss: 0.4548 - val_loss: 0.5888\n",
      "Epoch 1072/8192\n",
      "test_loss:  1.2309191795\n",
      "18s - loss: 0.4555 - val_loss: 0.5847\n",
      "Epoch 1073/8192\n",
      "test_loss:  1.25543007245\n",
      "17s - loss: 0.4555 - val_loss: 0.6040\n",
      "Epoch 1074/8192\n",
      "test_loss:  1.24165130914\n",
      "17s - loss: 0.4540 - val_loss: 0.6316\n",
      "Epoch 1075/8192\n",
      "test_loss:  1.22988628273\n",
      "18s - loss: 0.4545 - val_loss: 0.5848\n",
      "Epoch 1076/8192\n",
      "test_loss:  1.20526252253\n",
      "18s - loss: 0.4538 - val_loss: 0.5807\n",
      "Epoch 1077/8192\n",
      "test_loss:  1.22061945475\n",
      "17s - loss: 0.4526 - val_loss: 0.5888\n",
      "Epoch 1078/8192\n",
      "test_loss:  1.2570443821\n",
      "17s - loss: 0.4558 - val_loss: 0.5919\n",
      "Epoch 1079/8192\n",
      "test_loss:  1.24412283374\n",
      "17s - loss: 0.4557 - val_loss: 0.5888\n",
      "Epoch 1080/8192\n",
      "test_loss:  1.20538669679\n",
      "17s - loss: 0.4540 - val_loss: 0.5714\n",
      "Epoch 1081/8192\n",
      "test_loss:  1.25552113194\n",
      "17s - loss: 0.4545 - val_loss: 0.5844\n",
      "Epoch 1082/8192\n",
      "test_loss:  1.2522224107\n",
      "16s - loss: 0.4542 - val_loss: 0.5915\n",
      "Epoch 1083/8192\n",
      "test_loss:  1.21410437598\n",
      "17s - loss: 0.4539 - val_loss: 0.5777\n",
      "Epoch 1084/8192\n",
      "test_loss:  1.22242884983\n",
      "17s - loss: 0.4533 - val_loss: 0.5742\n",
      "Epoch 1085/8192\n",
      "test_loss:  1.24089567274\n",
      "17s - loss: 0.4542 - val_loss: 0.5711\n",
      "Epoch 1086/8192\n",
      "test_loss:  1.20283013841\n",
      "18s - loss: 0.4550 - val_loss: 0.5999\n",
      "Epoch 1087/8192\n",
      "test_loss:  1.23506113989\n",
      "17s - loss: 0.4548 - val_loss: 0.5846\n",
      "Epoch 1088/8192\n",
      "test_loss:  1.24589750029\n",
      "17s - loss: 0.4546 - val_loss: 0.5934\n",
      "Epoch 1089/8192\n",
      "test_loss:  1.21898665941\n",
      "18s - loss: 0.4538 - val_loss: 0.5816\n",
      "Epoch 1090/8192\n",
      "test_loss:  1.26040395006\n",
      "18s - loss: 0.4530 - val_loss: 0.5850\n",
      "Epoch 1091/8192\n",
      "test_loss:  1.24802455686\n",
      "18s - loss: 0.4542 - val_loss: 0.6268\n",
      "Epoch 1092/8192\n",
      "test_loss:  1.24229856812\n",
      "17s - loss: 0.4546 - val_loss: 0.5885\n",
      "Epoch 1093/8192\n",
      "test_loss:  1.22640110768\n",
      "17s - loss: 0.4547 - val_loss: 0.5966\n",
      "Epoch 1094/8192\n",
      "test_loss:  1.20631179677\n",
      "17s - loss: 0.4537 - val_loss: 0.5915\n",
      "Epoch 1095/8192\n",
      "test_loss:  1.25802543382\n",
      "17s - loss: 0.4534 - val_loss: 0.5962\n",
      "Epoch 1096/8192\n",
      "test_loss:  1.23435328892\n",
      "18s - loss: 0.4550 - val_loss: 0.5706\n",
      "Epoch 1097/8192\n",
      "test_loss:  1.27989093703\n",
      "18s - loss: 0.4545 - val_loss: 0.5829\n",
      "Epoch 1098/8192\n",
      "test_loss:  1.25482720283\n",
      "17s - loss: 0.4542 - val_loss: 0.5973\n",
      "Epoch 1099/8192\n",
      "test_loss:  1.28777938389\n",
      "17s - loss: 0.4536 - val_loss: 0.6106\n",
      "Epoch 1100/8192\n",
      "test_loss:  1.30509806408\n",
      "17s - loss: 0.4540 - val_loss: 0.5978\n",
      "Epoch 1101/8192\n",
      "test_loss:  1.20937824807\n",
      "17s - loss: 0.4542 - val_loss: 0.5921\n",
      "Epoch 1102/8192\n",
      "test_loss:  1.22894166835\n",
      "17s - loss: 0.4544 - val_loss: 0.5811\n",
      "Epoch 1103/8192\n",
      "test_loss:  1.27813574291\n",
      "17s - loss: 0.4539 - val_loss: 0.6090\n",
      "Epoch 1104/8192\n",
      "test_loss:  1.25750729943\n",
      "17s - loss: 0.4543 - val_loss: 0.5947\n",
      "Epoch 1105/8192\n",
      "test_loss:  1.24113937132\n",
      "17s - loss: 0.4536 - val_loss: 0.5736\n",
      "Epoch 1106/8192\n",
      "test_loss:  1.22254009365\n",
      "17s - loss: 0.4529 - val_loss: 0.5809\n",
      "Epoch 1107/8192\n",
      "test_loss:  1.28293784711\n",
      "17s - loss: 0.4532 - val_loss: 0.5889\n",
      "Epoch 1108/8192\n",
      "test_loss:  1.25369493241\n",
      "17s - loss: 0.4540 - val_loss: 0.5702\n",
      "Epoch 1109/8192\n",
      "test_loss:  1.30807061503\n",
      "17s - loss: 0.4529 - val_loss: 0.6151\n",
      "Epoch 1110/8192\n",
      "test_loss:  1.25681868711\n",
      "17s - loss: 0.4540 - val_loss: 0.6147\n",
      "Epoch 1111/8192\n",
      "test_loss:  1.23952880369\n",
      "17s - loss: 0.4536 - val_loss: 0.5798\n",
      "Epoch 1112/8192\n",
      "test_loss:  1.24988367772\n",
      "17s - loss: 0.4534 - val_loss: 0.6177\n",
      "Epoch 1113/8192\n",
      "test_loss:  1.25678375453\n",
      "18s - loss: 0.4537 - val_loss: 0.5843\n",
      "Epoch 1114/8192\n",
      "test_loss:  1.22000752372\n",
      "17s - loss: 0.4529 - val_loss: 0.5908\n",
      "Epoch 1115/8192\n",
      "test_loss:  1.25958032936\n",
      "17s - loss: 0.4541 - val_loss: 0.6034\n",
      "Epoch 1116/8192\n",
      "test_loss:  1.28948881541\n",
      "17s - loss: 0.4516 - val_loss: 0.5959\n",
      "Epoch 1117/8192\n",
      "test_loss:  1.25965271308\n",
      "17s - loss: 0.4547 - val_loss: 0.5938\n",
      "Epoch 1118/8192\n",
      "test_loss:  1.27219838106\n",
      "17s - loss: 0.4543 - val_loss: 0.5784\n",
      "Epoch 1119/8192\n",
      "test_loss:  1.22444741627\n",
      "17s - loss: 0.4528 - val_loss: 0.5742\n",
      "Epoch 1120/8192\n",
      "test_loss:  1.2198119554\n",
      "17s - loss: 0.4523 - val_loss: 0.5774\n",
      "Epoch 1121/8192\n",
      "test_loss:  1.30975024078\n",
      "17s - loss: 0.4537 - val_loss: 0.6352\n",
      "Epoch 1122/8192\n",
      "test_loss:  1.26882945382\n",
      "16s - loss: 0.4538 - val_loss: 0.5860\n",
      "Epoch 1123/8192\n",
      "test_loss:  1.23953395818\n",
      "18s - loss: 0.4535 - val_loss: 0.5653\n",
      "Epoch 1124/8192\n",
      "test_loss:  1.25201396111\n",
      "17s - loss: 0.4539 - val_loss: 0.5889\n",
      "Epoch 1125/8192\n",
      "test_loss:  1.23420170559\n",
      "16s - loss: 0.4530 - val_loss: 0.5830\n",
      "Epoch 1126/8192\n",
      "test_loss:  1.30034082447\n",
      "18s - loss: 0.4533 - val_loss: 0.5894\n",
      "Epoch 1127/8192\n",
      "test_loss:  1.26437906433\n",
      "17s - loss: 0.4531 - val_loss: 0.5802\n",
      "Epoch 1128/8192\n",
      "test_loss:  1.27557800499\n",
      "17s - loss: 0.4524 - val_loss: 0.5730\n",
      "Epoch 1129/8192\n",
      "test_loss:  1.24148977895\n",
      "17s - loss: 0.4542 - val_loss: 0.5792\n",
      "Epoch 1130/8192\n",
      "test_loss:  1.25711082164\n",
      "17s - loss: 0.4545 - val_loss: 0.5898\n",
      "Epoch 1131/8192\n",
      "test_loss:  1.23956473252\n",
      "17s - loss: 0.4526 - val_loss: 0.5957\n",
      "Epoch 1132/8192\n",
      "test_loss:  1.22048273456\n",
      "17s - loss: 0.4526 - val_loss: 0.5725\n",
      "Epoch 1133/8192\n",
      "test_loss:  1.24778796146\n",
      "17s - loss: 0.4530 - val_loss: 0.5934\n",
      "Epoch 1134/8192\n",
      "test_loss:  1.21703491435\n",
      "17s - loss: 0.4522 - val_loss: 0.6253\n",
      "Epoch 1135/8192\n",
      "test_loss:  1.24315569098\n",
      "17s - loss: 0.4548 - val_loss: 0.5756\n",
      "Epoch 1136/8192\n",
      "test_loss:  1.23032102041\n",
      "17s - loss: 0.4530 - val_loss: 0.5824\n",
      "Epoch 1137/8192\n",
      "test_loss:  1.24600809452\n",
      "17s - loss: 0.4530 - val_loss: 0.5790\n",
      "Epoch 1138/8192\n",
      "test_loss:  1.24145937616\n",
      "17s - loss: 0.4526 - val_loss: 0.5786\n",
      "Epoch 1139/8192\n",
      "test_loss:  1.20221023471\n",
      "17s - loss: 0.4527 - val_loss: 0.5736\n",
      "Epoch 1140/8192\n",
      "test_loss:  1.25927912295\n",
      "18s - loss: 0.4539 - val_loss: 0.5784\n",
      "Epoch 1141/8192\n",
      "test_loss:  1.30641990383\n",
      "17s - loss: 0.4540 - val_loss: 0.6039\n",
      "Epoch 1142/8192\n",
      "test_loss:  1.34569582731\n",
      "17s - loss: 0.4535 - val_loss: 0.5898\n",
      "Epoch 1143/8192\n",
      "test_loss:  1.243557028\n",
      "17s - loss: 0.4529 - val_loss: 0.5819\n",
      "Epoch 1144/8192\n",
      "test_loss:  1.21637280533\n",
      "18s - loss: 0.4527 - val_loss: 0.5691\n",
      "Epoch 1145/8192\n",
      "test_loss:  1.26006409772\n",
      "17s - loss: 0.4530 - val_loss: 0.5799\n",
      "Epoch 1146/8192\n",
      "test_loss:  1.27192862746\n",
      "17s - loss: 0.4542 - val_loss: 0.5859\n",
      "Epoch 1147/8192\n",
      "test_loss:  1.26749266241\n",
      "17s - loss: 0.4542 - val_loss: 0.5852\n",
      "Epoch 1148/8192\n",
      "test_loss:  1.2968643563\n",
      "18s - loss: 0.4522 - val_loss: 0.6015\n",
      "Epoch 1149/8192\n",
      "test_loss:  1.2475198683\n",
      "17s - loss: 0.4541 - val_loss: 0.6075\n",
      "Epoch 1150/8192\n",
      "test_loss:  1.22326329785\n",
      "18s - loss: 0.4522 - val_loss: 0.5813\n",
      "Epoch 1151/8192\n",
      "test_loss:  1.24669593931\n",
      "17s - loss: 0.4546 - val_loss: 0.5821\n",
      "Epoch 1152/8192\n",
      "test_loss:  1.2311314719\n",
      "17s - loss: 0.4528 - val_loss: 0.5782\n",
      "Epoch 1153/8192\n",
      "test_loss:  1.22200614565\n",
      "17s - loss: 0.4535 - val_loss: 0.5709\n",
      "Epoch 1154/8192\n",
      "test_loss:  1.2783782186\n",
      "17s - loss: 0.4530 - val_loss: 0.6027\n",
      "Epoch 1155/8192\n",
      "test_loss:  1.22718974441\n",
      "17s - loss: 0.4540 - val_loss: 0.6192\n",
      "Epoch 1156/8192\n",
      "test_loss:  1.25791411798\n",
      "17s - loss: 0.4544 - val_loss: 0.5835\n",
      "Epoch 1157/8192\n",
      "test_loss:  1.29434669358\n",
      "17s - loss: 0.4525 - val_loss: 0.5856\n",
      "Epoch 1158/8192\n",
      "test_loss:  1.24582932017\n",
      "17s - loss: 0.4537 - val_loss: 0.5746\n",
      "Epoch 1159/8192\n",
      "test_loss:  1.23365903015\n",
      "17s - loss: 0.4542 - val_loss: 0.5725\n",
      "Epoch 1160/8192\n",
      "test_loss:  1.24574635911\n",
      "17s - loss: 0.4524 - val_loss: 0.5666\n",
      "Epoch 1161/8192\n",
      "test_loss:  1.22984493157\n",
      "17s - loss: 0.4533 - val_loss: 0.5985\n",
      "Epoch 1162/8192\n",
      "test_loss:  1.27605193116\n",
      "18s - loss: 0.4528 - val_loss: 0.5940\n",
      "Epoch 1163/8192\n",
      "test_loss:  1.29248355331\n",
      "17s - loss: 0.4519 - val_loss: 0.5825\n",
      "Epoch 1164/8192\n",
      "test_loss:  1.31675355217\n",
      "17s - loss: 0.4522 - val_loss: 0.6061\n",
      "Epoch 1165/8192\n",
      "test_loss:  1.25990096636\n",
      "17s - loss: 0.4535 - val_loss: 0.5647\n",
      "Epoch 1166/8192\n",
      "test_loss:  1.21570798638\n",
      "17s - loss: 0.4517 - val_loss: 0.5741\n",
      "Epoch 1167/8192\n",
      "test_loss:  1.26590100133\n",
      "17s - loss: 0.4530 - val_loss: 0.5843\n",
      "Epoch 01166: early stopping\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 8192\n",
    "batch_size = 256\n",
    "from keras import optimizers\n",
    "\n",
    "adamm = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1, clipnorm=1.)\n",
    "\n",
    "ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 50\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(150, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "                activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(input_layer)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(100, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "                activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "encoded = Dense(encoding_dim, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "                activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(100, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "                activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(150, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "                activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "\n",
    "decoder = Dense(input_dim)(encoded)\n",
    "decoder = prellll(decoder)\n",
    "# decoder = Dense(input_dim)(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "\n",
    "name = \"TestPrelusgd_l1e5\"\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=60,\n",
    "                                  verbose=True,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer=sgd, \n",
    "                            loss='mean_squared_error'\n",
    "                            # metrics=['accuracy']\n",
    "                           )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"/afs/cern.ch/user/f/fsiroky/models_ae/%s.h5\" % name),\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  verbose=False,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"min\")\n",
    "testerror = AdditionalValidationSets([(X_test, X_test, 'test')])\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.25,\n",
    "                            verbose=2,\n",
    "                            callbacks=[testerror, early_stopper, checkpoint_callback]\n",
    "                         ).history\n",
    "\n",
    "#np.save('/eos/cms/store/user/fsiroky/ae_models/%s.npy' % name, history)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_loss.npy' % name , history['loss'])\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_valloss.npy' % name, history['val_loss'])\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_testloss.npy' % name , testerror.history['test_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
